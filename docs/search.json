[
  {
    "objectID": "posts/2022-11-24-updating-R-and-packages/index.html",
    "href": "posts/2022-11-24-updating-R-and-packages/index.html",
    "title": "Updating R and your R packages",
    "section": "",
    "text": "I teach a few R workshops a year, and I often send out the typical “make sure R and Rstudio is up-to-date, using these helpful links. And then of course the workshop starts, and all the class notices that I myself have not updated R in .. 7 months.\nWhen I do decide to do it, I find I often need to google the correct procedure. In the spirit of this blog, here is a short note detailing the process, inspired by this stackoverflow note"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html",
    "href": "posts/2022-11-29-group-centering/index.html",
    "title": "Andrew tries Quarto",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#what-is-this-post",
    "href": "posts/2022-11-29-group-centering/index.html#what-is-this-post",
    "title": "Andrew tries Quarto",
    "section": "What is this post",
    "text": "What is this post\nThis post is meant to be a simple template for new posts – when I make a new post I’ll start by copying and pasting this current one over. Probably in the future there will be a plugin that does this for us, but until then this will be fine."
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#why-i-switched-to-quarto",
    "href": "posts/2022-11-29-group-centering/index.html#why-i-switched-to-quarto",
    "title": "Andrew tries Quarto",
    "section": "Why I switched to Quarto",
    "text": "Why I switched to Quarto\nA simple and frequent answer: overenthusiasm! I like seeing all the new things that the Rstudio team develop, and I know that this vibrant community will keep adding features and tutorials"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#quarto-resources",
    "href": "posts/2022-11-29-group-centering/index.html#quarto-resources",
    "title": "Andrew tries Quarto",
    "section": "quarto resources",
    "text": "quarto resources\n\nthe Ultimate Guide to starting a Quarto blog\nquarto discussions\nDanielle Navarro’s comments on the topic\nNick Tierney’s notes\nand of course Nicks exciting book project!"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#surprise-its-targets",
    "href": "posts/2022-11-29-group-centering/index.html#surprise-its-targets",
    "title": "Andrew tries Quarto",
    "section": "surprise it’s targets",
    "text": "surprise it’s targets\n\n\n\nvia GIPHY\n\nI’m also using targets. Here are some observations on that so far:\n\nLast error: ! System command 'quarto' failed I get this error message more that 10x more often than any other. As I’m learning Quarto I keep making errors which break the Quarto process but targets doesn’t (yet?) communicate the specific error message. To find out what has gone wrong, I go over to the terminal and run quarto render .\nyou have to go into _metadata.yml and stop the posts from freezing, by setting freeze: false . Targets will handle all the rest of it.\n\nI’m using targets with quarto because I want to work with large bayesian models fit with stan and brms., and fitting them in a regular blog post – including compiling, sampling etc."
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#my-own-observations-and-questions",
    "href": "posts/2022-11-29-group-centering/index.html#my-own-observations-and-questions",
    "title": "Andrew tries Quarto",
    "section": "My own observations and questions",
    "text": "My own observations and questions\nso far it is very straightforward!\n\n~it seems like post folder cant begin with dates?~ you can start a folder with a date, and I do, so that the posts sort in at least approximately the right temporal sequence. However the date for a post comes from the YAML, not the folder name.\nlove LOVE the bibliography automatically appearing\nDO think about changing the working directory for chunk evaluations to the project root, as described here. This is really important because that is the directory that targets uses when sourcing files. For example tar_load(my_target_name) will load in an object from your pipeline. But run this same line from an .qmd file in a subfolder (say posts/yourpostname/index.qmd), then THAT will be the location the computer looks in.."
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)\nlibrary(stantargets)"
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#the-equation",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#the-equation",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "The equation",
    "text": "The equation\nPredators eat prey. They eat prey faster when there is more prey – though they do have to slow down to catch, kill, and chew.1\nIf a predator can eat multiple things, then they might end up eating less of any one prey because they spread their kills around among all their prey. In the very simplest case, they do this in proportion to how frequent the different prey are – the predator has no preference, it just goes around randomly and eats what it finds\nThe classic OG version of this model comes from Holling (1966)\n\\[\nA = \\frac{\\alpha N}{1 + h\\alpha N}\n\\tag{1}\\]\nwhere\n\nN is the number of prey\n\\(\\alpha\\) is the attack rate\n\\(h\\) is the handling time\n\nAnd altogether you get the number of attacks \\(A\\) on prey in some unit of time."
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#multiple-species",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#multiple-species",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "multiple species",
    "text": "multiple species\nSuppose you have more than one species in this system. You could then rewrite Equation 1 to allow multiple animals to be included in the predation:\n\\[\nA_i = \\frac{\\alpha N_i}{1 + h\\alpha \\sum_{j = 1}^s N_j}\n\\tag{2}\\]\nhere \\(\\sum_{j = 1}^s N_j\\) means the sum over the abundances of all the prey. The subscript \\(i\\) just means that we are talking about one particular prey, which we label \\(i\\). This prey is included in the summation in the denominator.\nIt’s common to consider that different prey species might be attacked or handled at different rates (Smith and Smith 2020) (Smout et al. 2010)"
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#one-species-model",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#one-species-model",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "One species model",
    "text": "One species model\nLet’s begin with the classic model and add complexity later.\nI think it helps to think of predation as a binomial trial: out of so many prey individuals (\\(N\\)), some of them get attacked (\\(A\\)).\n\\[\n\\begin{align}\nA &\\sim \\text{Binomial}(p, N) \\\\\np &= \\frac{\\alpha}{1 + h\\alpha N} \\\\\n\\alpha &\\sim \\text{Beta}(2, 4) \\\\\nh &\\sim \\text{LogNormal}(0,1)\n\\end{align}\n\\] some things to note:\n\nthe \\(N\\) now appears in the Binomial distribution as a separate parameter, not in the expression for the probability of attack. Remember that the mean of a Binomial is \\(pN\\), so in this case we will come back to Equation 1\nboth the parameters have constraints: \\(a\\) cannot be outside of \\([0,1]\\), and \\(h\\) cannot be negative. We choose priors that respect these constraints!\n\nLet’s translate this into Stan and take a look:\n\nsimple_type2 <- cmdstan_model(here::here(\"posts\", \n                                         \"2022-11-11-multispecies-functional-response\",\n                                         \"simple_type2.stan\"))\n\nsimple_type2\n\n// simple predator-prey functional response for a binomial density\ndata {\n  int<lower=0> N;\n  array[N] int<lower=0> attacks;\n  array[N] int<lower=0> densities;\n}\nparameters {\n  real<lower=0,upper=1> a;\n  real<lower=0> h;\n}\ntransformed parameters{\n  vector<lower=0, upper = 1>[N] prob_attack;\n  prob_attack = a * inv(1 + a * h * to_vector(densities));\n}\nmodel {\n  a ~ beta(2,6);\n  h ~ lognormal(0, 1);\n  attacks ~ binomial(densities, prob_attack);\n}\n\n\nthe code above is mostly a direct translation of the equations. One technique is worth noting: the types of the input vectors. Binomial distributions deal in integers, and so we define densities and attacks as integers. However, in order to vectorize our calculations, we massage the input data from an array of integers to a vector of real numbers using to_vector. This highlights an important difference between R and Stan. Stan requires careful definition of the types of data, where R is much more informal."
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#simulate-from-a-model",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#simulate-from-a-model",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "simulate from a model:",
    "text": "simulate from a model:\n\ngenerate_one_spp_type_2 <- function(){\n  true_a <- stats::rbeta(n = 1, 8, 4)\n  true_h <- stats::rlnorm(n = 1, -2, .5)\n  densities <- seq(from = 5, to = 100, by =5)\n  prob <- true_a/(1 + true_a * true_h * densities)\n  attacks <- rbinom(n = length(densities), size = densities, prob = prob)\n  list(true_a = true_a,\n       true_h = true_h,\n       densities = densities,\n       attacks = attacks, \n       prob = prob)\n}\n\none_spp_sim <- generate_one_spp_type_2()\n\none_spp_sim\n\n$true_a\n[1] 0.7568304\n\n$true_h\n[1] 0.1605559\n\n$densities\n [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95\n[20] 100\n\n$attacks\n [1] 2 3 4 5 7 6 3 6 3 7 6 4 5 2 8 6 4 5 5 6\n\n$prob\n [1] 0.47079213 0.34166316 0.26812246 0.22063275 0.18743446 0.16292010\n [7] 0.14407649 0.12913995 0.11700947 0.10696221 0.09850396 0.09128539\n[13] 0.08505256 0.07961647 0.07483352 0.07059268 0.06680672 0.06340618\n[19] 0.06033505 0.05754769\n\nwith(one_spp_sim, rbinom(n = length(densities), size = densities, prob = prob))\n\n [1] 4 5 3 3 8 3 5 5 4 8 7 7 5 3 7 2 5 7 3 6\n\nwith(one_spp_sim, plot(densities, prob*densities))\n\n\n\nwith(one_spp_sim, plot(densities, attacks))\n\n\n\n\ntry it in targets:\n\n\ntar_option_set(packages = c(\"cmdstanr\",\n               \"ggplot2\", \"tidybayes\", \n               \"stantargets\"))\n\ngenerate_one_spp_type_too <- function(){\n  true_a <- stats::rbeta(n = 1, 8, 4)\n  true_h <- stats::rlnorm(n = 1, -2, .5)\n  densities <- seq(from = 5, to = 100, by =5)\n  prob <- true_a/(1 + true_a * true_h * densities)\n  attacks <- rbinom(n = length(densities), size = densities, prob = prob)\n  list(\n    N = length(attacks),\n    true_a = true_a,\n    true_h = true_h,\n    densities = densities,\n    attacks = attacks, \n    prob = prob\n    )\n}\n\nEstablish _targets.R and _targets_r/globals/some-globals.R.\n\n\n\nlist(\n  stantargets::tar_stan_mcmc(name = one_spp, \n                stan_files = \"simple_type2.stan\",\n                data = generate_one_spp_type_too(),\n                stdout = R.utils::nullfile(),\n                stderr = R.utils::nullfile()\n  )\n)\n\nEstablish _targets.R and _targets_r/targets/run_one_model.R.\n\n\n\ntar_visnetwork()\n\n\n\n\n\n\ntar_make()\n\n✔ skip target one_spp_data\n✔ skip target one_spp_file_simple_type2\n✔ skip target one_spp_mcmc_simple_type2\n✔ skip target one_spp_diagnostics_simple_type2\n✔ skip target one_spp_summary_simple_type2\n✔ skip target one_spp_draws_simple_type2\n✔ skip pipeline [0.17 seconds]"
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html",
    "title": "Growth curves",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(cmdstanr))\nMany animals and plants grow quickly when small and more slowly as they mature. There are many popular ways of describing this relationship; one very common and convenient relationship is the Von-Bertanalaffy (VB) growth curve:\n\\[\nL_t = L_0e^{-rt} + L_\\infty(1 - e^{-rt})\n\\tag{1}\\]\nThis can also be written as\n\\[\nL_t = L_\\infty - (L_\\infty - L_0)e^{-rt}\n\\]\nThis curve has a long tradition in ecology. It can be derived from simple assumptions about how different aspects of metabolism scale with the body size of an organism. I’m not going to derive it here because I don’t want this to be a huge post!\nI like this second way of writing the equation because it highlights that the VB equation is a linear transformation of an exponential function. We start out at \\(L_0\\) and exponentially decay towards \\(L_\\infty\\)."
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html#a-single-tree",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html#a-single-tree",
    "title": "Growth curves",
    "section": "a single tree",
    "text": "a single tree\nI’m going to do a simple simulation of one tree growing. here is code that does that\n\nsim_vb_one_tree <- function(\n    time = seq(from = 10, to = 200, by = 5),\n    Lo = .01,\n    Lmax = 150,\n    r = .03,\n    sd = 5){\n  tibble::tibble(time,\n                 Lt = Lmax - (Lmax - Lo) * exp(-r*time),\n                 Lt_obs  = rnorm(length(Lt),\n                                 mean = Lt,\n                                 sd = 5))\n}\n\nvb_one_tree <- sim_vb_one_tree()\n\nvb_one_tree |> \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() + \n  geom_line(aes(y = Lt)) + \n  theme_bw()"
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html#recover-parameters",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html#recover-parameters",
    "title": "Growth curves",
    "section": "Recover parameters",
    "text": "Recover parameters\nHere is a stan model that matches this data generating process:\n\n\ndata{\n  int<lower=0> n;\n  vector[n] time;\n  vector[n] Lt;\n}\nparameters{\n  real<lower=0> r;\n  real<lower=0> Lmax;\n  real<lower=0> sigma_obs;\n}\nmodel{\n  Lt ~ normal(Lmax * (1 - exp(-r*time)), sigma_obs);\n  r ~ lognormal(-3, 1);\n  Lmax ~ normal(200, 20);\n  sigma_obs ~ exponential(1);\n}\n\n\n\none_tree_sim <- sim_vb_one_tree(\n    Lmax = 150,\n    r = .03,\n    sd = 5)\n\none_tree_list <- list(n = nrow(one_tree_sim),\n                      time  = one_tree_sim$time, \n                      Lt = one_tree_sim$Lt_obs)\n\none_tree_post <- vb_one_tree$sample(data = one_tree_list,\n                                    refresh = 0L,\n                                    parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.4 seconds.\n\none_tree_post$summary() |> \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nlp__\n-84.0475862\n-83.7282000\n1.2654516\n1.0264040\n-86.4815100\n-82.6899900\n1.002211\n1726.271\n2253.937\n\n\nr\n0.0293271\n0.0292921\n0.0010091\n0.0009937\n0.0277237\n0.0310195\n1.002767\n2022.287\n2140.461\n\n\nLmax\n149.9431923\n149.9170000\n1.2118152\n1.1979408\n147.9899500\n151.9602000\n1.002967\n2163.288\n2291.239\n\n\nsigma_obs\n4.7191941\n4.6703850\n0.5231559\n0.5085615\n3.9512310\n5.6504515\n1.000848\n2331.954\n2412.225\n\n\n\n\n\nThese posterior intervals cover the numbers used to make up the data pretty well! Let’s look at the model predictions on a figure:\n\nexpected_df <- one_tree_post |> \n  spread_rvars(Lmax, r) |> \n  expand_grid(time = seq(0, 200, length.out = 14)) |> \n  mutate(Lt = Lmax * (1 - exp(-r * time)))\n\nexpected_plot <- expected_df |> \n  ggplot(aes(x = time, ydist = Lt)) + \n  stat_dist_lineribbon()\n\nexpected_plot\n\n\n\n\nGrowth curve for one tree. the line shows the expected value, with posterior uncertainty around exactly what that average should be.\n\n\n\n\nThis relationship shows the average line, the expected size of the tree. We can add the original data like this:\n\none_tree_sim |> \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() +\n  stat_dist_lineribbon(aes(x = time, dist = Lt),\n                  data = expected_df, inherit.aes = FALSE) + \n  theme_bw()\n\n\n\n\nAt the time of this writing the error messages here are particularly unhelpful. If you try to use stat_lineribbon rather than stat_dist_lineribbon you get the following misleading message:\n\none_tree_sim |> \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() +\n  stat_lineribbon(aes(x = time, y = Lt),\n                  data = expected_df, inherit.aes = FALSE)\n\nError: Discrete value supplied to continuous scale"
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html#adding-measurement-error",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html#adding-measurement-error",
    "title": "Growth curves",
    "section": "Adding measurement error",
    "text": "Adding measurement error\nThe above model reproduces predictions of the original line, but ignores measurement error. Here’s a few ways to add that into this same approach:\n\nSimulating observations in R\nOne way to do this is after the fact, using the handy tidyverse dplyr::rowwise() syntax, combined with posterior::rfun(). The latter function transforms rnorm into a function that both takes and produces an rvar, the specialized format for working with posterior draws. The latter function makes sure we redo this for every row of our dataframe.\n\nexpected_df <- one_tree_post |> \n  spread_rvars(Lmax, r, sigma_obs) |> \n  expand_grid(time = seq(0, 200, length.out = 14)) |> \n  mutate(Lt = Lmax * (1 - exp(-r * time))) |> \n  rowwise() |> \n  mutate(Lt_obs = posterior::rfun(rnorm)(n = 1, mean = Lt, sd = sigma_obs))\n\nexpected_df |> \n  ggplot(aes(x = time, dist = Lt_obs)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\") + \n  geom_point(aes(y = Lt_obs), data = one_tree_sim,\n             pch = 21, fill = \"darkorange\") + \n  theme_bw()\n\n\n\n\nThis has the advantage of happening all in R, keeping our posterior distribution slim.\nHowever sometimes it can be both convenient and more readable to keep the whole process inside Stan, and here’s how:\n\n\nPosterior predictive simulations in Stan\n\nvb_one_tree_gq <- cmdstan_model(\n  stan_file = here::here(\n    \"posts/2022-10-14-growth_curve_measurement_error/vb_one_tree_gq.stan\"))\n\nvb_one_tree_gq\n\ndata{\n  int<lower=0> n;\n  vector[n] time;\n  vector[n] Lt;\n  int<lower=0> n_new;\n  vector[n_new] time_new;\n}\nparameters{\n  real<lower=0> r;\n  real<lower=0> Lmax;\n  real<lower=0> sigma_obs;\n}\nmodel{\n  Lt ~ normal(Lmax * (1 - exp(-r*time)), sigma_obs);\n  r ~ lognormal(-3, 1);\n  Lmax ~ normal(200, 20);\n  sigma_obs ~ exponential(1);\n}\ngenerated quantities{\n  vector[n_new] Lt_predicted;\n\n  for (i in 1:n_new){\n    Lt_predicted[i] = normal_rng(Lmax * (1 - exp(-r*time_new[i])), sigma_obs);\n  }\n}\n\n\n\none_tree_predictions <- vb_one_tree_gq$sample(\n  data = purrr::splice(one_tree_list,\n                       time_new = seq(0, 200, length.out = 14),\n                       n_new = 14),\n  refresh = 0L,\n  parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.4 seconds.\n\none_tree_predictions |> \n  spread_rvars(Lt_predicted[i]) |> \n  mutate(time_new = seq(0, 200, length.out = 14)) |> \n  ggplot(aes(x = time_new, dist = Lt_predicted)) + \n  stat_lineribbon() +\n  scale_fill_brewer(palette = \"Greens\") + \n  geom_point(aes(x = time, y = Lt_obs), \n             inherit.aes = FALSE,\n             data = one_tree_sim,\n             pch = 21, fill = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html",
    "title": "The evolution of plasticity",
    "section": "",
    "text": "options(tidyverse.quiet = TRUE)\nlibrary(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\n\n## functions to make models and so on\nsimulate_many_moms <- function(pop_average_dponte = 138,\n                               pop_average_csize = 4,\n                               mom_quality_max = 4,\n                               quality_on_dponte = 2,\n                               quality_on_csize = .2,\n                               n_females = 42,\n                               lifespan = 5,\n                               temp_range  = c(2, 12)) {\n\n  general_temp <- runif(lifespan, temp_range[1], max = temp_range[2])\n\n  general_temp_c <- general_temp - mean(general_temp)\n\n  mom_qualities <- runif(n_females, min = 0, max = 4)\n\n  many_moms_temperature <- expand_grid(year = 1:lifespan,\n                                       idF1 = 1:n_females) |>\n    mutate(mom_quality = mom_qualities[idF1],\n           general_temp = general_temp[year],\n           general_temp_c = general_temp_c[year],\n           ## adding the biology\n           ## Effect of temperature -- does it depend on quality? let's say that it DOES (for now)\n           effet_temp_dponte_qual = -.7*mom_quality,\n           effet_temp_csize_qual = .1*log(mom_quality),\n           # csize\n           mom_avg_csize = log(pop_average_csize) +  quality_on_csize*log(mom_quality),\n           temp_avg_csize = exp(mom_avg_csize + effet_temp_csize_qual*general_temp_c),\n           # dponte\n           mom_avg_dponte = pop_average_dponte + quality_on_dponte*mom_quality,\n           temp_avg_dponte = mom_avg_dponte + effet_temp_dponte_qual*general_temp_c,\n           ## observations\n           obs_csize = rpois(n = length(year), lambda = temp_avg_csize),\n           obs_dponte = rnorm(n = length(year), mean = temp_avg_dponte, sd = 3) |> round()\n    )\n  return(many_moms_temperature)\n}\n\n\nbrms_dponte_csize <- function(many_moms_temperature) {\n  ## define formulae\n  csize_model_bf <- bf(obs_csize ~ 1 + general_temp_c + (1 + general_temp_c|f|idF1),\n                       family = poisson())\n\n  dponte_model_bf <- bf(obs_dponte ~ 1 + general_temp_c + (1 + general_temp_c|f|idF1),\n                        family = gaussian())\n\n\n  ## set priors\n\n  ## run full model\n  full_model <- brm(csize_model_bf + dponte_model_bf,\n                    data = many_moms_temperature,\n                    cores = 2, chains = 2)\n}"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#study-question",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#study-question",
    "title": "The evolution of plasticity",
    "section": "Study question",
    "text": "Study question\nPhenotypic plasiticity is the"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#data-simulation",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#data-simulation",
    "title": "The evolution of plasticity",
    "section": "Data simulation",
    "text": "Data simulation\nRARE to have more than two years per female\nlet’s start with one female\n\nn <- 1\navg_csize <- 5\n\nlifespan <- 5\n\ngeneral_temp <- runif(lifespan, 2, 12)\n\ngeneral_temp_c <- general_temp - mean(general_temp)"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#fecundity",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#fecundity",
    "title": "The evolution of plasticity",
    "section": "fecundity",
    "text": "fecundity\n\\[\n\\begin{align}\n\\text{eggs} &\\sim \\text{Poisson}(e^{\\beta_0 + \\beta_1*(x - \\bar{x})})\n\\end{align}\n\\]\n\neffet_temp <- .1\n\none_bird <- tibble(year = 1:lifespan,\n       general_temp,\n       general_temp_c,\n       expected_clutch = log(avg_csize) + effet_temp * general_temp_c,\n       observed_clutch = rpois(n = length(year), \n                               lambda = exp(expected_clutch)))\n\none_bird\n\nMake a simple model to measure\n\nsummary(glm(observed_clutch ~ general_temp_c, data = one_bird))\n\n\none_bird |> \n  ggplot(aes(x = general_temp, y = observed_clutch)) + \n  geom_point()"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#date-of-laying",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#date-of-laying",
    "title": "The evolution of plasticity",
    "section": "Date of laying",
    "text": "Date of laying\nWhen do birds lay eggs?\n\navg_dponte <- 138\n\neffet_temp_dponte <- -3\n\none_bird_dponte <- tibble(year = 1:lifespan,\n       general_temp,\n       general_temp_c,\n       expected_dponte = avg_dponte + effet_temp_dponte * general_temp_c,\n       observed_dponte = round(rnorm(n = length(year), \n                               mean = expected_dponte,\n                               sd = 5)))\n\n\none_bird_dponte |> \n  ggplot(aes(x = general_temp, y= observed_dponte)) + \n  geom_point()\n\nsummary(lm(observed_dponte ~ general_temp_c, data = one_bird_dponte))"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#combine-the-two",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#combine-the-two",
    "title": "The evolution of plasticity",
    "section": "combine the two",
    "text": "combine the two\nBirds which lay earlier also lay larger eggs, possibly because they are High Quality Moms.\n\n## population averages\npop_average_dponte <- 138\npop_average_csize <- 4\n\n## Effect of quality\nmom_quality <- 4\nquality_on_dponte <- 2\nquality_on_csize <- .2\n\nlet’s observe five years for the high-quality Mom:\n\nquality_effects <- tibble(\n  year = 1:lifespan,\n  mom_quality = mom_quality,\n  general_temp,\n  general_temp_c,\n  ## Effect of temperature -- does it depend on quality? let's say that it DOES (for now) \n  effet_temp_dponte_qual = -.7*mom_quality,\n  effet_temp_csize_qual = .1*log(mom_quality),\n  # csize\n  mom_avg_csize = log(pop_average_csize) +  quality_on_csize*log(mom_quality),\n  temp_avg_csize = exp(mom_avg_csize + effet_temp_csize_qual*general_temp_c),\n  # dponte\n  mom_avg_dponte = pop_average_dponte + quality_on_dponte*mom_quality,\n  temp_avg_dponte = mom_avg_dponte + effet_temp_dponte_qual*general_temp_c,\n  ## observations\n  obs_csize = rpois(n = length(year), lambda = temp_avg_csize),\n  obs_dponte = rnorm(n = length(year), mean = temp_avg_dponte, sd = 3) |> round()\n)\n\nSome of these values are unreasonable! we can adjust later\n\nquality_effects |> \n  ggplot(aes(x = general_temp, y = obs_csize)) + \n  geom_point()\n\nquality_effects |> \n  ggplot(aes(x = general_temp, y = obs_dponte)) + geom_point()"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#multiple-females",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#multiple-females",
    "title": "The evolution of plasticity",
    "section": "Multiple females",
    "text": "Multiple females\nWe can repeat this process for multiple females at once! let’s wrap it in a function to make it easier to work with.\n\nsimulate_many_moms\nmany_moms_temperature <- simulate_many_moms()\n\nlet’s plot it!\n\nmany_moms_temperature |> \n  ggplot(aes(x = general_temp, y = obs_dponte)) + \n  geom_point()\n\nmany_moms_temperature |> \n  ggplot(aes(x = general_temp, y = obs_dponte, group = idF1)) + \n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\nmany_moms_temperature |> \n  ggplot(aes(x = general_temp, y = obs_csize)) + \n  geom_point()\n\nmany_moms_temperature |> \n  ggplot(aes(x = general_temp, y = obs_csize, group = idF1)) + \n  stat_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#model-in-brms",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#model-in-brms",
    "title": "The evolution of plasticity",
    "section": "model in brms",
    "text": "model in brms\nThe model above describes a situation where female swallows have some underlying trait (“quality”). This trait determines if this female will be above or below the rest of her population in two different outcomes: the timeing of her laying and the size of her clutch. This is a model structure that can’t be easily fit in lme4 at least as far as I know. However we can specify it in a very straightforward way using brms:"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#fitness",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#fitness",
    "title": "The evolution of plasticity",
    "section": "fitness",
    "text": "fitness"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#notes",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#notes",
    "title": "The evolution of plasticity",
    "section": "notes",
    "text": "notes\nvery small sample sizes per female – experiment with this (2 years or more)\nThe data are 0-truncated: only nesting females are measured!\nI think it is particularly interesting that the data are already conditioned on reproduction. that is, females who fail to reproduce at all don’t get included in the dataset. What effects could this have on our ability to detect interactions?"
  },
  {
    "objectID": "posts/2016-12-02-twitter-recommends-stats-books/index.html",
    "href": "posts/2016-12-02-twitter-recommends-stats-books/index.html",
    "title": "Twitter recommends stats books",
    "section": "",
    "text": "Yesterday I asked my beloved Twitter nerds to recommend to me their favourite quantitative texts in Ecology:\n\n\nmy postdoc super tells me I can BUY BOOKS! 💓📚📚💓 quick sweet friends, what are your fun statistical ecology book recommendations?\n\n— Andrew MacDonald (@polesasunder) December 2, 2016\n\n\nAs a way of saying “Thank you!” I thought that I would put all the books in a list for anyone who is curious.\n\nStats texts\n\nStatistics for Spatio-Temporal Data, by Noel Cressie, Christopher K. Wikle, Wiley-Blackwell (19 avril 2011)\nStatistical Rethinking: A Bayesian Course with Examples in R and Stan by Richard McElreath – This one was a HUGE favourite recommended by multiple people. Going straight to the top of my list!\nApplied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS, 1st Edition- by Kery & Royle, Academic Press on the strength of recs from two fine quantitative folks, Auriel Fournier and Petr Keil\nQuantitative Ecology and Evolutionary Biology: Integrating models with data (Oxford Series in Ecology and Evolution) 1st Edition by Otso Ovaskainen, Henrik Johan de Knegt, Maria del Mar Delgado\nBayesian Models - A Statistical Primer for Ecologists (Anglais) Relié – 25 août 2015 - de N Thompson Hobbs , Mevin Hooten\nGeneralized Additive Models: An Introduction with R, Second Edition (Chapman & Hall/CRC Texts in Statistical Science) 2nd Edition by Simon N. Wood\nRegression Modeling Strategies With Applications to Linear Models, Logistic Regression, and Survival Analysis by Harrell, Frank\nGraph Theory and Complex Networks: An Introduction by Maarten van Steen\nData Analysis Using Regression and Multilevel/Hierarchical Models 1st Edition by Andrew Gelman, Jennifer Hill – this one (which I had already) was another very frequent recommendation!\n\n\n\nClassic works:\n\nA Crude Look at the Whole: The Science of Complex Systems in Business, Life, and Society by John H. Miller\nDaedalus, or Science and the Future by JBS Haldane\nEvidence and Evolution: the Logic behind the Science by Elliott Sober\nBiology as Ideology, Lewontin\n\nSo, as Margaret Kosmala of Ec0l0gy b1ts says: what are we reading next?"
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html",
    "title": "Validating a model of selection on plasticity",
    "section": "",
    "text": "library(cmdstanr)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html#no-0-birds",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html#no-0-birds",
    "title": "Validating a model of selection on plasticity",
    "section": "No 0 birds",
    "text": "No 0 birds\nThis system is a little challenging, since we never observe 0 eggs per bird – if a bird cannot lay eggs (e.g. it does not find a nest spot) then it goes uncounted\nTo simulate this, I’ll drop the 0 clutches before doing the rest of the simulations. This means that sample size will be less than or equal to the “nbirds” argument.\n\nsimulate_some_birds_nonzero <- function(nbirds = 57, \n         log_b_date = log(.97),\n         log_avgclutch = log(4.5),\n         logit_psuccess = qlogis(.84)){\n  \n  # simulate arrival dates -- two weeks before and after whatever the average is\n  darrive <- runif(nbirds, min = -14, max = 14) |> round()\n  \n  ## simulate clutch sizes -- decrease  each day\n  clutch <- rpois(nbirds, exp(log_avgclutch + log_b_date*darrive))\n  # drop 0 nests\n  nonzero_clutch <- which(clutch > 0)\n  \n  ## simulate success\n  success <- rbinom(nbirds, size = clutch, prob = plogis(logit_psuccess))\n  \n  return(list(\n    data_list = list(\n      nbirds = length(nonzero_clutch),\n      darrive = darrive[nonzero_clutch], \n      clutch  =  clutch[nonzero_clutch], \n      success = success[nonzero_clutch] \n    ),\n    true_values = tribble(\n      ~variable, ~true_value,\n      \"log_b_date\", log_b_date,\n      \"log_avgclutch\", log_avgclutch,\n      \"logit_psuccess\", logit_psuccess\n    )\n  ))\n}\n\nset.seed(1234)\nsome_nonzeros <- simulate_some_birds_nonzero(nbirds = 200)\n\na plot to confirm that it works:\n\nsome_nonzeros$data_list |> \n  as.data.frame() |> \n  ggplot(aes(x = darrive, y = clutch)) + geom_point()\n\n\n\n\nAnd fit the posterior\n\nplot_posterior_true <- function(simdata, stanmodel){\n  model_post <- stanmodel$sample(data = simdata$data_list,\n                                     refresh = 0, parallel_chains = 4)\n  \n  comparison <- model_post |> \n    tidybayes::tidy_draws() |> \n    tidybayes::gather_variables() |> \n    right_join(simdata$true_values, by = c(\".variable\" = \"variable\"))\n  \n  \n  comparison |> \n    ggplot(aes(y = .variable, x = .value)) + \n    stat_halfeye() + \n    geom_point(\n      aes(y = variable, x = true_value),\n      col = \"orange\",\n      pch = \"|\",\n      size = 10, data = simdata$true_values)\n}\n\n\nplot_posterior_true(some_nonzeros, one_indiv)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n\n\n\n\n\nThere’s already some bias happening! let’s try what happens when the average is lower (and gives more 0s)\n\nset.seed(420)\nsimulate_some_birds_nonzero(log_avgclutch = log(2.4),\n                            log_b_date = log(.7),\n                            nbirds = 300) |> \n  plot_posterior_true(one_indiv)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.7 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.7 seconds.\nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.9 seconds.\n\n\n\n\n\nsome preliminary repetitions show that it usually misses either the average or the hatching success, frequently both.\n\none_indiv_noZero <- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv_noZero.stan\"))\n\none_indiv_noZero\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n}\nmodel {\n  vector[nbirds] alpha = log_avgclutch + log_b_date * darrive;\n  success ~ binomial_logit(clutch, logit_psuccess);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n  clutch ~ poisson_log(alpha);\n  // no zeros -- this normalizes the poisson density for a 0-truncated variable\n  target += -log1m_exp(-exp(alpha));\n}\n\n\n\nset.seed(420)\nsimulate_some_birds_nonzero(log_avgclutch = log(4.4),\n                            log_b_date = log(.9),\n                            nbirds = 300) |> \n  plot_posterior_true(one_indiv_noZero)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.4 seconds.\nChain 2 finished in 1.4 seconds.\nChain 3 finished in 1.4 seconds.\nChain 4 finished in 1.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.4 seconds.\nTotal execution time: 1.5 seconds.\n\n\n\n\n\n\nTruncating using a different syntax\nThe manual uses a different syntax. To write the equation above this way, I found I needed to make a few changes:\n\npoisson_log has to be replaced with poisson and the exp() link function\n\n… that was actually the only change. It runs at the same speed as the previous way of writing it, and gets the same answer:\n\none_indiv_zerotrunc <- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv_zerotrunc.stan\"))\n\none_indiv_zerotrunc\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n}\nmodel {\n  success ~ binomial_logit(clutch, logit_psuccess);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n  vector[nbirds] alpha = log_avgclutch + log_b_date * darrive;\n  clutch ~ poisson(exp(alpha)) T[1,];\n}\n\n\n\nset.seed(420)\nsimulate_some_birds_nonzero(log_avgclutch = log(4.4),\n                            log_b_date = log(.9),\n                            nbirds = 300) |> \n  plot_posterior_true(one_indiv_zerotrunc)\n\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 1.3 seconds.\nChain 1 finished in 1.4 seconds.\nChain 2 finished in 1.4 seconds.\nChain 4 finished in 1.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.4 seconds.\nTotal execution time: 1.6 seconds.\n\n\n\n\n\n\n\n\n\n\n\nTwo parameterizations diverged in a yellow wood\n\n\n\nThese two ways of writing the model both work. Which to choose? Well, I was pleased with myself for manually normalizing the Poisson likelihood in the first model. However the second is clearer to the reader. In the first, it takes two lines of code – not even necessarily next to each other. In the second, the big T for Truncation indicates clearly what is going on. Code is communication; clarity wins.\n\n\n\n\nZero inflated binomial success\nOnce in a while, a nest will just be completely destroyed by, say, a predator. This has nothing to do with anything, probably, and is just a catastrophic Act of Weasel. Let’s imagine that some small proportion of the nests just die:\n\nsimulate_some_birds_nonzero_zeroinflated <- function(nbirds = 57, \n         log_b_date = log(.97),\n         log_avgclutch = log(4.5),\n         logit_psuccess = qlogis(.84),\n         logit_pfail = qlogis(.1)){\n  \n  # simulate arrival dates -- two weeks before and after whatever the average is\n  darrive <- runif(nbirds, min = -14, max = 14) |> round()\n  \n  ## simulate clutch sizes -- decrease  each day\n  clutch <- rpois(nbirds, exp(log_avgclutch + log_b_date*darrive))\n  # drop 0 nests\n  nonzero_clutch <- which(clutch > 0)\n  n_laid <- length(nonzero_clutch)\n  \n  # simulate success -- among birds which laid at least 1 egg, there is a chance of failing completely\n  success_among_nonzero <- rbinom(n_laid,\n                                  size = clutch[nonzero_clutch],\n                                  prob = plogis(logit_psuccess))\n  failed_nests <- rbinom(n_laid, size = 1, prob = plogis(logit_pfail))\n\n  success_zi <- success_among_nonzero * (1 - failed_nests)\n  \n  # success <- rbinom(nbirds, \n  #                   size = clutch,\n  #                   prob = plogis(logit_psuccess))\n  # failed_nests <- rbinom(nbirds, size = 1, prob = plogis(logit_pfail))\n  # \n  # success_zi <- success * (1 - failed_nests)\n  \n  return(list(\n    data_list = list(\n      nbirds = n_laid,\n      darrive = darrive[nonzero_clutch], \n      clutch  =  clutch[nonzero_clutch], \n      success = success_zi#[nonzero_clutch]\n    ),\n    true_values = tribble(\n      ~variable, ~true_value,\n      \"log_b_date\", log_b_date,\n      \"log_avgclutch\", log_avgclutch,\n      \"logit_psuccess\", logit_psuccess,\n      \"logit_pfail\", logit_pfail\n    )\n  ))\n}\n\n\none_indiv_ztrunc_zinf <- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv_ztrunc_zinf.stan\"))\n\none_indiv_ztrunc_zinf\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n  real logit_pfail;\n}\nmodel {\n  logit_pfail ~ normal(-1, .5);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n\n  // Eggs laid -- at least one\n  vector[nbirds] alpha = log_avgclutch + log_b_date * darrive;\n  clutch ~ poisson(exp(alpha)) T[1,];\n\n  // nestling success\n  for (n in 1:nbirds) {\n    if (success[n] == 0) {\n      target += log_sum_exp(\n        log_inv_logit(logit_pfail),\n        log1m_inv_logit(logit_pfail) + binomial_logit_lpmf(0 | clutch[n], logit_psuccess)\n        );\n    } else {\n      target += log1m_inv_logit(logit_pfail) + binomial_logit_lpmf(success[n] | clutch[n], logit_psuccess);\n    }\n  }\n\n}\n\n\n\nset.seed(477)\nsome_zi_birds <- simulate_some_birds_nonzero_zeroinflated(log_avgclutch = log(4.4),\n                            log_b_date = log(.9),\n                            nbirds = 300) \n\nsome_zi_birds|> \n  plot_posterior_true(one_indiv_ztrunc_zinf)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 3.2 seconds.\nChain 4 finished in 3.5 seconds.\nChain 3 finished in 3.6 seconds.\nChain 1 finished in 3.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.5 seconds.\nTotal execution time: 3.8 seconds.\n\n\n\n\n\nI initially failed to recover the parameter for logit_pfail. Here I some things I learned:\n\nSimulating zero-inflated numbers can be tricky! I flipped back and forth between simulating 0-inflation for all nests, and simulating for only those with at least 1 egg. In retrospect, it is clear that these are equivalent. There are two independent things here: the probability of a clutch having 0 eggs and the probability of a 0 coming from the zero-inflated binomial.\nthis zero-inflated model is quite sensitive to the prior. That’s because there are not very many zeros being inflated: in my simulation at least, failed nests are rare and success is naturally low. it would be pretty important to decide in advance if sudden nest failure (e.g. by predation) is rare or common"
  },
  {
    "objectID": "posts/30-03-2023-growth-continuous-discrete/index.html",
    "href": "posts/30-03-2023-growth-continuous-discrete/index.html",
    "title": "Should we model growth as continuous or discrete",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/30-03-2023-growth-continuous-discrete/index.html#how-do-things-grow-or-decay",
    "href": "posts/30-03-2023-growth-continuous-discrete/index.html#how-do-things-grow-or-decay",
    "title": "Should we model growth as continuous or discrete",
    "section": "How do things grow (or: decay)",
    "text": "How do things grow (or: decay)\nLet’s start with a simple simulation:\n\nstart_size <- 45\ndecay_rate <- .3\ntibble(time = 0:20, \n       mass = start_size * exp(-decay_rate * time)) |> \n  ggplot(aes(x = time, y = mass)) + \n  geom_point()"
  },
  {
    "objectID": "posts/2022-11-21-growth-curve-known-age/index.html",
    "href": "posts/2022-11-21-growth-curve-known-age/index.html",
    "title": "Simple nonlinear growth",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(brms))"
  },
  {
    "objectID": "posts/2022-11-21-growth-curve-known-age/index.html#growth-when-you-know-the-age",
    "href": "posts/2022-11-21-growth-curve-known-age/index.html#growth-when-you-know-the-age",
    "title": "Simple nonlinear growth",
    "section": "Growth when you know the age",
    "text": "Growth when you know the age\nWe’re doing a lot of work with growth equations these days! This is how to use brms to fit the growth of an animal when we know:\n\nthe birth year\nsize at each year (measured as the length of a leg)\ntime\n\nWe’ll start with the classic VB growth equation that has been in several other posts:\n\\[\nL_t = L_0e^{-rt} + L_\\infty(1 - e^{-rt})\n\\tag{1}\\]\nThe model we use will resemble the others as well:\n\\[\n\\begin{align}\n\\text{Measurements} &\\sim \\text{Normal}(L_t, \\sigma_{meas})\\\\\nL_t &= L_0e^{-rt} + L_\\infty(1 - e^{-rt}) \\\\\nL_0 &\\sim ...\\\\\nL_\\infty &\\sim ...\\\\\nr &\\sim ...\n\\end{align}\n\\]\n\nSimulating data\nI’m starting off with a function to simulate data; this will make it easy to repeat experiments with this model.\n\nsimulate_one_growth_known_age <- function(age, r,\n                                          Lmax,\n                                          size_at_first,\n                                          sd_obs){\n  tibble(age = 0:age,\n         size = size_at_first * exp(-r * age) + Lmax * (1 - exp(-r * age)),\n         obs_size = rnorm(n = length(age), mean = size, sd = sd_obs))\n}\n\n\none_animal <- simulate_one_growth_known_age(9, Lmax = 550, size_at_first = 277, r = .7, sd_obs = 6)\n\none_animal |> \n  ggplot(aes(x = age, y = obs_size)) + \n  geom_point() + \n  theme_bw() + \n  geom_line(aes(y = size)) + \n  labs(x = \"Age\", y = \"Size\") \n\n\n\n\nGrowth curve for a single individual. The curved line is the true size, and the dots are observations around it. The observations are taken in the field while the semi-tranquilized animal is struggling, so they show some slight variation."
  },
  {
    "objectID": "posts/2022-11-21-growth-curve-known-age/index.html#nonlinear-modelling-with-brms",
    "href": "posts/2022-11-21-growth-curve-known-age/index.html#nonlinear-modelling-with-brms",
    "title": "Simple nonlinear growth",
    "section": "Nonlinear modelling with BRMS",
    "text": "Nonlinear modelling with BRMS\nThere are three steps to defining and elementary model with brms:\n\nwrite the model\nwrite down some priors\ncondition the model on data\n\nIn practice there are many more steps, including prior predictive checks to make sure our priors make sense. In this post I’m going to focus on the mechanistic how-to of fitting a nonlinear model in brms and I’ll come back to Prior Predictive checks, which I love, in another post.\nFirst we define the model, here we need to indicate what are parameters by doing a ~1 after each. Yes it is a formula with multiple little formulae inside it! Feel the power flow through you.\n\nvb_form <- bf(obs_size ~ startsize * exp(-growthrate * age) + maxsize * (1 - exp(-growthrate * age)),\n              startsize ~ 1, \n              growthrate ~ 1,\n              maxsize ~ 1,\n              nl = TRUE,\n              family = gaussian())\n\nget_prior(vb_form, data = one_animal)\n\n                 prior class      coef group resp dpar      nlpar lb ub\n student_t(3, 0, 22.6) sigma                                       0   \n                (flat)     b                           growthrate      \n                (flat)     b Intercept                 growthrate      \n                (flat)     b                              maxsize      \n                (flat)     b Intercept                    maxsize      \n                (flat)     b                            startsize      \n                (flat)     b Intercept                  startsize      \n       source\n      default\n      default\n (vectorized)\n      default\n (vectorized)\n      default\n (vectorized)\n\nvb_prior <- c(\n  prior(exponential(1), class = \"sigma\"),\n  prior(normal(0,1), nlpar = \"growthrate\", lb = 0),\n  prior(normal(550, 20), nlpar = \"maxsize\", lb = 0),\n  prior(normal(200, 50), nlpar = \"startsize\", lb = 0)\n)\n\nvb_model <- brm(formula = vb_form,\n                prior = vb_prior, \n                data = one_animal, \n                chains = 2, \n                file = here::here(\"posts/2022-11-21-growth-curve-known-age/vb_model.rds\"))\n\n\none_animal |> \n  tidybayes::add_predicted_rvars(vb_model) |> \n  ggplot(aes(x = age, dist = .prediction)) + \n  stat_dist_lineribbon() + \n  geom_point(aes(x = age, y = obs_size), inherit.aes = FALSE) \n\nWarning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Unknown or uninitialised column: `linewidth`.\n\n\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Unknown or uninitialised column: `linewidth`.\nUnknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\nsummary(vb_model)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: obs_size ~ startsize * exp(-growthrate * age) + maxsize * (1 - exp(-growthrate * age)) \n         startsize ~ 1\n         growthrate ~ 1\n         maxsize ~ 1\n   Data: one_animal (Number of observations: 10) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nstartsize_Intercept    275.83      4.47   266.81   284.49 1.00     1050\ngrowthrate_Intercept     0.66      0.03     0.61     0.72 1.00      918\nmaxsize_Intercept      551.62      2.43   546.94   556.62 1.00      814\n                     Tail_ESS\nstartsize_Intercept       866\ngrowthrate_Intercept      796\nmaxsize_Intercept         765\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.51      0.96     3.05     6.76 1.00      899      951\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/2022-12-05-growth-increment/index.html",
    "href": "posts/2022-12-05-growth-increment/index.html",
    "title": "Uncertainty in growth increments",
    "section": "",
    "text": "The following is a simulation by Will Vieira which explores how process uncertainty and measurement error combine when we measure trees.\nThe model imagines a simple linear growth scenario: every year, individuals grow by a random amount \\(g_i\\). This growth increment is random and varies each year according to a normal distribution\n\\[\n\\begin{align}\nL_{\\text{year}[i]} \\sim N \\\\\n\\end{align}\n\\]\n\\[\nL = lo\n\\]\nHere is Will’s very clean and concise simulation code.\nbut why are these correlated?"
  },
  {
    "objectID": "posts/2022-12-05-growth-increment/index.html#distribution-of-a-difference",
    "href": "posts/2022-12-05-growth-increment/index.html#distribution-of-a-difference",
    "title": "Uncertainty in growth increments",
    "section": "Distribution of a difference",
    "text": "Distribution of a difference\n\nu1 <- 8\nu2 <- 15\nsdx <- 1.5\nn <- 1e5\nx1 <- rnorm(n, mean = u1, sd = sdx)\nx2 <- rnorm(n, mean = u2, sd = sdx)\ntibble(diff = x2 - x1) |> \n  ggplot(aes(x = diff)) + \n  stat_function(fun = \\(x) dnorm(x, mean = u2 - u1, sd = sqrt(2) * sdx),\n                size = 3, col = \"darkgreen\") + \n  geom_density(size = 1, col = \"orange\")\n\n\n\n\nThis is what happens when you have two constant values which are measured with error and then contrasted.\nif another process adds to the variation, then the two variances add – THEN get scaled but sqrt(2)"
  },
  {
    "objectID": "posts/2022-12-01-cholesky-correlation/index.html",
    "href": "posts/2022-12-01-cholesky-correlation/index.html",
    "title": "The Cholesky decomposition",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2022-12-01-cholesky-correlation/index.html#code",
    "href": "posts/2022-12-01-cholesky-correlation/index.html#code",
    "title": "The Cholesky decomposition",
    "section": "code",
    "text": "code\n\nplot(density(tan(runif(5000,min = 0, max = pi/2))*.3, from = 0), \n     xlim = c(0, 100))\n\ncurve(dexp(x, rate = 3), add = TRUE, col = \"red\", xlim = c(0, 100))\n\n\n\ncurve(tan(x), xlim = c(-5,5))\n\n\n\nhist(-log(runif(200)), probability = TRUE)\ncurve(dexp(x), add = TRUE)\n\n\n\neg <- rethinking::rlkjcorr(1, 2, 1)\ncc <- chol(eg)\n\npurrr::rerun(30,{\n  zz <- matrix(data = rnorm(500), ncol = 2)\n  # plot(zz)\n  rr <- t(cc) %*% t(zz)\n  # plot(t(rr))\n  cor(t(rr))[1,2]\n}) |> \n  purrr::flatten_dbl() |> density() |> plot()\nabline(v=eg[1,2])\n\n\n\n#' modelling the plasticity slopes (xik - xbar) for each evironmental variable?\n#' what's correlated here? slopes and intercepts\n\neg <- rethinking::rlkjcorr(1, 2, 5)\ncc <- chol(eg)\n\nt(cc) %*% cc\n\n          [,1]      [,2]\n[1,] 1.0000000 0.6450641\n[2,] 0.6450641 1.0000000\n\nmm <- matrix(rnorm(4000, mean = 0, sd = 1), ncol = 2)\n\nplot(mm)\n\n\n\nyy <- t(t(cc) %*% t(mm))\n\nplot(yy)\n\n\n\ncor(yy)\n\n          [,1]      [,2]\n[1,] 1.0000000 0.6548221\n[2,] 0.6548221 1.0000000\n\npurrr::rerun(50,{\n  zz <- matrix(data = rnorm(500), ncol = 2)\n  # plot(zz)\n  rr <- t(cc) %*% t(zz)\n  # plot(t(rr))\n  cor(t(rr))[1,2]\n}) |> \n  purrr::flatten_dbl() |> density() |> plot()\nabline(v=eg[1,2])\n\n\n\np <- -.8\nL <- matrix(c(1,0, p, sqrt(1 - p^2)), ncol = 2, byrow = TRUE)\nzz <- matrix(data = rnorm(1000), ncol = 2)\nyy <- t(L %*% t(zz))\nplot(yy)\n\n\n\ncor(yy)\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.7913319\n[2,] -0.7913319  1.0000000\n\n\nAnother way to make correlation matrices is here: https://www.rdatagen.net/post/2023-02-14-flexible-correlation-generation-an-update-to-gencorgen-in-simstudy/"
  },
  {
    "objectID": "posts/2016-12-07-population-growth-functional-programming/index.html",
    "href": "posts/2016-12-07-population-growth-functional-programming/index.html",
    "title": "Population growth with functional programming",
    "section": "",
    "text": "Today I want to tell you about an approach for functional programming in R – and then apply it to studying population growth!\nI have been studying some of the purrr functions lately. They are a useful family of functions for performing two common tasks in R: manipulating lists and altering the behaviour of functions. If you’d like a high-quality guide to this group of functions, set aside some time to work through Jenny Bryan’s excellent tutorial and Hadley Wickham’s chapter on Lists.\nI was inspired to write this post after reading This StackOverflow question by jebyrnes. He asks:\nand there is! An answerer mentioned purr::accumulate(). In this post I’m going to expand on the approach they suggest. accumulate, and its twin reduce, are examples of functionals – functions that take functions as their arguments, and manipulate their behaviour in some way. purrr::accumulate is a wrapper around to Reduce from the base package, with the argument accumulate = TRUE.\naccumulate is normally used when you want to do some cumulative function all along a vector. For example, we can reproduce the cumulative some of a vector like this (same output as cumsum(1:10))\n.x and .y here are just a handy way of writing “the first thing” and “the second thing”. Then accumulate goes down the vector 1:10, and takes the first thing (1) adds it to the second thing (2) and so on….\nHowever, this is not the only way it works! accumulate can take an initial value (.init) and can work on a dummy variable. If its starting function does nothing but modify an element, it will just keep modifying it: so instead of f(1, 2), f(f(1, 2), 3) we get f(.init), f(f(.init)) etc:\nClearly, the 0s are not involved in any calculation (the answer would be 0!). instead, you just get the starting value multiplied by 1.5 each time!\nThis already suggests an awesome biological interpretation: logistic population growth.\nOn thing I like about this is that is is to much easier to look at – it look like the common biological equation for population growth:\n\\[\nN_{t+1} = r*N_t\n\\]"
  },
  {
    "objectID": "posts/2016-12-07-population-growth-functional-programming/index.html#so-is-this-useful",
    "href": "posts/2016-12-07-population-growth-functional-programming/index.html#so-is-this-useful",
    "title": "Population growth with functional programming",
    "section": "So, is this useful?",
    "text": "So, is this useful?\nI wonder if this might be an interesting pedagogical tool. I feel like it might place the emphasis a bit differently to for-loops. Perhaps a for loop emphasizes the passage of time – how, at each time step (each i for example) certain things happen in a certain order (the Resource grows, the Predator kills some, then some predators die, etc). On the other hand, I feel like the functional programming approach emphasizes how a population (or pair of populations) is transformed. Each little function has some parameters – which are either, constant, varying, and/or influenced by something besides population size – and each little function does only one thing – transform the population between one time step and the next."
  },
  {
    "objectID": "posts/2022-12-15-setting-priors/index.html",
    "href": "posts/2022-12-15-setting-priors/index.html",
    "title": "Setting priors on hierarchical multivariate models",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2022-12-15-setting-priors/index.html#what-is-this-post",
    "href": "posts/2022-12-15-setting-priors/index.html#what-is-this-post",
    "title": "Setting priors on hierarchical multivariate models",
    "section": "What is this post",
    "text": "What is this post\n\ndemo_data <- structure(list(csize = c(5, 5, 5), \n                            age_morpho_indic = c(1, 1, \n1), mass = c(20.64, 25.58, 20.2), log_mass = c(3.02723094061336, \n3.2418107961507, 3.00568260440716), dponte = c(134, 135, 136), \n    annee = c(2005, 2006, 2005), ferme = c(9, 9, 9), idF1 = c(188197003, \n    188197004, 188197004), general_csize = c(8.972, 9.344, 8.972\n    ), coldsnap_csize = c(3.9, 7.15, 3.9), general_dponte = c(8.704651163, \n    10.48604651, 8.704651163), coldsnap_dponte = c(2.3, 4.7, \n    2.3), density_TRSW = c(8, 9, 8), density_HOSP = c(0, 0, 0\n    ), paysa_ext = c(0.224174146, 0.223943415, 0.224166189), \n    general_mean_csize = c(-0.482520728753587, -0.343044978530785, \n    -0.343044978530785), difference_general_csize = c(0, 0.186, \n    -0.186), coldsnap_mean_csize = c(-1.09248466640163, -0.156587649057671, \n    -0.156587649057671), difference_coldsnap_csize = c(0, 1.625, \n    -1.625), general_mean_dponte = c(-2.54912170305933, -1.57828050221117, \n    -1.57828050221117), difference_general_dponte = c(0, 0.890698, \n    -0.890698), coldsnap_mean_dponte = c(-1.63910623622377, -0.951828417972457, \n    -0.951828417972457), difference_coldsnap_dponte = c(0, 1.2, \n    -1.2), density_TRSW_mean = c(0.248353096875404, 0.484967912059555, \n    0.484967912059555), difference_density_TRSW = c(0, 0.5, -0.5\n    ), paysa_ext_mean = c(-0.306307397416911, -0.308050462721618, \n    -0.308050462721618), difference_paysa_ext = c(-0.000913607068423686, \n    -0.0152243604543345, 0.0133971463174872), density_HOSP_mean = c(-0.525930668145508, \n    -0.525930668145508, -0.525930668145508), difference_density_HOSP = c(0, \n    0, 0), noisenvol = c(5, 5, 0)), row.names = c(NA, -3L), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"))\n\n\nlibrary(brms)\ndponte_model_bf_2 = bf(dponte ~ 1 + age_morpho_indic + general_mean_dponte + difference_general_dponte + (1|annee) + (1|ferme) + \n                         (1 + difference_general_dponte|f|idF1),\n                       family = gaussian(), center = FALSE)\n\n \n\ncsize_model_bf_2 = bf(csize ~ 1 + age_morpho_indic + general_mean_csize + difference_general_csize + (1|annee) + (1|ferme) + \n                        (1 + difference_general_csize|f|idF1),\n                      family = poisson(), center = FALSE)\n\n \n\nsucess_model_bf_2 = bf(noisenvol ~ 1 + (1|f|idF1) + (1|annee) + (1|ferme),\n                       family = poisson(), center = FALSE)\n\n# combine all three into one model\nfull_model_bf <- dponte_model_bf_2 + csize_model_bf_2 + sucess_model_bf_2\n\nget_prior(full_model_bf, data = demo_data)\n\nfull_model_prior <- c(\n  ## individual level correlations\n  prior(lkj(3), class = \"cor\", group = \"idF1\"),\n  ## clutch size model\n  prior(normal(0,1),    class = \"b\", resp = \"csize\"),\n  # prior(normal(0,1),    class = \"Intercept\", resp = \"csize\"),\n  prior(exponential(1), lb = 0, class = \"sd\", resp = \"csize\"),\n  ## laying date model\n  prior(normal(0,1),    class = \"b\",         resp = \"dponte\"),\n  # prior(normal(0,1),    class = \"Intercept\", resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sd\",   resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sigma\", resp = \"dponte\"),\n  # fitness -- no slopes here\n  # prior(normal(0,1),    class = \"Intercept\", resp = \"noisenvol\"),\n  prior(exponential(1), lb = 0, class = \"sd\",        resp = \"noisenvol\")\n)\n\n# Run fuul model\nfull_model_prior_predict = brm(full_model_bf,\n                    data = demo_data,\n                    prior = full_model_prior,\n                    cores = 4, chains = 4, \n                    sample_prior = \"only\")\n\n \n\nsummary(full_model_prior_predict)\n\n\nlibrary(tidybayes)\n\n# noisenvol\nprior_predictions <- demo_data |> \n  add_predicted_rvars(object = full_model_prior_predict, resp = \"noisenvol\")\n\nprior_predictions |> glimpse()\n\nlibrary()\nprior_predictions |> \n  select(idF1, noisenvol, .prediction) |> \n  ungroup() |> \n  ggplot(aes(y = idF1, xdist = .prediction)) + \n  stat_halfeye() + \n  coord_cartesian(xlim =c(0,1e5))\n\nalready we can see that this is probably way too wide!\n\nfull_model_smaller_noisenvol_prior <- c(\n  ## individual level correlations\n  prior(lkj(3), class = \"cor\", group = \"idF1\"),\n  ## clutch size model\n  prior(normal(0,1),    class = \"b\", resp = \"csize\"),\n  prior(exponential(1), lb = 0, class = \"sd\", resp = \"csize\"),\n  ## laying date model\n  prior(normal(138,5),    class = \"b\",         resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sd\",   resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sigma\", resp = \"dponte\"),\n  # fitness -- no slopes here\n  prior(normal(1, 0.1),    class = \"b\", resp = \"noisenvol\"),\n  prior(exponential(4), lb = 0, class = \"sd\",        resp = \"noisenvol\")\n)\n\n\n\n# Run fuul model\nfull_model_noisenvol_prior_predict = brm(full_model_bf,\n                    data = demo_data,\n                    prior = full_model_smaller_noisenvol_prior,\n                    cores = 4, chains = 4, \n                    sample_prior = \"only\")\n\n \n\ndemo_data |> \n  add_predicted_rvars(object = full_model_noisenvol_prior_predict, resp = \"noisenvol\") |> \n  select(idF1, noisenvol, .prediction) |> \n  ungroup() |> \n  ggplot(aes(y = idF1, xdist = .prediction)) + \n  stat_halfeye() + \n  coord_cartesian(xlim =c(0,12))\n\n\n# age_morpho_indic + general_mean_dponte + difference_general_dponte\none_female <- tibble(difference_general_dponte = c(-3,0, 3),\n                     age_morpho_indic = c(0,1,1))\n\nfake_data <- expand_grid(one_female,\n                         nesting(general_mean_dponte = c(-2,0,2),\n                                 idF1 = letters[1:3])) |> \n  arrange(general_mean_dponte) |> \n  mutate(ferme = \"f\",\n         annee = \"y\")\n\ndponte_prior_pred <- fake_data |> \n  add_predicted_draws(object = full_model_noisenvol_prior_predict, \n                      resp = \"dponte\", allow_new_levels = TRUE, ndraws = 3)\n\n\ndponte_prior_pred |> \n  ggplot(aes(x= difference_general_dponte, y = .prediction, group = .draw)) + \n  geom_line() + \n  facet_wrap(~general_mean_dponte)"
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html",
    "title": "Continuous fractions with Map and Reduce",
    "section": "",
    "text": "Over the summer, some of us here at UBC started a reading group based around Hadley Wickham’s book, Advanced R Programming. The goal was to compare our answers to the exercises and our impressions of the content.\nWe recently read my favourite chapter, Functionals, where readers are challenged to read about some algorithms in Structure and Interpretation of Computer Programs, and implement it in R.\nI wanted to share some functions I wrote to calculate these exotic things called k-term finite continued fractions, based on that challenge:"
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#continued-fractions",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#continued-fractions",
    "title": "Continuous fractions with Map and Reduce",
    "section": "continued fractions",
    "text": "continued fractions\nContinued Fractions look like this:\n\\[\n\\frac{n_1}{d_1 + \\frac{n_2}{d_2 + \\frac{n_3}{d_3 + \\cdots } } }\n\\]\nand have an infinite number of \\(n\\) and \\(d\\) values. However, if after \\(k\\) values you just replace the remaining ones (the \\(...\\) above) with 0, then you get a k-term finite continued fraction, which is maybe close enough:\n\\[\n\\frac{n_1}{d_1  + \\frac{n_2}{\\ddots + \\frac{n_k}{d_k}}}\n\\]\nContinued fractions have the values of \\(n\\) and \\(d\\) defined by a series. We need a function that will take the series for the numerator and denominator, calculate \\(k\\) terms, and put them together into a continued fraction.\nSo, how do we calculate this in R? Well, it turns out we can apply two concepts we learned from Wickham’s book to do so: closures and functionals.\n\nCreating a series of closures\nWe could say that each “part” of the continued fraction was a unit that looks like\n\\[\n\\frac{n}{d + x}\n\\]\nwhere \\(n\\) and \\(d\\) are a “pair” of numerator and denominator, and \\(x\\) is the next “part”, and so on. (there are probably mathematical terms for these, but I’m an ecologist, not a mathematician!). If you “build” the continued fraction from the inside out, you’d start with \\(x = 0\\), and calculate \\(\\frac{n_k}{d_k}\\). Then you move on to \\(n_{k-1}\\); for this fraction you have the numerator-denominator pair, plus the term (\\(x\\)) which you just calculated.\nWe can use closures to calculate each of these “parts” in turn, in order to keep the numerator-denominator pairs together. Closures are functions which are created by other functions; they “enclose” the environment in which they were created (hence the name), which means they can use variables from that environment (in our case, the values of a numerator & denominator)\nFirst, we make a “function factory”, a function which creates other functions (closures which retain different values of \\(n\\) and \\(d\\)):\n\nfrac_maker <- function(n, d){\n  force(n)\n  force(d)\n  function(x) n / (d + x)\n  }\n\nThis function takes a pair of numbers and defines a new function which uses them. But how can we create lots of closures, one for every numerator-denominator “pair” between 1 and \\(k\\)? We can use the function Map to run this function on each variable pair. Map works like a zipper, combining the first elements of two (or more) vectors with a function, then the second, etc. For example, the reciprocal of the Golden Ratio is the result of a continued fraction where \\(n\\) and \\(d\\) are both 1:\n\n  Ns <- rep(1, 20)\n  Ds <- rep(1, 20)\n  funs <- Map(frac_maker, Ns, Ds)\n\nfuns is now a list of functions, each one remembering its own particular value of \\(n\\) or \\(d\\). Now all we need to do is put them together and run them all. For that, we need another functional: Reduce."
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#using-reduce",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#using-reduce",
    "title": "Continuous fractions with Map and Reduce",
    "section": "using Reduce",
    "text": "using Reduce\nReduce is just lovely. It takes a vector and “reduces” it to a single number by applying a function: the first two arguments are the first and second vector elements, then the result of that calculation and the third element, then that result and the fourth element:\nReduce(sum, c(1, 2, 3, 4)) = sum(sum(sum(1, 2), 3), 4)\nHere we have a list of functions, not values, so we use Reduce to run a function that simply executes its second argument on its first:\n\nanswer <- Reduce(function(f1,f2) f2(f1), x = funs, init = 0)\n## take reciprocal to get the Golden Ratio:\n1/answer\n\n[1] 1.618034\n\n\nWe start with the value of 0, because as we said the approximation of the continuous fraction simply replaces all the the “parts” after \\(k\\) with 0. So our function runs the first function on 0, the second function on that result, the third function on that result, etc. The result is the whole approximation of a continuous fraction, “built” from the inside out."
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#combine-into-a-function",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#combine-into-a-function",
    "title": "Continuous fractions with Map and Reduce",
    "section": "combine into a function",
    "text": "combine into a function\nSo now we combine this to form a single function that calculates the value of a continuous series for \\(k\\) terms:\n\ncontinuous_frac <- function(Ns, Ds, frac_fun = frac_maker){\n  Ns <- rev(Ns)\n  Ds <- rev(Ds)\n  funs <- Map(frac_fun, Ns, Ds)\n  Reduce(function(f1,f2) f2(f1), x = funs, init = 0)\n  }\n\nNote that we have to reverse the series Ns and Ds, simply because the series are usually defined from \\(n_1\\) to \\(n_k\\), but we are building our function from \\(n_k\\) backwards."
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-numbers",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-numbers",
    "title": "Continuous fractions with Map and Reduce",
    "section": "calculating numbers",
    "text": "calculating numbers\nWith this function in hand, we can approximate any continued fraction. Here are a few examples:\n\nThe value of \\(e\\) (for biological content)\nAre you wondering what all this has to do with biology? Well, Euler’s number certainly appears in plenty of biological models, so let’s calculate it:\n\ndenominator <- function(k){\n  nums <- lapply(seq_len(k)*2, function(x) c(1, 1, x))\n  out <- do.call(c, nums)\n  out[-1]\n}\n\n2 + continuous_frac(Ds = denominator(20), Ns = rep(1, 3 * 20 -1))\n\n[1] 2.718282"
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-pi",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-pi",
    "title": "Continuous fractions with Map and Reduce",
    "section": "calculating \\(\\pi\\)",
    "text": "calculating \\(\\pi\\)\nThere are several ways to calculate \\(\\pi\\), based on different forms of this equation. Apparently they converge at different rates. here are some examples:\n\nleibniz <- function(k){\n  seqs <- seq(from = 1, by = 2, length.out = k -1)\n  N <- c(4, seqs ^ 2)\n  D <- c(1, rep(2, k-1))\n  continuous_frac(N, D)\n}\n\nsomayaji <- function(k){\n  N <- seq(from = 1, by = 2, length.out = k) ^ 2\n  D <- rep(6, k)\n  3 + continuous_frac(N, D)\n}\n\nlinear <- function(k){\n  N <- seq(from = 1, by = 1, length.out = k -1) ^ 2\n  N <- c(4, N)\n  D <- seq(from = 1, by = 2, length.out = k)\n  continuous_frac(N, D)\n}\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndata.frame(ks = seq(2, 100, by = 5)) %>%\n  rowwise %>%\n  mutate(leibniz = leibniz(ks),\n         somayaji = somayaji(ks),\n         linear = linear(ks)) %>%\n  gather(method, pi_value, -ks) %>%\n  ggplot(aes(x = ks, y = pi_value, colour = method)) + geom_point() + geom_path() + theme_bw()\n\n\n\n\nIf we zoom in we can see that the third form outperforms Somayaji’s:\n\ndata.frame(ks = seq(5, 10, by = 1)) %>%\n  rowwise %>%\n  mutate(somayaji = somayaji(ks),\n         linear = linear(ks)) %>%\n  gather(method, pi_value, -ks) %>%\n  ggplot(aes(x = ks, y = pi_value, colour = method)) + geom_point() + geom_path() + xlab(\"k\") + ylab(expression(pi)) + theme_bw()"
  },
  {
    "objectID": "posts/2022-11-29-selection-on-plasticity/index.html",
    "href": "posts/2022-11-29-selection-on-plasticity/index.html",
    "title": "Selection on plasticity",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\n# source(\"posts/2022-11-29-selection-on-plasticity/functions.R\")\n## functions for measuring selection on plasticity\nsimulate_many_moms <- function(pop_average_dponte = 138,\n                               mom_quality_max = 4,\n                               quality_on_dponte = 2,\n                               quality_on_csize = .2,\n                               n_females = 42,\n                               lifespan = 5,\n                               temp_range  = c(2, 12)) {\n\n  general_temp <- runif(lifespan, temp_range[1], max = temp_range[2])\n\n  general_temp_c <- general_temp - mean(general_temp)\n\n  mom_qualities <- runif(n_females, min = 0, max = 4)\n\n  many_moms_temperature <- expand_grid(year = 1:lifespan,\n                                       idF1 = 1:n_females) |>\n    mutate(mom_quality = mom_qualities[idF1],\n           general_temp = general_temp[year],\n           general_temp_c = general_temp_c[year],\n           ## adding the biology\n           ## Effect of temperature -- does it depend on quality? let's say that it DOES (for now)\n           effet_temp_dponte_qual = -.7*mom_quality,\n           effet_temp_csize_qual = .1*log(mom_quality),\n           # csize\n           mom_avg_csize = log(pop_average_csize) +  quality_on_csize*log(mom_quality),\n           temp_avg_csize = exp(mom_avg_csize + effet_temp_csize_qual*general_temp_c),\n           # dponte\n           mom_avg_dponte = pop_average_dponte + quality_on_dponte*mom_quality,\n           temp_avg_dponte = mom_avg_dponte + effet_temp_dponte_qual*general_temp_c,\n           ## observations\n           obs_csize = rpois(n = length(year), lambda = temp_avg_csize),\n           obs_dponte = rnorm(n = length(year), mean = temp_avg_dponte, sd = 3) |> round()\n    )\n  return(many_moms_temperature)\n}"
  },
  {
    "objectID": "posts/2022-11-29-selection-on-plasticity/index.html#model-for-measuring-selection-directly",
    "href": "posts/2022-11-29-selection-on-plasticity/index.html#model-for-measuring-selection-directly",
    "title": "Selection on plasticity",
    "section": "Model for measuring selection directly:",
    "text": "Model for measuring selection directly:\n\ntrue_corr_plasticity_avg <- .7\nsd_avg <- .9\nsd_plasticity <- .3\nn_females <- 47\n\ncorrmat <- matrix(c(1, true_corr_plasticity_avg, true_corr_plasticity_avg, 1),\n       byrow = TRUE, ncol = 2)\n\nvar_covar <- diag(c(sd_avg, sd_plasticity)) %*% corrmat %*% diag(c(sd_avg, sd_plasticity))\n\nfemale_avg_and_plasticity <- MASS::mvrnorm(\n  n = n_females,\n  mu = c(0,0),\n  Sigma = var_covar)\n\nWe have the average and the slope for each female.\nFor a bit of extra realism, lets simulate several years and let the females belong to different cohorts. for simplicity, lets say each female lives for\n\ntwenty_years_environment <- runif(20, min = -3, max = 3)\n\nfemale_start_years <- sample(1:16, size = n_females, replace = TRUE)\nlibrary(tidyverse)\n\ndf <- tibble(\n  female_id = rep(1:n_females, each = 4),\n  year_id = rep(female_start_years, each = 4) + 0:3,\n  env = twenty_years_environment[year_id]\n)\n\ndf$env |> mean()"
  },
  {
    "objectID": "posts/9999-template/index.html",
    "href": "posts/9999-template/index.html",
    "title": "Andrew tries Quarto",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/9999-template/index.html#what-is-this-post",
    "href": "posts/9999-template/index.html#what-is-this-post",
    "title": "Andrew tries Quarto",
    "section": "What is this post",
    "text": "What is this post\nTwo things:\n\na living notebook of my observations and notes in working with Quarto\nA simple template for new posts – when I make a new post I’ll start by copying and pasting this current one over. Probably in the future there will be a plugin that does this for us, but until then this will be fine."
  },
  {
    "objectID": "posts/9999-template/index.html#why-i-switched-to-quarto",
    "href": "posts/9999-template/index.html#why-i-switched-to-quarto",
    "title": "Andrew tries Quarto",
    "section": "Why I switched to Quarto",
    "text": "Why I switched to Quarto\nA simple and frequent answer: overenthusiasm! I like seeing all the new things that the Rstudio Posit team develop, and I know that the vibrant R community community will keep adding features and tutorials"
  },
  {
    "objectID": "posts/9999-template/index.html#quarto-resources",
    "href": "posts/9999-template/index.html#quarto-resources",
    "title": "Andrew tries Quarto",
    "section": "quarto resources",
    "text": "quarto resources\n\nthe Ultimate Guide to starting a Quarto blog\nquarto discussions\nDanielle Navarro’s comments on the topic\nNick Tierney’s notes\nand of course Nicks exciting book project!"
  },
  {
    "objectID": "posts/9999-template/index.html#surprise-its-targets",
    "href": "posts/9999-template/index.html#surprise-its-targets",
    "title": "Andrew tries Quarto",
    "section": "surprise it’s targets",
    "text": "surprise it’s targets\n\n\n\n\n\n\n\n\nNote\n\n\n\nBelow I describe my attempt to start a whole-blog workflow using targets. However, this got unwieldly very fast! My new plan is to use a targets workflow on a post-by-post basis, using\n\n\n\nvia GIPHY\n\nI’m also using targets. Here are some observations on that so far:\n\nLast error: ! System command 'quarto' failed I get this error message more that 10x more often than any other. As I’m learning Quarto I keep making errors which break the Quarto process but targets doesn’t (yet?) communicate the specific error message. To find out what has gone wrong, I go over to the terminal and run quarto render .\nyou have to go into _metadata.yml and stop the posts from freezing, by setting freeze: false . Targets will handle all the rest of it.\n\nI’m using targets with quarto because I want to work with large bayesian models fit with stan and brms., and fitting them in a regular blog post – including compiling, sampling etc."
  },
  {
    "objectID": "posts/9999-template/index.html#my-own-observations-and-questions",
    "href": "posts/9999-template/index.html#my-own-observations-and-questions",
    "title": "Andrew tries Quarto",
    "section": "My own observations and questions",
    "text": "My own observations and questions\nso far it is very straightforward!\n\nit seems like post folder cant begin with dates? you can start a folder with a date, and I do, so that the posts sort in at least approximately the right temporal sequence. However the date for a post comes from the YAML, not the folder name.\nlove LOVE the bibliography automatically appearing\nDO think about changing the working directory for chunk evaluations to the project root, as described here. This is really important because that is the directory that targets uses when sourcing files. For example tar_load(my_target_name) will load in an object from your pipeline. But run this same line from an .qmd file in a subfolder (say posts/yourpostname/index.qmd), then THAT will be the location the computer looks in.."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew MacDonald, PhD",
    "section": "",
    "text": "I love insects, statistics, but most of all I like solving interesting problems in a collaborative environment\nI am currently a Research Associate at the Université de Sherbrooke. My goal is to improve our ability to make predictions for ecological systems, and to build a community of practice around statistical methods in ecology."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Andrew MacDonald, PhD",
    "section": "Education",
    "text": "Education\n\nPhD, University of British Columbia, 2017\nMSc, University of Toronto, 2009\nBSc, Cape Breton University, 2006"
  },
  {
    "objectID": "index.html#selected-publications",
    "href": "index.html#selected-publications",
    "title": "Andrew MacDonald, PhD",
    "section": "Selected publications",
    "text": "Selected publications"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Study of the Household",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nLatent continuous variables\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nModelling continuous latent states in Stan.\n\n\n\n\n\n\nMar 20, 2023\n\n\nAndrew MacDonald\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValidating a model of selection on plasticity\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nPlus ça change, plus c’est la change qui change\n\n\n\n\n\n\nFeb 2, 2023\n\n\nAndrew MacDonald\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe evolution of plasticity\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nbrms\n\n\nsimulation\n\n\n\n\nHow to measure plasticity with a bayesian hierarchical model\n\n\n\n\n\n\nNov 23, 2022\n\n\nAndrew MacDonald and Audrey Tremblay\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple nonlinear growth\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nHow to model the growth of an individual of known age.\n\n\n\n\n\n\nNov 21, 2022\n\n\nAndrew MacDonald\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a multispecies functional response in Stan\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nHow many things get eaten, if more than one animal gets eaten?\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald & Ben Mercier\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating R and your R packages\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nBecause sometimes: you have to\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAndrew tries Quarto\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nQuick notes on how to make a blog with Quarto and also targets.\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelection on plasticity\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nplasticity\n\n\n\n\nStan model where a slope is also a prdictor\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cholesky decomposition\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nIt’s giving correlations.\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty in growth increments\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nProcess and Measurement error in a simple growth process.\n\n\n\n\n\n\nNov 11, 2022\n\n\nWill Vieira, Andrew MacDonald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting priors on hierarchical multivariate models\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nkeep it realistic.\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShould we model growth as continuous or discrete\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nHow to model without making nonsensical predictions.\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAndrew tries Quarto\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\nQuick notes on how to make a blog with Quarto and also targets.\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nGrowth curves\n\n\n\n\n\n\n\nstan\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nAndrew\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nPopulation growth with functional programming\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2016\n\n\nAndrew MacDonald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter recommends stats books\n\n\n\n\n\n\n\nbooks\n\n\nstatistics\n\n\ntwitter\n\n\n\n\nWhat are my brilliant friends reading?\n\n\n\n\n\n\nDec 2, 2016\n\n\nAndrew MacDonald\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nContinuous fractions with Map and Reduce\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2014\n\n\nAndrew MacDonald\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html#the-challenge",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html#the-challenge",
    "title": "Validating a model of selection on plasticity",
    "section": "The challenge",
    "text": "The challenge\nStudying selection on phenotypic plasticity is challenging. First, because phenotypic plasticity is a slope – it is the change in a trait when an environmental variable changes. Secondly, because many traits of animals also cause other traits. For example, arriving later at a breeding site causes an individual to lay fewer eggs (because less food is available)."
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html#the-system",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html#the-system",
    "title": "Validating a model of selection on plasticity",
    "section": "The system",
    "text": "The system\nLet’s begin just with a simulation of three interrelated traits:\n\nThe date when a bird arrives, which determines\nHow many eggs they lay, out of which there is\nSome number of surviving offspring\n\n\n## how many birds\nnbirds <- 57\n# simulate arrival dates -- two weeks before and after whatever the average is\ndarrive <- runif(nbirds, min = -14, max = 14) |> round()\n\n## simulate clutch sizes -- decrease by 4% each day\navg_clutch <- 4.5\neffect_per_day <- .97\n\nclutch <- rpois(nbirds, exp(log(avg_clutch) + log(effect_per_day)*darrive))\n\nplot(darrive, clutch)\n\n\n\n\nAs an aside, this would be 0-truncated, since birds who don’t lay eggs don’t get observed at all.\n\n# simulate hatching success\nsuccess <- rbinom(nbirds, size = clutch, prob = .86)\n\nplot(darrive, success)\n\n\n\n\nwhat’s important to see here is that there is a negative correlation, even though the arrival date has no direct effect on the outcome\n\nsummary(glm(success ~ darrive, family = \"poisson\"))\n\n\nCall:\nglm(formula = success ~ darrive, family = \"poisson\")\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-3.10217  -0.67558  -0.03471   0.72290   2.20791  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.370106   0.073503  18.640  < 2e-16 ***\ndarrive     -0.033492   0.008242  -4.064 4.83e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 87.015  on 56  degrees of freedom\nResidual deviance: 69.548  on 55  degrees of freedom\nAIC: 251.66\n\nNumber of Fisher Scoring iterations: 5\n\n\nBut, if we use a binomial model that knows about the number of possible successful chicks, then we see what we expect:\n\nbin_mod <- (glm(cbind(success, clutch - success) ~ 1, family = binomial(link = \"logit\")))\n\nplogis(coef(bin_mod))\n\n(Intercept) \n  0.8784722 \n\n\nWhich matches the simulation above.\nif we put darrive in the model, the effect should be very close to 0 with overlap.\nIf we imagine that the laying date effects the survival p, then we should see an effect close to 0\n\nsummary(glm(\n  cbind(success, clutch - success) ~ 1 + darrive,\n            family = binomial(link = \"logit\")))\n\n\nCall:\nglm(formula = cbind(success, clutch - success) ~ 1 + darrive, \n    family = binomial(link = \"logit\"))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9768  -0.5478   0.6749   1.0866   1.8095  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 2.015257   0.217113   9.282   <2e-16 ***\ndarrive     0.007689   0.024139   0.319     0.75    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 76.324  on 55  degrees of freedom\nResidual deviance: 76.222  on 54  degrees of freedom\nAIC: 124.2\n\nNumber of Fisher Scoring iterations: 4\n\n\nOne Stan model can model all of these at the same time\n\none_indiv <- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv.stan\"))\n\none_indiv\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n}\nmodel {\n  success ~ binomial_logit(clutch, logit_psuccess);\n  clutch ~ poisson_log(log_avgclutch + log_b_date * darrive);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n}\n\n\n\none_indiv_post <- one_indiv$sample(\n  data = list(nbirds = nbirds,\n              clutch = clutch, \n              success = success, \n              darrive = darrive))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\none_indiv_post\n\n       variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n lp__           71.30  71.65 1.26 0.96 68.81 72.62 1.00     1973     2450\n logit_psuccess  1.58   1.58 0.12 0.12  1.38  1.79 1.00     2708     2568\n log_avgclutch   1.44   1.44 0.07 0.07  1.33  1.55 1.00     2362     2497\n log_b_date     -0.04  -0.04 0.01 0.01 -0.05 -0.03 1.00     2769     2671\n\n\nreasonably close to true values:\n\nplogis(1.44)\n\n[1] 0.8084547\n\nlog(avg_clutch)\n\n[1] 1.504077\n\nlog(effect_per_day)\n\n[1] -0.03045921\n\n\n\nsimulate_some_birds <- function(nbirds = 57, \n         log_b_date = log(.97),\n         log_avgclutch = log(4.5),\n         logit_psuccess = qlogis(.84)){\n  \n  # simulate arrival dates -- two weeks before and after whatever the average is\n  darrive <- runif(nbirds, min = -14, max = 14) |> round()\n  \n  ## simulate clutch sizes -- decrease  each day\n  clutch <- rpois(nbirds, exp(log_avgclutch + log_b_date*darrive))\n  \n  ## simulate success\n  success <- rbinom(nbirds, size = clutch, prob = plogis(logit_psuccess))\n  \n  return(list(\n    data_list = list(\n      nbirds = nbirds,\n      darrive = darrive, \n      clutch = clutch, \n      success = success \n    ),\n    true_values = tribble(\n      ~variable, ~true_value,\n      \"log_b_date\", log_b_date,\n      \"log_avgclutch\", log_avgclutch,\n      \"logit_psuccess\", logit_psuccess\n    )\n  ))\n}\n\n\ndata_for_simulation <- simulate_some_birds()\n\none_indiv_post <- one_indiv$sample(data = data_for_simulation$data_list,\n                                   refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n\ncomparison <- one_indiv_post |> \n  # tidybayes::gather_rvars(logit_psuccess, log_avgclutch, log_b_date) |> \n  tidybayes::tidy_draws() |> tidybayes::gather_variables() |> \n  right_join(data_for_simulation$true_values, by = c(\".variable\" = \"variable\"))\n\n\ncomparison |> \n  ggplot(aes(y = .variable, x = .value)) + \n  stat_halfeye() + \n  geom_point(aes(y = variable, x = true_value),\n             col = \"orange\",\n             pch = \"|\",\n             size = 10, data = data_for_simulation$true_values)\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "posts/2023-03-20-latent-continuous/index.html",
    "href": "posts/2023-03-20-latent-continuous/index.html",
    "title": "Latent continuous variables",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)\nImagine we have many measurments, all correlated with each other. Where do these correlations come from? One possibility is that all these measurents are caused by the values of some unobservable, latent trait. This happens in ecology when we imagine that, for example, the Competitive Ability of a species appears to us in the form of measurable traits that (to us) seem to represent this trait: leaf size, growth rate, alleopathy, etc.\nIn mathematics, this model is the following:\n$$\n\\[\\begin{align}\nY_{ij} &\\sim \\text{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij} &= \\alpha_i \\times \\beta_j\\\\\n\\boldsymbol{ \\alpha } &\\sim \\text{Normal}(0, 1) \\\\\n\\boldsymbol{ \\beta } &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align}\\]\n$$"
  },
  {
    "objectID": "posts/2023-03-20-latent-continuous/index.html#partially-constrained-model",
    "href": "posts/2023-03-20-latent-continuous/index.html#partially-constrained-model",
    "title": "Latent continuous variables",
    "section": "Partially constrained model",
    "text": "Partially constrained model\nIn this model I set constraints on the sign of the coefficients that relate the latent state to any observed variable. This is the sort of thing that would work in most ecological systems. For example, if an underlying state is “competitive ability” we might know it relates positively to traits like growth rate and leaf size, etc.\n\nlatent_cont_constrained <- cmdstan_model(\n  stan_file = here::here(\n    \"posts/2023-03-20-latent-continuous/latent_cont_constrained.stan\"))\n\nlatent_cont_constrained\n\n\ndata {\n  int<lower=0> N;\n  matrix[N, 5] y;\n}\nparameters {\n  row_vector[5] beta;\n  real<lower=0> sigma;\n  vector[N] alpha;\n}\ntransformed parameters {\n  row_vector[5] betatrans;\n  betatrans[1] = exp(beta[1]);\n  betatrans[2] = -exp(beta[2]);\n  betatrans[3] = exp(beta[3]);\n  betatrans[4] = exp(beta[4]);\n  betatrans[5] = -exp(beta[5]);\n}\nmodel {\n  // take the outer product: alpha multiplied by each beta in turn\n  matrix[N, 5] mu = alpha * betatrans;\n  for (i in 1:N){\n    y[i] ~ normal(mu[i], sigma);\n  }\n  beta ~ std_normal();\n  sigma ~ exponential(1);\n  alpha ~ std_normal();\n}\n\n\n\nlatent_cont_constrained_samp <- latent_cont_constrained$sample(\n  data = datalist, parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Location parameter[1] is inf, but must be finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c943da0867.stan', line 23, column 4 to column 32)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c943da0867.stan', line 23, column 4 to column 32)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 2 finished in 12.0 seconds.\nChain 3 finished in 12.2 seconds.\nChain 1 finished in 12.3 seconds.\nChain 4 finished in 12.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 12.2 seconds.\nTotal execution time: 12.4 seconds.\n\nlatent_cont_constrained_samp\n\n variable   mean median    sd   mad      q5    q95 rhat ess_bulk ess_tail\n lp__     -84.70 -84.05 15.31 15.23 -110.27 -60.68 1.00      618      960\n beta[1]    0.83   0.83  0.05  0.05    0.74   0.91 1.00      526      968\n beta[2]    0.18   0.17  0.06  0.06    0.08   0.28 1.00      667     1292\n beta[3]   -0.51  -0.51  0.09  0.09   -0.66  -0.37 1.00     1385     2103\n beta[4]    0.17   0.17  0.06  0.06    0.06   0.27 1.00      648     1108\n beta[5]   -1.07  -1.06  0.13  0.13   -1.30  -0.87 1.00     2402     1938\n sigma      0.60   0.60  0.02  0.01    0.57   0.62 1.00     5000     2966\n alpha[1]  -1.03  -1.02  0.21  0.22   -1.37  -0.69 1.00     5037     2288\n alpha[2]   0.48   0.48  0.21  0.21    0.14   0.83 1.00     8091     2531\n alpha[3]   0.43   0.43  0.20  0.20    0.09   0.77 1.00     8381     2650\n\n # showing 10 of 212 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n# tidybayes::get_variables(latent_cont_constrained_samp)\n\ntidybayes::gather_rvars(latent_cont_constrained_samp, betatrans[id]) |> \n  mutate(true_value = five_betas) |> \n  ggplot(aes(x = id, dist  = .value)) + \n  tidybayes::stat_halfeye(fill = \"darkgreen\") + \n  geom_point(aes(x = id, y = true_value), pch = 21, fill = \"orange\", size = 5)\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\nSo it seems that with some simple constraints on the signs, this model samples just fine!\nDoes it also get the latent states right?\n\ntidybayes::summarise_draws(latent_cont_constrained_samp, quantile) |> \n  filter(stringr::str_detect(variable, \"alpha\")) |> \n  bind_cols(true_alpha = alpha) |> \n  mutate(id = readr::parse_number(variable),\n         rnk = dense_rank(true_alpha)) |> \n  ggplot(aes(x = `25%`, xend = `75%`, y = rnk, yend = rnk)) + \n  geom_segment() + \n  geom_point(aes(x = true_alpha, y  = rnk), inherit.aes = FALSE, col = \"red\")\n\n\n\n\nWhat if we experiment with a standard deviation for the alpha (ie a hierarchical model)\n\nlatent_cont_constr_hier <- cmdstan_model(\n  stan_file = here::here(\"posts/2023-03-20-latent-continuous/latent_cont_constr_hier.stan\"))\n\nlatent_cont_constr_hier\n\n\ndata {\n  int<lower=0> N;\n  matrix[N, 5] y;\n}\nparameters {\n  row_vector[5] beta;\n  real<lower=0> sigma;\n  vector[N] alpha;\n  real<lower=0> s_alpha;\n}\ntransformed parameters {\n  row_vector[5] betatrans;\n  betatrans[1] = exp(beta[1]);\n  betatrans[2] = -exp(beta[2]);\n  betatrans[3] = exp(beta[3]);\n  betatrans[4] = exp(beta[4]);\n  betatrans[5] = -exp(beta[5]);\n}\nmodel {\n  // take the outer product: alpha multiplied by each beta in turn\n  matrix[N, 5] mu = alpha * betatrans;\n  for (i in 1:N){\n    y[i] ~ normal(mu[i], sigma);\n  }\n  beta ~ std_normal();\n  sigma ~ exponential(1);\n  alpha ~ normal(0, s_alpha);\n  s_alpha ~ exponential(.1);\n}\n\n\n\nlatent_cont_constr_hier_samp <- latent_cont_constr_hier$sample(\n  data = datalist, parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 24, column 4 to column 32)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 28, column 2 to column 29)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 28, column 2 to column 29)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 28, column 2 to column 29)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 3 finished in 20.9 seconds.\nChain 4 finished in 23.3 seconds.\nChain 1 finished in 23.9 seconds.\nChain 2 finished in 25.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 23.4 seconds.\nTotal execution time: 25.7 seconds.\n\n\nWarning: 4 of 4 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\ntidybayes::summarise_draws(latent_cont_constr_hier_samp, quantile) |> \n  filter(stringr::str_detect(variable, \"alpha\\\\[\")) |> \n  bind_cols(true_alpha = alpha) |> \n  mutate(id = readr::parse_number(variable),\n         rnk = dense_rank(true_alpha)) |> \n  ggplot(aes(x = `25%`, xend = `75%`, y = rnk, yend = rnk)) + \n  geom_segment() + \n  geom_point(aes(x = true_alpha, y  = rnk), inherit.aes = FALSE, col = \"red\")"
  }
]