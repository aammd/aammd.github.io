[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Study of the Household",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a multispecies functional response in Stan\n\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald & Ben Mercier\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability integral transforms\n\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2022\n\n\nAndrew\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrowth curves\n\n\n\n\n\n\n\nstan\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nAndrew\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation growth with functional programming\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2016\n\n\nAndrew MacDonald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous fractions with Map and Reduce\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2014\n\n\nAndrew MacDonald\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew MacDonald, PhD",
    "section": "",
    "text": "I love insects, statistics, but most of all I like solving interesting problems in a collaborative environment\nI am currently a Research Associate at the Université de Sherbrooke. My goal is to improve our ability to make predictions for ecological systems, and to build a community of practice around statistical methods in ecology."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Andrew MacDonald, PhD",
    "section": "Education",
    "text": "Education\n\nPhD, University of British Columbia, 2017\nMSc, University of Toronto, 2009\nBSc, Cape Breton University, 2006"
  },
  {
    "objectID": "posts/continuous-fraction-functional-programming/index.html",
    "href": "posts/continuous-fraction-functional-programming/index.html",
    "title": "Continuous fractions with Map and Reduce",
    "section": "",
    "text": "Over the summer, some of us here at UBC started a reading group based around Hadley Wickham’s book, Advanced R Programming. The goal was to compare our answers to the exercises and our impressions of the content.\nWe recently read my favourite chapter, Functionals, where readers are challenged to read about some algorithms in Structure and Interpretation of Computer Programs, and implement it in R.\nI wanted to share some functions I wrote to calculate these exotic things called k-term finite continued fractions, based on that challenge:"
  },
  {
    "objectID": "posts/continuous-fraction-functional-programming/index.html#continued-fractions",
    "href": "posts/continuous-fraction-functional-programming/index.html#continued-fractions",
    "title": "Continuous fractions with Map and Reduce",
    "section": "continued fractions",
    "text": "continued fractions\nContinued Fractions look like this:\n\\[\n\\frac{n_1}{d_1 + \\frac{n_2}{d_2 + \\frac{n_3}{d_3 + \\cdots } } }\n\\]\nand have an infinite number of \\(n\\) and \\(d\\) values. However, if after \\(k\\) values you just replace the remaining ones (the \\(...\\) above) with 0, then you get a k-term finite continued fraction, which is maybe close enough:\n\\[\n\\frac{n_1}{d_1  + \\frac{n_2}{\\ddots + \\frac{n_k}{d_k}}}\n\\]\nContinued fractions have the values of \\(n\\) and \\(d\\) defined by a series. We need a function that will take the series for the numerator and denominator, calculate \\(k\\) terms, and put them together into a continued fraction.\nSo, how do we calculate this in R? Well, it turns out we can apply two concepts we learned from Wickham’s book to do so: closures and functionals.\n\nCreating a series of closures\nWe could say that each “part” of the continued fraction was a unit that looks like\n\\[\n\\frac{n}{d + x}\n\\]\nwhere \\(n\\) and \\(d\\) are a “pair” of numerator and denominator, and \\(x\\) is the next “part”, and so on. (there are probably mathematical terms for these, but I’m an ecologist, not a mathematician!). If you “build” the continued fraction from the inside out, you’d start with \\(x = 0\\), and calculate \\(\\frac{n_k}{d_k}\\). Then you move on to \\(n_{k-1}\\); for this fraction you have the numerator-denominator pair, plus the term (\\(x\\)) which you just calculated.\nWe can use closures to calculate each of these “parts” in turn, in order to keep the numerator-denominator pairs together. Closures are functions which are created by other functions; they “enclose” the environment in which they were created (hence the name), which means they can use variables from that environment (in our case, the values of a numerator & denominator)\nFirst, we make a “function factory”, a function which creates other functions (closures which retain different values of \\(n\\) and \\(d\\)):\n\nfrac_maker <- function(n, d){\n  force(n)\n  force(d)\n  function(x) n / (d + x)\n  }\n\nThis function takes a pair of numbers and defines a new function which uses them. But how can we create lots of closures, one for every numerator-denominator “pair” between 1 and \\(k\\)? We can use the function Map to run this function on each variable pair. Map works like a zipper, combining the first elements of two (or more) vectors with a function, then the second, etc. For example, the reciprocal of the Golden Ratio is the result of a continued fraction where \\(n\\) and \\(d\\) are both 1:\n\n  Ns <- rep(1, 20)\n  Ds <- rep(1, 20)\n  funs <- Map(frac_maker, Ns, Ds)\n\nfuns is now a list of functions, each one remembering its own particular value of \\(n\\) or \\(d\\). Now all we need to do is put them together and run them all. For that, we need another functional: Reduce."
  },
  {
    "objectID": "posts/continuous-fraction-functional-programming/index.html#using-reduce",
    "href": "posts/continuous-fraction-functional-programming/index.html#using-reduce",
    "title": "Continuous fractions with Map and Reduce",
    "section": "using Reduce",
    "text": "using Reduce\nReduce is just lovely. It takes a vector and “reduces” it to a single number by applying a function: the first two arguments are the first and second vector elements, then the result of that calculation and the third element, then that result and the fourth element:\nReduce(sum, c(1, 2, 3, 4)) = sum(sum(sum(1, 2), 3), 4)\nHere we have a list of functions, not values, so we use Reduce to run a function that simply executes its second argument on its first:\n\nanswer <- Reduce(function(f1,f2) f2(f1), x = funs, init = 0)\n## take reciprocal to get the Golden Ratio:\n1/answer\n\n[1] 1.618034\n\n\nWe start with the value of 0, because as we said the approximation of the continuous fraction simply replaces all the the “parts” after \\(k\\) with 0. So our function runs the first function on 0, the second function on that result, the third function on that result, etc. The result is the whole approximation of a continuous fraction, “built” from the inside out."
  },
  {
    "objectID": "posts/continuous-fraction-functional-programming/index.html#combine-into-a-function",
    "href": "posts/continuous-fraction-functional-programming/index.html#combine-into-a-function",
    "title": "Continuous fractions with Map and Reduce",
    "section": "combine into a function",
    "text": "combine into a function\nSo now we combine this to form a single function that calculates the value of a continuous series for \\(k\\) terms:\n\ncontinuous_frac <- function(Ns, Ds, frac_fun = frac_maker){\n  Ns <- rev(Ns)\n  Ds <- rev(Ds)\n  funs <- Map(frac_fun, Ns, Ds)\n  Reduce(function(f1,f2) f2(f1), x = funs, init = 0)\n  }\n\nNote that we have to reverse the series Ns and Ds, simply because the series are usually defined from \\(n_1\\) to \\(n_k\\), but we are building our function from \\(n_k\\) backwards."
  },
  {
    "objectID": "posts/continuous-fraction-functional-programming/index.html#calculating-numbers",
    "href": "posts/continuous-fraction-functional-programming/index.html#calculating-numbers",
    "title": "Continuous fractions with Map and Reduce",
    "section": "calculating numbers",
    "text": "calculating numbers\nWith this function in hand, we can approximate any continued fraction. Here are a few examples:\n\nThe value of \\(e\\) (for biological content)\nAre you wondering what all this has to do with biology? Well, Euler’s number certainly appears in plenty of biological models, so let’s calculate it:\n\ndenominator <- function(k){\n  nums <- lapply(seq_len(k)*2, function(x) c(1, 1, x))\n  out <- do.call(c, nums)\n  out[-1]\n}\n\n2 + continuous_frac(Ds = denominator(20), Ns = rep(1, 3 * 20 -1))\n\n[1] 2.718282"
  },
  {
    "objectID": "posts/continuous-fraction-functional-programming/index.html#calculating-pi",
    "href": "posts/continuous-fraction-functional-programming/index.html#calculating-pi",
    "title": "Continuous fractions with Map and Reduce",
    "section": "calculating pi",
    "text": "calculating pi\nThere are several ways to calculate \\(\\pi\\), based on different forms of this equation. Apparently they converge at different rates. here are some examples:\n\nleibniz <- function(k){\n  seqs <- seq(from = 1, by = 2, length.out = k -1)\n  N <- c(4, seqs ^ 2)\n  D <- c(1, rep(2, k-1))\n  continuous_frac(N, D)\n}\n\nsomayaji <- function(k){\n  N <- seq(from = 1, by = 2, length.out = k) ^ 2\n  D <- rep(6, k)\n  3 + continuous_frac(N, D)\n}\n\nlinear <- function(k){\n  N <- seq(from = 1, by = 1, length.out = k -1) ^ 2\n  N <- c(4, N)\n  D <- seq(from = 1, by = 2, length.out = k)\n  continuous_frac(N, D)\n}\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\ndata.frame(ks = seq(2, 100, by = 5)) %>%\n  rowwise %>%\n  mutate(leibniz = leibniz(ks),\n         somayaji = somayaji(ks),\n         linear = linear(ks)) %>%\n  gather(method, pi_value, -ks) %>%\n  ggplot(aes(x = ks, y = pi_value, colour = method)) + geom_point() + geom_path() + theme_bw()\n\n\n\n\nIf we zoom in we can see that the third form outperforms Somayaji’s:\n\ndata.frame(ks = seq(5, 10, by = 1)) %>%\n  rowwise %>%\n  mutate(somayaji = somayaji(ks),\n         linear = linear(ks)) %>%\n  gather(method, pi_value, -ks) %>%\n  ggplot(aes(x = ks, y = pi_value, colour = method)) + geom_point() + geom_path() + xlab(\"k\") + ylab(expression(pi)) + theme_bw()"
  },
  {
    "objectID": "posts/growth_curve_measurement_error/index.html",
    "href": "posts/growth_curve_measurement_error/index.html",
    "title": "Growth curves",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nMany animals and plants grow quickly when small and more slowly as they mature. There are many popular ways of describing this relationship; one very common and convenient relationship is the Von-Bertanalaffy (VB) growth curve:\n\\[\nL_t = L_0e^{-rt} + L_\\infty(1 - e^{-rt})\n\\tag{1}\\]\nThis can also be written as\n\\[\nL_t = L_\\infty - (L_\\infty - L_0)e^{-rt}\n\\]\nThis curve has a long tradition in ecology. It can be derived from simple assumptions about how different aspects of metabolism scale with the body size of an organism. I’m not going to derive it here because I don’t want this to be a huge post!\nI like this second way of writing the equation because it highlights that the VB equation is a linear transformation of an exponential function."
  },
  {
    "objectID": "posts/growth_curve_measurement_error/index.html#a-single-tree",
    "href": "posts/growth_curve_measurement_error/index.html#a-single-tree",
    "title": "Growth curves",
    "section": "a single tree",
    "text": "a single tree\nI’m going to do a simple simulation of one tree growing. here is code that does that\n\n\nfunction (time = seq(from = 10, to = 200, by = 5), Lo = 0.01, \n    Lmax = 150, r = 0.03, sd = 5) \n{\n    tibble::tibble(time, Lt = Lmax - (Lmax - Lo) * exp(-r * time), \n        Lt_obs = rnorm(length(Lt), mean = Lt, sd = 5))\n}\n\n\n\ntar_load(vb_one_tree)\n\nvb_one_tree |> \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() + \n  geom_line(aes(y = Lt)) + \n  theme_bw()\n\n\n\n\nFigure 1: One tree’s growth\n\n\n\n\nsimulate this same data in targets\nfit a stan model to it! here is the Stan model: formatted?\n\n\ndata{\n  int<lower=0> n;\n  vector[n] time;\n  vector[n] Lt;\n}\nparameters{\n  real<lower=0> r;\n  real<lower=0> Lmax;\n  real<lower=0> sigma_obs;\n}\nmodel{\n  Lt ~ normal(Lmax * (1 - exp(-r*time)), sigma_obs);\n  r ~ lognormal(-3, 1);\n  Lmax ~ normal(200, 20);\n  sigma_obs ~ exponential(1);\n}\n\n\n\nvb_one_tree <- cmdstan_model(here::here(\"posts\", \"growth_curve_measurement_error\",\"vb_one_tree.stan\"))\n\none_tree_sim <- sim_vb_one_tree()\n\none_tree_list <- list(n = nrow(one_tree_sim),\n                      time  = one_tree_sim$time, \n                      Lt = one_tree_sim$Lt_obs)\n\none_tree_post <- vb_one_tree$sample(data = one_tree_list,\n                                    refresh = 0L,\n                                    parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.5 seconds.\n\none_tree_post\n\n  variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n lp__      -89.55 -89.22 1.25 0.99 -92.06 -88.20 1.00     1726     2006\n r           0.03   0.03 0.00 0.00   0.03   0.03 1.00     2215     2329\n Lmax      150.51 150.49 1.36 1.34 148.31 152.81 1.00     2329     2376\n sigma_obs   5.33   5.27 0.56 0.55   4.48   6.33 1.00     2472     2497\n\n\nplot it\n\nlibrary(tidybayes)\n\nexpected_df <- one_tree_post |> \n  spread_rvars(Lmax, r) |> \n  expand_grid(time = seq(0, 200, length.out = 14)) |> \n  mutate(Lt = Lmax * (1 - exp(-r * time)))\n\nexpected_plot <- expected_df |> \n  ggplot(aes(x = time, y = Lt)) + \n  stat_lineribbon()\nexpected_plot\n\n\n\n\nGrowth curve for one tree. the line shows the expected value, with posterior uncertainty around exactly what that average should be.\n\n\n\n\nThis relationship shows the average line, the expected size of the tree. We can add the original data like this:\n\none_tree_sim |> \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() +\n  stat_dist_lineribbon(aes(x = time, dist = Lt),\n                  data = expected_df, inherit.aes = FALSE)\n\n\n\n\nAt the time of this writing the error messages here are particularly unhelpful. If you try to use stat_lineribbon rather than stat_dist_lineribbon you get the foloing misleading message:\n\none_tree_sim |> \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() +\n  stat_lineribbon(aes(x = time, y = Lt),\n                  data = expected_df, inherit.aes = FALSE)"
  },
  {
    "objectID": "posts/multispecies-functional-response/index.html",
    "href": "posts/multispecies-functional-response/index.html",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/multispecies-functional-response/index.html#the-equation",
    "href": "posts/multispecies-functional-response/index.html#the-equation",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "The equation",
    "text": "The equation\nPredators eat prey. They eat prey faster when there is more prey – though they do have to slow down to catch, kill, and chew.1\nIf a predator can eat multiple things, then they might end up eating less of any one prey because they spread their kills around among all their prey. In the very simplest case, they do this in proportion to how frequent the different prey are – the predator has no preference, it just goes around randomly and eats what it finds\nThe classic OG version of this model comes from Holling (1966)\n\\[\nA = \\frac{\\alpha N}{1 + h\\alpha N}\n\\tag{1}\\]\nwhere\n\nN is the number of prey\n\\(\\alpha\\) is the attack rate\n\\(h\\) is the handling time\n\nAnd altogether you get the number of attacks \\(A\\) on prey in some unit of time."
  },
  {
    "objectID": "posts/multispecies-functional-response/index.html#multiple-species",
    "href": "posts/multispecies-functional-response/index.html#multiple-species",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "multiple species",
    "text": "multiple species\nSuppose you have more than one species in this system. You could then rewrite Equation 1 to allow multiple animals to be included in the predation:\n\\[\nA_i = \\frac{\\alpha N_i}{1 + h\\alpha \\sum_{j = 1}^s N_j}\n\\tag{2}\\]\nhere \\(\\sum_{j = 1}^s N_j\\) means the sum over the abundances of all the prey. The subscript \\(i\\) just means that we are talking about one particular prey, which we label \\(i\\). This prey is included in the summation in the denominator.\nIt’s common to consider that different prey species might be attacked or handled at different rates (Smith and Smith 2020) (Smout et al. 2010)"
  },
  {
    "objectID": "posts/population-growth-functional-programming/index.html",
    "href": "posts/population-growth-functional-programming/index.html",
    "title": "Population growth with functional programming",
    "section": "",
    "text": "Today I want to tell you about an approach for functional programming in R – and then apply it to studying population growth!\nI have been studying some of the purrr functions lately. They are a useful family of functions for performing two common tasks in R: manipulating lists and altering the behaviour of functions. If you’d like a high-quality guide to this group of functions, set aside some time to work through Jenny Bryan’s excellent tutorial and Hadley Wickham’s chapter on Lists.\nI was inspired to write this post after reading This StackOverflow question by jebyrnes. He asks:\nand there is! An answerer mentioned purr::accumulate(). In this post I’m going to expand on the approach they suggest. accumulate, and its twin reduce, are examples of functionals – functions that take functions as their arguments, and manipulate their behaviour in some way. purrr::accumulate is a wrapper around to Reduce from the base package, with the argument accumulate = TRUE.\naccumulate is normally used when you want to do some cumulative function all along a vector. For example, we can reproduce the cumulative some of a vector like this (same output as cumsum(1:10))\n.x and .y here are just a handy way of writing “the first thing” and “the second thing”. Then accumulate goes down the vector 1:10, and takes the first thing (1) adds it to the second thing (2) and so on….\nHowever, this is not the only way it works! accumulate can take an initial value (.init) and can work on a dummy variable. If its starting function does nothing but modify an element, it will just keep modifying it: so instead of f(1, 2), f(f(1, 2), 3) we get f(.init), f(f(.init)) etc:\nClearly, the 0s are not involved in any calculation (the answer would be 0!). instead, you just get the starting value multiplied by 1.5 each time!\nThis already suggests an awesome biological interpretation: logistic population growth.\nOn thing I like about this is that is is to much easier to look at – it look like the common biological equation for population growth:\n\\[\nN_{t+1} = r*N_t\n\\]"
  },
  {
    "objectID": "posts/population-growth-functional-programming/index.html#so-is-this-useful",
    "href": "posts/population-growth-functional-programming/index.html#so-is-this-useful",
    "title": "Population growth with functional programming",
    "section": "So, is this useful?",
    "text": "So, is this useful?\nI wonder if this might be an interesting pedagogical tool. I feel like it might place the emphasis a bit differently to for-loops. Perhaps a for loop emphasizes the passage of time – how, at each time step (each i for example) certain things happen in a certain order (the Resource grows, the Predator kills some, then some predators die, etc). On the other hand, I feel like the functional programming approach emphasizes how a population (or pair of populations) is transformed. Each little function has some parameters – which are either, constant, varying, and/or influenced by something besides population size – and each little function does only one thing – transform the population between one time step and the next."
  },
  {
    "objectID": "posts/probability-integral-transform/index.html",
    "href": "posts/probability-integral-transform/index.html",
    "title": "Probability integral transforms",
    "section": "",
    "text": "take random numbers from a distribution\nThen run them through that distribution’s CDF\nsure enough it is uniform!"
  },
  {
    "objectID": "posts/probability-integral-transform/index.html#what-happens-when-you-are-wrong",
    "href": "posts/probability-integral-transform/index.html#what-happens-when-you-are-wrong",
    "title": "Probability integral transforms",
    "section": "what happens when you are wrong",
    "text": "what happens when you are wrong\nlet’s make some curves that don’t really match\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6          ✔ purrr   0.3.4.9000\n✔ tibble  3.1.7          ✔ dplyr   1.0.8     \n✔ tidyr   1.1.3          ✔ stringr 1.4.0     \n✔ readr   2.1.1          ✔ forcats 0.5.1     \n\n\nWarning: package 'ggplot2' was built under R version 4.1.2\n\n\nWarning: package 'tibble' was built under R version 4.1.2\n\n\nWarning: package 'dplyr' was built under R version 4.1.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nn <- 4000\ntibble(meanval = seq(from = 1, to = 14, length.out = 6),\n       sd = 2.5) |> \n  expand_grid(x = seq(from = 0, to = 18, length.out = 30)) |>\n  mutate(normal_dist = dnorm(x, mean = meanval, sd = sd),\n         gamma_dist = dgamma(x, \n                        shape = meanval^2/sd^2,\n                        rate = meanval/sd^2)) |> \n  pivot_longer(ends_with(\"dist\"), \n               names_to = \"distribution\",\n               values_to = \"value\") |> \n  ggplot(aes(x = x, y = value, colour = distribution)) + \n  geom_line()  +\n  facet_wrap(~meanval)\n\n\n\n\nWe can see that the fit gets worse as the mean drops\nlet’s simulate data from the gamma and use the PIT assuming instead it is normal:\n\nn <- 4000\ntibble(meanval = seq(from = 1, to = 14, length.out = 6),\n       sd = 2.5) |> \n  rowwise() |>\n  mutate(normal_dist = list(rnorm(n, mean = meanval, sd = sd)),\n         gamma_dist = list(rgamma(n, \n                        shape = meanval^2/sd^2,\n                        rate = meanval/sd^2))) |> \n  pivot_longer(ends_with(\"dist\"), \n               names_to = \"distribution\",\n               values_to = \"samples\") |> \n  rowwise() |> \n  mutate(pit_samples = list(pnorm(samples, mean = meanval, sd = sd))) |> \n  select(-samples) |> \n  # filter(distribution == \"gamma_dist\") |> \n  unnest(pit_samples) |> \n  ggplot(aes(x = pit_samples)) + \n  geom_histogram() + \n  facet_grid(distribution~meanval)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nlet’s try it with just the gamma, but changing both moments and always using the normal:\n\nn <- 4000\nexpand_grid(meanval = seq(from = 1, to = 14, length.out = 6),\n       sdval = seq(from = .2, to = 7, length.out = 4)) |> \n  rowwise() |>\n  mutate(gamma_dist = list(rgamma(n, \n                        shape = meanval^2/sdval^2,\n                        rate = meanval/sdval^2))) |> \n  rowwise() |> \n  mutate(pit_samples = list(\n    pnorm(gamma_dist,\n          mean = meanval,\n          sd = sdval))) |> \n  select(-gamma_dist) |> \n  # filter(distribution == \"gamma_dist\") |> \n  unnest(pit_samples) |> \n  ggplot(aes(x = pit_samples)) + \n  geom_histogram() + \n  facet_grid(sdval~meanval)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nand with the lognormal\n\nn <- 4000\nexpand_grid(meanval = seq(from = 1, \n                          to = 14, \n                          length.out = 6),\n       sdval = seq(from = .2, \n                   to = 7, \n                   length.out = 4)) |> \n  rowwise() |>\n  mutate(\n    cf = log(sdval/meanval)^2 + 1,\n    lnorm_dist = list(rlnorm(n, \n                        meanlog = log(meanval) - .5*cf, \n                        sdlog = sqrt(cf))\n                      )\n    )|> \n  rowwise() |> \n  mutate(pit_samples = list(\n    pnorm(lnorm_dist,\n          mean = meanval,\n          sd = sdval)\n    # plnorm(lnorm_dist, \n    #        meanlog = log(meanval) - .5*cf, \n    #        sdlog = sqrt(cf))\n    )) |> \n  select(-lnorm_dist) |> \n  # filter(distribution == \"gamma_dist\") |> \n  unnest(pit_samples) |> \n  ggplot(aes(x = pit_samples)) + \n  geom_histogram() + \n  facet_grid(sdval~meanval)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  }
]