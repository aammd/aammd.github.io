[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew MacDonald, PhD",
    "section": "",
    "text": "I love insects, statistics, but most of all I like solving interesting problems in a collaborative environment\nI am currently a Research Associate at the Université de Sherbrooke. My goal is to improve our ability to make predictions for ecological systems, and to build a community of practice around statistical methods in ecology."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Andrew MacDonald, PhD",
    "section": "",
    "text": "I love insects, statistics, but most of all I like solving interesting problems in a collaborative environment\nI am currently a Research Associate at the Université de Sherbrooke. My goal is to improve our ability to make predictions for ecological systems, and to build a community of practice around statistical methods in ecology."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Andrew MacDonald, PhD",
    "section": "Education",
    "text": "Education\n\nPhD, University of British Columbia, 2017\nMSc, University of Toronto, 2009\nBSc, Cape Breton University, 2006"
  },
  {
    "objectID": "index.html#selected-publications",
    "href": "index.html#selected-publications",
    "title": "Andrew MacDonald, PhD",
    "section": "Selected publications",
    "text": "Selected publications"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Study of the Household",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nUseful Zeros: poisson mixtures and custom families\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nMaking something out of the right kind of nothing.\n\n\n\n\n\nApr 24, 2025\n\n\nAndrew MacDonald\n\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\n\nPhylogeny\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nit is that (evolutionary) time. \n\n\n\n\n\nSep 27, 2024\n\n\nAndrew MacDonald\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nRicker Model with Allee effects\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nHow to model discrete growth with low-density effects. \n\n\n\n\n\nSep 25, 2024\n\n\nAndrew MacDonald\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nDifference of normals is normal\n\n\n\n\n\n\nprobability\n\n\nlikelihood\n\n\n\nCalculating something like growth with measurement error. \n\n\n\n\n\nMay 3, 2024\n\n\nAndrew MacDonald\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nContrasts\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nWhat’s the (kind of) difference? \n\n\n\n\n\nDec 14, 2023\n\n\nAndrew MacDonald\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to model discrete growth\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nreproduction\n\n\nMAR\n\n\n\nComparing a lagged AR-1 model with the marginalized transition distribution. \n\n\n\n\n\nNov 24, 2023\n\n\nAndrew MacDonald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete-time population growth in Stan\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nreproduction\n\n\nMAR\n\n\n\nInspired by Ives 2003. \n\n\n\n\n\nNov 17, 2023\n\n\nAndrew MacDonald\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nVectorizing a multilevel AR-1 model\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nVectorizing with multiple species \n\n\n\n\n\nNov 14, 2023\n\n\nAndrew MacDonald\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\ncausal DAGS and full-luxury shade trees\n\n\n\n\n\n\nQCBS\n\n\nstan\n\n\n\nShady DAGs \n\n\n\n\n\nNov 8, 2023\n\n\nAndrew MacDonald, Bella Richmond\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nModelling discrete growth\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nHow to model the growth of things in a sensible fashion \n\n\n\n\n\nNov 7, 2023\n\n\nAndrew MacDonald, Kelly Forester\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nPower laws on log-linear plots\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nHow to fit and plot Power Law curves with lm. \n\n\n\n\n\nNov 3, 2023\n\n\nAndrew MacDonald, Amélie Morin\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nStan-dalone generated quantites\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\ngenerated quantities\n\n\n\nworking with Stan after you’ve done that once already. \n\n\n\n\n\nNov 1, 2023\n\n\nAndrew MacDonald\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nTransformed regression in Stan\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nQCBS\n\n\n\nCurvy lines done dirt cheap. \n\n\n\n\n\nAug 22, 2023\n\n\nAndrew MacDonald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nMissing data in non-normal distributions\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nQCBS\n\n\n\nWe could have counted them but we didn’t. \n\n\n\n\n\nAug 22, 2023\n\n\nAndrew MacDonald, Flavio Affinito\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nBreakpoint regression in Stan\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nQCBS\n\n\n\nTwo lines diverged at a particular point. \n\n\n\n\n\nAug 5, 2023\n\n\nAndrew MacDonald\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nLatent continuous variables\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nModelling continuous latent states in Stan. \n\n\n\n\n\nMar 20, 2023\n\n\nAndrew MacDonald\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nValidating a model of selection on plasticity\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nPlus ça change, plus c’est la change qui change\n\n\n\n\n\nFeb 2, 2023\n\n\nAndrew MacDonald\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe evolution of plasticity\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nbrms\n\n\nsimulation\n\n\n\nHow to measure plasticity with a bayesian hierarchical model \n\n\n\n\n\nNov 23, 2022\n\n\nAndrew MacDonald and Audrey Tremblay\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nSimple nonlinear growth\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nHow to model the growth of an individual of known age. \n\n\n\n\n\nNov 21, 2022\n\n\nAndrew MacDonald\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a multispecies functional response in Stan\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nHow many things get eaten, if more than one animal gets eaten? \n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald & Ben Mercier\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating R and your R packages\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nBecause sometimes: you have to \n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAndrew tries Quarto\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nQuick notes on how to make a blog with Quarto and also targets. \n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSelection on plasticity\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\nplasticity\n\n\n\nStan model where a slope is also a prdictor \n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cholesky decomposition\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nIt’s giving correlations. \n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty in growth increments\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nProcess and Measurement error in a simple growth process. \n\n\n\n\n\nNov 11, 2022\n\n\nWill Vieira, Andrew MacDonald\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nSetting priors on hierarchical multivariate models\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nkeep it realistic. \n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nShould we model growth as continuous or discrete\n\n\n\n\n\n\nUdeS\n\n\nstan\n\n\n\nHow to model without making nonsensical predictions. \n\n\n\n\n\nNov 11, 2022\n\n\nAndrew MacDonald\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nProbability integral transforms\n\n\n\n\n\nUnderstanding a diagnostic tool for models. \n\n\n\n\n\nOct 23, 2022\n\n\nAndrew\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nGrowth curves\n\n\n\n\n\n\nstan\n\n\nsimulation\n\n\ngrowth\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nAndrew\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation growth with functional programming\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 7, 2016\n\n\nAndrew MacDonald\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter recommends stats books\n\n\n\n\n\n\nbooks\n\n\nstatistics\n\n\ntwitter\n\n\n\nWhat are my brilliant friends reading?\n\n\n\n\n\nDec 2, 2016\n\n\nAndrew MacDonald\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous fractions with Map and Reduce\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nSep 27, 2014\n\n\nAndrew MacDonald\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html",
    "title": "Continuous fractions with Map and Reduce",
    "section": "",
    "text": "Over the summer, some of us here at UBC started a reading group based around Hadley Wickham’s book, Advanced R Programming. The goal was to compare our answers to the exercises and our impressions of the content.\nWe recently read my favourite chapter, Functionals, where readers are challenged to read about some algorithms in Structure and Interpretation of Computer Programs, and implement it in R.\nI wanted to share some functions I wrote to calculate these exotic things called k-term finite continued fractions, based on that challenge:"
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#continued-fractions",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#continued-fractions",
    "title": "Continuous fractions with Map and Reduce",
    "section": "continued fractions",
    "text": "continued fractions\nContinued Fractions look like this:\n\\[\n\\frac{n_1}{d_1 + \\frac{n_2}{d_2 + \\frac{n_3}{d_3 + \\cdots } } }\n\\]\nand have an infinite number of \\(n\\) and \\(d\\) values. However, if after \\(k\\) values you just replace the remaining ones (the \\(...\\) above) with 0, then you get a k-term finite continued fraction, which is maybe close enough:\n\\[\n\\frac{n_1}{d_1  + \\frac{n_2}{\\ddots + \\frac{n_k}{d_k}}}\n\\]\nContinued fractions have the values of \\(n\\) and \\(d\\) defined by a series. We need a function that will take the series for the numerator and denominator, calculate \\(k\\) terms, and put them together into a continued fraction.\nSo, how do we calculate this in R? Well, it turns out we can apply two concepts we learned from Wickham’s book to do so: closures and functionals.\n\nCreating a series of closures\nWe could say that each “part” of the continued fraction was a unit that looks like\n\\[\n\\frac{n}{d + x}\n\\]\nwhere \\(n\\) and \\(d\\) are a “pair” of numerator and denominator, and \\(x\\) is the next “part”, and so on. (there are probably mathematical terms for these, but I’m an ecologist, not a mathematician!). If you “build” the continued fraction from the inside out, you’d start with \\(x = 0\\), and calculate \\(\\frac{n_k}{d_k}\\). Then you move on to \\(n_{k-1}\\); for this fraction you have the numerator-denominator pair, plus the term (\\(x\\)) which you just calculated.\nWe can use closures to calculate each of these “parts” in turn, in order to keep the numerator-denominator pairs together. Closures are functions which are created by other functions; they “enclose” the environment in which they were created (hence the name), which means they can use variables from that environment (in our case, the values of a numerator & denominator)\nFirst, we make a “function factory”, a function which creates other functions (closures which retain different values of \\(n\\) and \\(d\\)):\n\nfrac_maker &lt;- function(n, d){\n  force(n)\n  force(d)\n  function(x) n / (d + x)\n  }\n\nThis function takes a pair of numbers and defines a new function which uses them. But how can we create lots of closures, one for every numerator-denominator “pair” between 1 and \\(k\\)? We can use the function Map to run this function on each variable pair. Map works like a zipper, combining the first elements of two (or more) vectors with a function, then the second, etc. For example, the reciprocal of the Golden Ratio is the result of a continued fraction where \\(n\\) and \\(d\\) are both 1:\n\n  Ns &lt;- rep(1, 20)\n  Ds &lt;- rep(1, 20)\n  funs &lt;- Map(frac_maker, Ns, Ds)\n\nfuns is now a list of functions, each one remembering its own particular value of \\(n\\) or \\(d\\). Now all we need to do is put them together and run them all. For that, we need another functional: Reduce."
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#using-reduce",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#using-reduce",
    "title": "Continuous fractions with Map and Reduce",
    "section": "using Reduce",
    "text": "using Reduce\nReduce is just lovely. It takes a vector and “reduces” it to a single number by applying a function: the first two arguments are the first and second vector elements, then the result of that calculation and the third element, then that result and the fourth element:\nReduce(sum, c(1, 2, 3, 4)) = sum(sum(sum(1, 2), 3), 4)\nHere we have a list of functions, not values, so we use Reduce to run a function that simply executes its second argument on its first:\n\nanswer &lt;- Reduce(function(f1,f2) f2(f1), x = funs, init = 0)\n## take reciprocal to get the Golden Ratio:\n1/answer\n\n[1] 1.618034\n\n\nWe start with the value of 0, because as we said the approximation of the continuous fraction simply replaces all the the “parts” after \\(k\\) with 0. So our function runs the first function on 0, the second function on that result, the third function on that result, etc. The result is the whole approximation of a continuous fraction, “built” from the inside out."
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#combine-into-a-function",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#combine-into-a-function",
    "title": "Continuous fractions with Map and Reduce",
    "section": "combine into a function",
    "text": "combine into a function\nSo now we combine this to form a single function that calculates the value of a continuous series for \\(k\\) terms:\n\ncontinuous_frac &lt;- function(Ns, Ds, frac_fun = frac_maker){\n  Ns &lt;- rev(Ns)\n  Ds &lt;- rev(Ds)\n  funs &lt;- Map(frac_fun, Ns, Ds)\n  Reduce(function(f1,f2) f2(f1), x = funs, init = 0)\n  }\n\nNote that we have to reverse the series Ns and Ds, simply because the series are usually defined from \\(n_1\\) to \\(n_k\\), but we are building our function from \\(n_k\\) backwards."
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-numbers",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-numbers",
    "title": "Continuous fractions with Map and Reduce",
    "section": "calculating numbers",
    "text": "calculating numbers\nWith this function in hand, we can approximate any continued fraction. Here are a few examples:\n\nThe value of \\(e\\) (for biological content)\nAre you wondering what all this has to do with biology? Well, Euler’s number certainly appears in plenty of biological models, so let’s calculate it:\n\ndenominator &lt;- function(k){\n  nums &lt;- lapply(seq_len(k)*2, function(x) c(1, 1, x))\n  out &lt;- do.call(c, nums)\n  out[-1]\n}\n\n2 + continuous_frac(Ds = denominator(20), Ns = rep(1, 3 * 20 -1))\n\n[1] 2.718282"
  },
  {
    "objectID": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-pi",
    "href": "posts/2014-11-27-continuous-fraction-functional-programming/index.html#calculating-pi",
    "title": "Continuous fractions with Map and Reduce",
    "section": "calculating \\(\\pi\\)",
    "text": "calculating \\(\\pi\\)\nThere are several ways to calculate \\(\\pi\\), based on different forms of this equation. Apparently they converge at different rates. here are some examples:\n\nleibniz &lt;- function(k){\n  seqs &lt;- seq(from = 1, by = 2, length.out = k -1)\n  N &lt;- c(4, seqs ^ 2)\n  D &lt;- c(1, rep(2, k-1))\n  continuous_frac(N, D)\n}\n\nsomayaji &lt;- function(k){\n  N &lt;- seq(from = 1, by = 2, length.out = k) ^ 2\n  D &lt;- rep(6, k)\n  3 + continuous_frac(N, D)\n}\n\nlinear &lt;- function(k){\n  N &lt;- seq(from = 1, by = 1, length.out = k -1) ^ 2\n  N &lt;- c(4, N)\n  D &lt;- seq(from = 1, by = 2, length.out = k)\n  continuous_frac(N, D)\n}\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\ndata.frame(ks = seq(2, 100, by = 5)) %&gt;%\n  rowwise %&gt;%\n  mutate(leibniz = leibniz(ks),\n         somayaji = somayaji(ks),\n         linear = linear(ks)) %&gt;%\n  gather(method, pi_value, -ks) %&gt;%\n  ggplot(aes(x = ks, y = pi_value, colour = method)) + geom_point() + geom_path() + theme_bw()\n\n\n\n\n\n\n\n\nIf we zoom in we can see that the third form outperforms Somayaji’s:\n\ndata.frame(ks = seq(5, 10, by = 1)) %&gt;%\n  rowwise %&gt;%\n  mutate(somayaji = somayaji(ks),\n         linear = linear(ks)) %&gt;%\n  gather(method, pi_value, -ks) %&gt;%\n  ggplot(aes(x = ks, y = pi_value, colour = method)) + geom_point() + geom_path() + xlab(\"k\") + ylab(expression(pi)) + theme_bw()"
  },
  {
    "objectID": "posts/2024-05-03-difference_of_two_normals/index.html",
    "href": "posts/2024-05-03-difference_of_two_normals/index.html",
    "title": "Difference of normals is normal",
    "section": "",
    "text": "Motivating example\nSuppose you were calculating the growth rate of a fish! The true size of the fish increases from one year to the next. Each year is measured by a different person, such that measurement error is not the same number in each year.\nWe might calculate growth rate using the traditional calculation for relative growth rate, as the log ratio of size at time \\(t\\) and size at time \\(t-1\\). that is:\n\\[\n\\begin{align}\n&\\log \\left(\\frac{\\text{this year size}}{\\text{last year size}}\\right) \\\\\n&\\log (\\text{this year size})- \\log(\\text{last year size})\n\\end{align}\n\\]\nThe difference between two normal distributions is another normal distribution!\n\\[\n\\begin{align}\n[X_1] &\\sim \\text{N}(\\mu_1, \\sigma_1) \\\\\n[X_2] &\\sim \\text{N}(\\mu_2, \\sigma_2) \\\\\nZ &= X_1 - X_2 \\\\\n\\\\\n[Z] &\\sim \\text{N}\\left(\\mu_1 - \\mu_2, \\sqrt{\\sigma_1^2 - \\sigma_2^2}\\right)\n\\end{align}\n\\]\nThe mean of the new distribution is the differences of the two means: not surprising!\nThe standard deviation might look like a fancy formula, but it follows directly from what a variance is. A variance is a sum of squares, so when you add or subtract two normal distributions you add or subtract their sums of squares.\nHere’s a quick demonstration via simulation.\n\nsuppressPackageStartupMessages(library(tidyverse))\n\n# 3.8 is about log(45), seems like a good fish size.\ntrue_size_last_year &lt;- 3.8\ntrue_size_this_year &lt;- 3.92\n\n## observation error\nsigma_last_year &lt;- 0.05\nsigma_this_year &lt;- 0.01\n\ngrowth &lt;- purrr:::map_dbl(1:5e3,   \n            \\(x) {\n              last_year_obs &lt;- rnorm(1, true_size_last_year, sigma_last_year)\n              this_year_obs &lt;- rnorm(1, true_size_this_year, sigma_this_year)\n              \n              this_year_obs - last_year_obs}\n) \n\n\n\ngrowth |&gt; \n  enframe(value = \"growth\") |&gt; \n  ggplot(aes(x = growth)) + \n  geom_histogram(aes(y = ..density..), binwidth = .005) +\n  # stat_density() +\n  stat_function(fun = dnorm, \n                args = list(mean = true_size_this_year - true_size_last_year,\n                            sd = sqrt(abs(sigma_this_year^2 - sigma_last_year^2)))) + \nNULL"
  },
  {
    "objectID": "posts/2023-10-23-discrete-vb-brms-stan/index.html",
    "href": "posts/2023-10-23-discrete-vb-brms-stan/index.html",
    "title": "Modelling discrete growth",
    "section": "",
    "text": "library(brms)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)"
  },
  {
    "objectID": "posts/2023-10-23-discrete-vb-brms-stan/index.html#growing-things-get-bigger",
    "href": "posts/2023-10-23-discrete-vb-brms-stan/index.html#growing-things-get-bigger",
    "title": "Modelling discrete growth",
    "section": "growing things get bigger",
    "text": "growing things get bigger\nAnimals get bigger over time, and young change in size faster than more mature individuals. The classic Von Bertanaffy growth equation has animals growing from a starting size to a final asymptotic size:\n\\[\nL_t = L_0e^{-rt} + L_{max}(1 - e^{-rt})\n\\]\n\n\\(L_0\\) is the starting size\n\\(L_{max}\\) is the final size\n\\(r\\) is a growth rate\n\nThis equation yields a simple result:\n\nL0 &lt;- 13\nLmax &lt;- 120\nr &lt;- .3\ncurve(L0 * exp(-r*x) + Lmax*(1 - exp(-r * x)), xlim = c(0, 20))\n\n\n\n\n\n\n\n\nThis is the equation in continuous time\nHowever we often measure animals at discreet moments in time, having as a reference their last measurement. We can use a discrete version of this equation in these cases:\n\nvb_disc &lt;- function(L_tm1, r, time, Lmax) {\n  L_tm1 * exp(-r*time) + Lmax*(1 - exp(-r * time))\n}\n\ntimevec &lt;- rep(1, times = 13)\nsize &lt;- numeric(length(timevec)+1)\nsize[1] &lt;- 13\n\nfor (t in 1:length(timevec)){\n  size[t+1] = vb_disc(size[t],\n                      r = r,\n                      time = timevec[t],\n                      Lmax = Lmax)\n}\n\ncurve(L0 * exp(-r*x) + Lmax*(1 - exp(-r * x)),\n      xlim = c(0, 20))\npoints(cumsum(c(0,timevec)), size)\n\n\n\n\n\n\n\n\nThis works even if the points we measure at are not regular:\n\ntimevec &lt;- runif(n = 13, min = .7, max = 3)\nsize &lt;- numeric(length(timevec)+1)\nsize[1] &lt;- 13\n\nfor (t in 1:length(timevec)){\n  size[t+1] = vb_disc(size[t],\n                      r = r,\n                      time = timevec[t],\n                      Lmax = Lmax)\n}\n\ncurve(L0 * exp(-r*x) + Lmax*(1 - exp(-r * x)),\n      xlim = c(0, 20))\npoints(cumsum(c(0,timevec)), size)\n\n\n\n\n\n\n\n\nSo we can see that this is the same equation. Let’s simulate observations of a growing animal with measurement error\n\nL0 &lt;- 13\nLmax &lt;- 120\nr &lt;- .3\nsigma = 2\n\ngrow_data &lt;- tibble(time = seq(from = .5, to = 21, length.out = 40),\n       size = L0 * exp(-r* time) + Lmax*(1 - exp(-r * time)),\n       size_obs = rnorm(n = length(size), mean = size, sd = sigma))\n\ngrow_data |&gt; \n  ggplot(aes(x = time, y = size_obs)) + geom_point()"
  },
  {
    "objectID": "posts/2023-10-23-discrete-vb-brms-stan/index.html#translating-the-model-to-stan",
    "href": "posts/2023-10-23-discrete-vb-brms-stan/index.html#translating-the-model-to-stan",
    "title": "Modelling discrete growth",
    "section": "Translating the model to Stan",
    "text": "Translating the model to Stan\n\nvb_discrete &lt;- cmdstan_model(\n  here::here(\n    \"posts/2023-10-23-discrete-vb-brms-stan/vb_discrete_meas.stan\"),\n  pedantic = TRUE)\n\nvb_discrete \n\ndata {\n  int&lt;lower=0&gt; n;\n  real age_first_meas;\n  vector[n-1] time_diff;\n  vector[n] obs_size;\n  int&lt;lower=0&gt; n_pred;\n  vector[n_pred-1] diff_pred;\n}\nparameters {\n  real&lt;lower=0&gt; Lstart;\n  real&lt;lower=0&gt; Lmax;\n  real&lt;lower=0&gt; r;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  Lstart ~ normal(10, 2);\n  Lmax ~ normal(120, 10);\n  r ~ exponential(1);\n  sigma ~ exponential(1);\n\n  // could add measurment error to age\n  obs_size[1] ~ normal(Lstart * exp(-r*age_first_meas) + Lmax*(1 - exp(-r * age_first_meas)), sigma);\n  obs_size[2:n] ~ normal(obs_size[1:(n-1)] .* exp(-r*time_diff) + Lmax*(1 - exp(-r*time_diff)), sigma);\n}\ngenerated quantities {\n  vector[n_pred] mu;\n  vector[n_pred] obs;\n  mu[1] = Lstart;\n\n  for (i in 2:n_pred){\n    mu[i] = mu[i-1] .* exp(-r*diff_pred[i-1]) + Lmax*(1 - exp(-r*diff_pred[i-1]));\n  }\n\n  for( j in 1:n_pred){\n    obs[j] = normal_rng(mu[j], sigma);\n  }\n\n}\n\n\n\nsome_obs &lt;- grow_data |&gt; \n  mutate(sampled = sample(sample(0:1, length(time), replace = TRUE, prob = c(.4, .6)))) |&gt; \n  filter(sampled &gt; 0) |&gt; \n  # lagged time\n  mutate(time_diff = time - lag(time))\n\nfirst &lt;- some_obs |&gt; head(1)\nrest &lt;- some_obs |&gt; slice(-1)\n\ndiff_pred &lt;- c(rep(2, times = 5), rep(5, 3))\n\nvb_discrete_post &lt;- vb_discrete$sample(\n  data = list(\n    n = nrow(some_obs),\n    time_diff = rest$time_diff,\n    age_first_meas = first$time,\n    obs_size = some_obs$size_obs,\n    n_pred = length(diff_pred) + 1,\n    diff_pred = diff_pred\n  ),\n  refresh = 0\n  )\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\n\n\nvb_discrete_post$draws() |&gt; \n  gather_rvars(mu[i]) |&gt; \n  mutate(time = cumsum(c(0, diff_pred))) |&gt; \n  ggplot(aes(x = time, dist = .value)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\") +\n  geom_point(aes(x = time, y = size_obs), \n             inherit.aes = FALSE, data = grow_data)\n\nWarning: Using the `size` aesthetic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Unknown or uninitialised column: `linewidth`.\n\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Unknown or uninitialised column: `linewidth`.\nUnknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\n\n\n\nvb_discrete_post$summary()\n\n# A tibble: 23 × 10\n   variable    mean  median     sd    mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;      &lt;num&gt;   &lt;num&gt;  &lt;num&gt;  &lt;num&gt;   &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n 1 lp__     -21.6   -21.2   1.57   1.34   -24.6   -19.8    1.00    1697.\n 2 Lstart    10.0    10.0   1.61   1.57     7.30   12.6    1.00    3028.\n 3 Lmax     118.    118.    1.95   1.88   115.    122.     1.00    3063.\n 4 r          0.332   0.332 0.0238 0.0226   0.295   0.372  1.00    2828.\n 5 sigma      2.13    2.07  0.362  0.334    1.62    2.78   1.00    2748.\n 6 mu[1]     10.0    10.0   1.61   1.57     7.30   12.6    1.00    3028.\n 7 mu[2]     62.5    62.5   2.16   2.02    58.9    66.0    1.00    3801.\n 8 mu[3]     89.4    89.5   2.18   2.05    85.8    92.9    1.00    4074.\n 9 mu[4]    103.    103.    1.78   1.67   100.    106.     1.00    5036.\n10 mu[5]    110.    110.    1.56   1.51   108.    113.     1.00    4967.\n# ℹ 13 more rows\n# ℹ 1 more variable: ess_tail &lt;num&gt;"
  },
  {
    "objectID": "posts/2023-10-23-discrete-vb-brms-stan/index.html#can-it-be-written-in-brms",
    "href": "posts/2023-10-23-discrete-vb-brms-stan/index.html#can-it-be-written-in-brms",
    "title": "Modelling discrete growth",
    "section": "can it be written in BRMS?",
    "text": "can it be written in BRMS?\nI want to ask, what happens if we fit a similar model in brms? I’m using a lagged column of size.\nThis feels like a different model, but at least in this simple example, the posterior is close to the real value.\n\n## add a lagged growth measurement\nlagged_obs &lt;- some_obs |&gt; \n  mutate(sizelast = lag(size_obs)) |&gt; \n  # drop first row\n  slice(-1)\n\nvb_formula &lt;- bf(size_obs ~ sizelast * exp(- exp(logR) * time_diff) + \n                   sizeMax * (1 - exp(-exp(logR) * time_diff)),\n                 logR ~ 1,\n                 sizeMax ~ 1, nl = TRUE)\n\n\nget_prior(vb_formula, data = lagged_obs)\n\n                prior class      coef group resp dpar   nlpar lb ub\n student_t(3, 0, 4.3) sigma                                    0   \n               (flat)     b                              logR      \n               (flat)     b Intercept                    logR      \n               (flat)     b                           sizeMax      \n               (flat)     b Intercept                 sizeMax      \n       source\n      default\n      default\n (vectorized)\n      default\n (vectorized)\n\nvb_prior &lt;- c(\n  prior(normal(120, 10), nlpar = \"sizeMax\", class = \"b\"),\n  prior(normal(0, 1), nlpar = \"logR\", class = \"b\"),\n  prior(exponential(1), class = \"sigma\")\n)\n\n\nvb_post &lt;- brm(vb_formula, \n               data = lagged_obs,\n               prior = vb_prior,\n               file = here::here(\"posts/2023-10-23-discrete-vb-brms-stan/vb_brms.rds\"), refresh = 0)\n\n\nsummary(vb_post)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: size_obs ~ sizelast * exp(-exp(logR) * time_diff) + sizeMax * (1 - exp(-exp(logR) * time_diff)) \n         logR ~ 1\n         sizeMax ~ 1\n   Data: lagged_obs (Number of observations: 28) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nlogR_Intercept       -1.12      0.13    -1.41    -0.87 1.00     2561     2247\nsizeMax_Intercept   119.29      3.19   113.38   126.18 1.00     2492     1794\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.05      0.40     2.40     3.94 1.00     2809     2233\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nrv &lt;- c(\"b_logR_Intercept\" = \"r\", \n        \"b_sizeMax_Intercept\" = \"sizeMax\")\n\n\nvb_post |&gt; \n  gather_rvars(b_logR_Intercept, b_sizeMax_Intercept) |&gt; \n  mutate(.value = if_else(.variable == \"b_logR_Intercept\", exp(.value), .value),\n         parname = rv[.variable]) |&gt; \n  ggplot(aes(x = parname, dist = .value)) + stat_pointinterval() + \n  facet_wrap(~parname, scales = \"free\") + \n  geom_point(aes(x = parname, y = value), \n             inherit.aes = FALSE,\n             data = tribble(\n               ~parname, ~value,\n               \"r\"      , r,\n               \"sizeMax\", Lmax\n             ), col = \"red\", size = 4)\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nHm! interestingly, it seems to work just fine."
  },
  {
    "objectID": "posts/2023-03-20-latent-continuous/index.html",
    "href": "posts/2023-03-20-latent-continuous/index.html",
    "title": "Latent continuous variables",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)\nImagine we have many measurments, all correlated with each other. Where do these correlations come from? One possibility is that all these measurents are caused by the values of some unobservable, latent trait. This happens in ecology when we imagine that, for example, the Competitive Ability of a species appears to us in the form of measurable traits that (to us) seem to represent this trait: leaf size, growth rate, alleopathy, etc.\nIn mathematics, this model is the following:\n$$\n\\[\\begin{align}\nY_{ij} &\\sim \\text{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij} &= \\alpha_i \\times \\beta_j\\\\\n\\boldsymbol{ \\alpha } &\\sim \\text{Normal}(0, 1) \\\\\n\\boldsymbol{ \\beta } &\\sim \\text{Normal}(0, 1) \\\\\n\\end{align}\\]\n$$\nset.seed(1234)\n\nalpha &lt;- runif(200, min = -2, max = 2)\n\nfive_betas &lt;- c(2,-1, .5, 1, -.3)\n\nsigma_obs &lt;- .6\n\nymean &lt;- alpha %o% five_betas\n\n## sample a random number for each of these and put in back\n\nind &lt;- which(ymean != 0, arr.ind = TRUE)\n\nyobs &lt;- matrix(rep(0L, times = 200*5), nrow = 200, ncol = 5)\nyobs[ind] &lt;- rnorm(n = 200*5, mean = ymean[ind], sd = sigma_obs)\npairs(yobs)"
  },
  {
    "objectID": "posts/2023-03-20-latent-continuous/index.html#partially-constrained-model",
    "href": "posts/2023-03-20-latent-continuous/index.html#partially-constrained-model",
    "title": "Latent continuous variables",
    "section": "Partially constrained model",
    "text": "Partially constrained model\nIn this model I set constraints on the sign of the coefficients that relate the latent state to any observed variable. This is the sort of thing that would work in most ecological systems. For example, if an underlying state is “competitive ability” we might know it relates positively to traits like growth rate and leaf size, etc.\n\nlatent_cont_constrained &lt;- cmdstan_model(\n  stan_file = here::here(\n    \"posts/2023-03-20-latent-continuous/latent_cont_constrained.stan\"))\n\nlatent_cont_constrained\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 5] y;\n}\nparameters {\n  row_vector[5] beta;\n  real&lt;lower=0&gt; sigma;\n  vector[N] alpha;\n}\ntransformed parameters {\n  row_vector[5] betatrans;\n  betatrans[1] = exp(beta[1]);\n  betatrans[2] = -exp(beta[2]);\n  betatrans[3] = exp(beta[3]);\n  betatrans[4] = exp(beta[4]);\n  betatrans[5] = -exp(beta[5]);\n}\nmodel {\n  // take the outer product: alpha multiplied by each beta in turn\n  matrix[N, 5] mu = alpha * betatrans;\n  for (i in 1:N){\n    y[i] ~ normal(mu[i], sigma);\n  }\n  beta ~ std_normal();\n  sigma ~ exponential(1);\n  alpha ~ std_normal();\n}\n\n\n\nlatent_cont_constrained_samp &lt;- latent_cont_constrained$sample(\n  data = datalist, parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Location parameter[1] is inf, but must be finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c943da0867.stan', line 23, column 4 to column 32)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c943da0867.stan', line 23, column 4 to column 32)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 2 finished in 12.0 seconds.\nChain 3 finished in 12.2 seconds.\nChain 1 finished in 12.3 seconds.\nChain 4 finished in 12.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 12.2 seconds.\nTotal execution time: 12.4 seconds.\n\nlatent_cont_constrained_samp\n\n variable   mean median    sd   mad      q5    q95 rhat ess_bulk ess_tail\n lp__     -84.70 -84.05 15.31 15.23 -110.27 -60.68 1.00      618      960\n beta[1]    0.83   0.83  0.05  0.05    0.74   0.91 1.00      526      968\n beta[2]    0.18   0.17  0.06  0.06    0.08   0.28 1.00      667     1292\n beta[3]   -0.51  -0.51  0.09  0.09   -0.66  -0.37 1.00     1385     2103\n beta[4]    0.17   0.17  0.06  0.06    0.06   0.27 1.00      648     1108\n beta[5]   -1.07  -1.06  0.13  0.13   -1.30  -0.87 1.00     2402     1938\n sigma      0.60   0.60  0.02  0.01    0.57   0.62 1.00     5000     2966\n alpha[1]  -1.03  -1.02  0.21  0.22   -1.37  -0.69 1.00     5037     2288\n alpha[2]   0.48   0.48  0.21  0.21    0.14   0.83 1.00     8091     2531\n alpha[3]   0.43   0.43  0.20  0.20    0.09   0.77 1.00     8381     2650\n\n # showing 10 of 212 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n# tidybayes::get_variables(latent_cont_constrained_samp)\n\ntidybayes::gather_rvars(latent_cont_constrained_samp, betatrans[id]) |&gt; \n  mutate(true_value = five_betas) |&gt; \n  ggplot(aes(x = id, dist  = .value)) + \n  tidybayes::stat_halfeye(fill = \"darkgreen\") + \n  geom_point(aes(x = id, y = true_value), pch = 21, fill = \"orange\", size = 5)\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nSo it seems that with some simple constraints on the signs, this model samples just fine!\nDoes it also get the latent states right?\n\ntidybayes::summarise_draws(latent_cont_constrained_samp, quantile) |&gt; \n  filter(stringr::str_detect(variable, \"alpha\")) |&gt; \n  bind_cols(true_alpha = alpha) |&gt; \n  mutate(id = readr::parse_number(variable),\n         rnk = dense_rank(true_alpha)) |&gt; \n  ggplot(aes(x = `25%`, xend = `75%`, y = rnk, yend = rnk)) + \n  geom_segment() + \n  geom_point(aes(x = true_alpha, y  = rnk), inherit.aes = FALSE, col = \"red\")\n\n\n\n\n\n\n\n\nWhat if we experiment with a standard deviation for the alpha (ie a hierarchical model)\n\nlatent_cont_constr_hier &lt;- cmdstan_model(\n  stan_file = here::here(\"posts/2023-03-20-latent-continuous/latent_cont_constr_hier.stan\"))\n\nlatent_cont_constr_hier\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 5] y;\n}\nparameters {\n  row_vector[5] beta;\n  real&lt;lower=0&gt; sigma;\n  vector[N] alpha;\n  real&lt;lower=0&gt; s_alpha;\n}\ntransformed parameters {\n  row_vector[5] betatrans;\n  betatrans[1] = exp(beta[1]);\n  betatrans[2] = -exp(beta[2]);\n  betatrans[3] = exp(beta[3]);\n  betatrans[4] = exp(beta[4]);\n  betatrans[5] = -exp(beta[5]);\n}\nmodel {\n  // take the outer product: alpha multiplied by each beta in turn\n  matrix[N, 5] mu = alpha * betatrans;\n  for (i in 1:N){\n    y[i] ~ normal(mu[i], sigma);\n  }\n  beta ~ std_normal();\n  sigma ~ exponential(1);\n  alpha ~ normal(0, s_alpha);\n  s_alpha ~ exponential(.1);\n}\n\n\n\nlatent_cont_constr_hier_samp &lt;- latent_cont_constr_hier$sample(\n  data = datalist, parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 24, column 4 to column 32)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 28, column 2 to column 29)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 28, column 2 to column 29)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmpi4HWQ5/model-72c940a3b623.stan', line 28, column 2 to column 29)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 3 finished in 20.9 seconds.\nChain 4 finished in 23.3 seconds.\nChain 1 finished in 23.9 seconds.\nChain 2 finished in 25.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 23.4 seconds.\nTotal execution time: 25.7 seconds.\n\n\nWarning: 4 of 4 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\ntidybayes::summarise_draws(latent_cont_constr_hier_samp, quantile) |&gt; \n  filter(stringr::str_detect(variable, \"alpha\\\\[\")) |&gt; \n  bind_cols(true_alpha = alpha) |&gt; \n  mutate(id = readr::parse_number(variable),\n         rnk = dense_rank(true_alpha)) |&gt; \n  ggplot(aes(x = `25%`, xend = `75%`, y = rnk, yend = rnk)) + \n  geom_segment() + \n  geom_point(aes(x = true_alpha, y  = rnk), inherit.aes = FALSE, col = \"red\")"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html",
    "href": "posts/2025-03-12-useful-zeros/index.html",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(brms))"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#the-question",
    "href": "posts/2025-03-12-useful-zeros/index.html#the-question",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "The question",
    "text": "The question\nWhen placed on a host place in a nice environment with no enemies, aphids produce new aphids exponentially.[to me they seem to be bacteria with legs]{/aside} But host plants are not all good food, and on some aphids grow poorly – but on others they grow well.\nWhat is the growth rate of these aphids on different host plants? What proportion of total variance can be assigned to genotype?"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#the-design",
    "href": "posts/2025-03-12-useful-zeros/index.html#the-design",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "The design",
    "text": "The design\nA large number of genotypes are chosen, three replicates per genotype. Each plant grown alone and is “inoculated” with two aphid nymphs. At the end of 2 weeks we count all the aphids, and model growth rate by the classic exponential\n\\[\ny \\sim \\text{Poisson}(2e^{rt})\n\\] and using 2 for the time parameter to get the weekly population growth rate.\n\n\n\n\n\n\nPoisson or?\n\n\n\nI feel like there are probably multiple ways to model the observation error of population growth, and Poisson is only one. EC Pielou gives another in her book, deriving a distribution for the population size of a growing population that is based on the Binomial distribution. Here I’m going with Poisson, but another post could explore another technique!"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#the-problem",
    "href": "posts/2025-03-12-useful-zeros/index.html#the-problem",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "The problem",
    "text": "The problem\nAt the end of the experiment, we see that there are a suprisingly large number of zeros – some plants have no aphids on them! And there’s another surprising detail: when we calculate the mean and variance of aphid population size for each genotype, we find they have a negative relationship. This is surprising because normally when the average of a count is high, the variance is higher too!\nWhat’s going on?"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#the-probability-of-death-by-experimenter-m",
    "href": "posts/2025-03-12-useful-zeros/index.html#the-probability-of-death-by-experimenter-m",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "The probability of death-by-experimenter ( \\(m\\) )",
    "text": "The probability of death-by-experimenter ( \\(m\\) )\nMaybe the aphids don’t survive transfer to the host plant at the start. After all, in order to maximize the impact of the host plant on the female, the experimenters started them at as early an age as possible. I’ll say that the probability of being killed in transfer is \\(m\\). That means that by the time the aphids start reproducing, the plants can be put in three categories in three proportions:\n\n\\(m^2\\) plants have no aphids (both died)\n\\(2m(1-m)\\) plants have ONE aphid\n\\((1-m)^2\\) plants have both aphids\n\nRight away, that suggests a sort of artisanal mixture model for the aphid population sizes:\n\\[\n\\begin{align}\n\\text{Pr}(Y=0|m,r) &\\sim m^2 + 2m(1-m)\\times\\text{Poisson}(0|e^{rt}) + (1-m)^2\\times\\text{Poisson}(0|2e^{rt}) \\\\\n\\text{Pr}(Y=y|m,r) &\\sim 2m(1-m)\\times\\text{Poisson}(y|e^{rt}) + (1-m)^2\\times\\text{Poisson}(y|2e^{rt}) \\\\\nm &\\sim \\text{Beta}(2,5) \\\\\nr &\\sim \\text{Normal}(1.5, 2) \\\\\n\\end{align}\n\\] I guess you could call this a “zero-inflated mixture of Poissons”. But the name doesn’t matter as much, what is interesting it that it follows logically from the experimental situation described.\n\n\n\n\n\n\nsome observations about this model\n\n\n\n\nI feel like this notation (and the model code) would be “tighter” if you replace \\(Pois(0|e^{rt})\\) with the probability of zero in a poisson distribution: \\(e^{-\\lambda}\\) which in this case would be $e{-e{rt}}. But I also think that would make the model less readable, which is a BAD thing!\nWe decided to use a Normal prior for r since it is possible to be both negative or positive. Most of the prior probability in this model is positive, though, because these animals are supposed to do well on these plants!\nI suppose you could give \\(m\\) a logit link and model it as caused by different factors, such as the experimenter who transferred the immature insects at the start, or maybe even something about the host plants. However, here we’re treating it as a constant!"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#simulation-study-identical-hosts",
    "href": "posts/2025-03-12-useful-zeros/index.html#simulation-study-identical-hosts",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "Simulation study: identical hosts",
    "text": "Simulation study: identical hosts\nThe goal is the same as always: simulate the data-generating process and see if models can fit it! I try lots of things here, and eventually build up to fitting and testing the model i just described in a Stan program, and then in a brms workflow.\n\nExponential growth, no mortality: simplest model\nHere is a simple model of exponential population growth with poisson variation, just to confirm it works as I thought it would\n\\[\n\\begin{align}\n\\text{abd} &\\sim \\text{Poisson}(N_0e^{rt}) \\\\\nr &\\sim \\text{Normal}(1.5,.2)  \\\\\n\\end{align}\n\\]\nIn all the stan code that follows, you’ll see that I’ve written this on the log scale. I have a few reasons for doing this. One is that population growth rates are usually modelled as a log ratio, so this keeps the code looking more like the R scripts used for this kind of problem. Another is that this is a parameterization of the Poisson available in Stan, which is a bit more efficient\n\\[\n\\begin{align}\n\\text{abd} &\\sim \\text{PoissonLog}(\\ln{N_0} + rt) \\\\\nr &\\sim \\text{Normal}(1.5,.2)  \\\\\n\\end{align}\n\\] Here is a simulated dataset for plenty of replicate clones:\n\nnrep &lt;- 300\n\nr &lt;- log(6)\n\naphid_data &lt;- expand_grid(rep_id = 1:nrep, first_aphid = 1:2) |&gt;\n  # expected aphids per aphid (No = 1)\n mutate(expect_aphids = 1*exp(r*2),\n        obs_aphids = rpois(n = length(expect_aphids), lambda = expect_aphids))\n\n# combine the two aphids per plant\naphid_data_sum &lt;- aphid_data |&gt; \n  group_by(rep_id) |&gt; \n  summarize(obs_aphids = sum(obs_aphids))\n\nggplot(aphid_data_sum, aes(x = obs_aphids)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSimulated population sizes after two weeks.\n\n\n\n\nHere’s the same model, written as a Stan program\n\nsimple_growth &lt;- cmdstanr::cmdstan_model(here::here(\n  \"posts/2025-03-12-useful-zeros/simple_growth.stan\"))\n\nsimple_growth \n\ndata{\n  int n;\n  real time;\n  real N0;\n  array[n] int abd;\n}\nparameters {\n  real r;\n}\nmodel {\n  r ~ normal(1.5, 2);\n  abd ~ poisson_log(log(N0) + r*time);\n\n}\n\n\n\nsimple_growth_post &lt;- simple_growth$sample(\n  data = list(n = nrow(aphid_data_sum), \n              N0 = 2,\n              time = 2,\n              abd = aphid_data_sum$obs_aphids),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.6 seconds.\n\n\n\nbayesplot::mcmc_areas(x = simple_growth_post$draws(), pars = \"r\") + \n  geom_vline(xintercept = r, col = \"red\", size = 1) + \n  # coord_cartesian(xlim = c(r*.99, r*1.01)) +\n  NULL\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe posterior distribution covers the true value pretty well.\n\n\n\n\n\n\nExponential growth, but with transfer mortality\nAs described above, we don’t live in an easy world. We live in a world where aphids die sometimes as you transfer them. What does the data look like in that case? Let’s say that you kill about 20% (a .8 chance of surviving).\n\naphid_data_surv &lt;- aphid_data |&gt; \n  mutate(surv = rbinom(n = length(first_aphid), size = 1, prob = .8),\n         obs_aphids = obs_aphids*surv) |&gt; \n  # filter(surv == 1) |&gt; \n  group_by(rep_id) |&gt; \n  summarize(obs_aphids = sum(obs_aphids),\n            n=n(),\n            No = sum(surv))\n  \naphid_data_surv |&gt; \n  ggplot(aes(x = obs_aphids)) + \n  geom_histogram() + \n  labs(x = \"Observed aphids\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA histogram of aphid population sizes, if some aphids die when they are transferred at the experiment’s start.\n\n\n\n\n\nAlternative models\nI have already described the mixture model above, but I want to compare this model with a few other possibilities:\n\nIgnore the problem and fit a model with \\(N_0 = 2\\)\nDrop all 0 abundances, and fit the \\(N_0 = 2\\) model to the non-zero numbers\nthough experiment imagine we know the starting sizes for all the populations (perhaps thanks to observations made a few days after the start of the experiment).\nadd an observation-level random effect (on the log scale) to compensate for the variance introduced by the unpredictable starting densities.\n\nRather than write all these out in math, I’m just going to show the code for each!\nFor the first two listed above, we can reuse the original model and just change the data that goes in. The model is L\n\nsimple_growth\n\ndata{\n  int n;\n  real time;\n  real N0;\n  array[n] int abd;\n}\nparameters {\n  real r;\n}\nmodel {\n  r ~ normal(1.5, 2);\n  abd ~ poisson_log(log(N0) + r*time);\n\n}\n\n\nAnother possibility is that we know the starting population size. This of course means modifying the model to let the starting population size be data instead:\n\nsimple_growth_knownN0 &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2025-03-12-useful-zeros/simple_growth_knownN0.stan\"))\n\nsimple_growth_knownN0 \n\ndata{\n  int n;\n  real time;\n  vector[n] N0;\n  array[n] int abd;\n}\nparameters {\n  real&lt;lower=0&gt; r;\n}\nmodel {\n  abd ~ poisson_log(log(N0) + r*time);\n}\n\n\nFinally we have an observation level random effect (OLRE) which is often used to model an overdispersed poisson. On the log scale, the starting population size becomes the intercept of the linear model. The random effect for each row might help represent the extra variation caused by the intercept having unpredictably values (that is, 2, 1, or 0):\n\nsimple_growth_olre &lt;- cmdstanr::cmdstan_model(here::here(\n  \"posts/2025-03-12-useful-zeros/simple_growth_olre.stan\"))\n\nsimple_growth_olre \n\ndata{\n  int n;\n  real time;\n  real N0;\n  array[n] int abd;\n}\nparameters {\n  real r;\n  real&lt;lower=0&gt; sigma;\n  vector[n] obs_z;\n}\ntransformed parameters {\n  vector[n] obs_i = obs_z*sigma;\n}\nmodel {\n  obs_z ~ std_normal();\n  sigma ~ exponential(3);\n  r ~ normal(1.7, .2);\n  abd ~ poisson_log(log(N0) + r*time + obs_i);\n}\n\n\nFitting and comparing each model\n\nsimple_growth_mortality_post &lt;- simple_growth$sample(\n  data = list(n = nrow(aphid_data_surv), \n              N0 = 2,\n              time = 2,\n              abd = aphid_data_surv$obs_aphids),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\nno_mort_r &lt;- simple_growth_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"no mortality\")\n\nN02_0s_r &lt;- simple_growth_mortality_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"N0=2, with 0s\") \n\nbind_rows(no_mort_r, N02_0s_r)|&gt; \n  ggplot(aes(y = m, dist = r)) + \n  stat_halfeye() + \n  geom_vline(xintercept = r)\n\n\n\n\n\n\n\n\n\naphid_no0 &lt;- aphid_data_surv |&gt; \n  filter(No &gt;0)\n\nsimple_growth_dropzero_post &lt;- simple_growth$sample(\n  data = list(n = nrow(aphid_no0), \n              N0 = 2,\n              time = 2,\n              abd = aphid_no0$obs_aphids),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\nno_mort_r &lt;- simple_growth_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"no mortality\")\n\nN02_0s_r &lt;- simple_growth_mortality_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"N0=2, with 0s\") \n\nN02_drop0_r &lt;- simple_growth_dropzero_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"N0=2, drop 0s\") \n\n## known N0\nsimple_known_N0_post &lt;- simple_growth_knownN0$sample(\n  data = list(n = nrow(aphid_no0), \n              N0 = aphid_no0$No,\n              time = 2,\n              abd = aphid_no0$obs_aphids),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\nknown_N0_r &lt;- simple_known_N0_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"N0 known, with 0s\") \n\n## Observation level random effect\nsimple_growth_olre_post &lt;- simple_growth_olre$sample(\n  data = list(n = nrow(aphid_data_surv), \n              N0 = 2,\n              time = 2,\n              abd = aphid_data_surv$obs_aphids),\n  refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 4 finished in 1.0 seconds.\nChain 1 finished in 1.1 seconds.\nChain 2 finished in 1.1 seconds.\nChain 3 finished in 1.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.1 seconds.\nTotal execution time: 1.2 seconds.\n\nolre_r &lt;- simple_growth_olre_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"OLRE, with 0s\") \n  \n\nbind_rows(no_mort_r, \n          N02_0s_r, \n          N02_drop0_r,\n          known_N0_r,\n          olre_r)|&gt; \n  ggplot(aes(y = m, dist = r)) + \n  stat_halfeye() + \n  geom_vline(xintercept = r)\n\n\n\n\n\n\n\n\nNone of these work particularly well, though I’m truly surprised that the observation level random effect created such a biased estimate! Is there anything short of knowing the true value a the start of the experiment that could help us?\n\n\n\nEstimating mortality and mixtures\nHere is the mixture model from earlier, expressed in Stan\n\nmixture_growth &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2025-03-12-useful-zeros/mixture_growth.stan\"))\n\nmixture_growth \n\ndata{\n  int n;\n  real time;\n  // real N0;\n  array[n] int abd;\n}\nparameters {\n  real r;\n  real&lt;lower=0,upper=1&gt; m;\n}\nmodel {\n  m ~ beta(3,3);\n  r ~ normal(1.8, .2);\n\n  for (i in 1:n) {\n    if (abd[i] == 0) {\n      target += log_sum_exp(\n        [\n          2*log(m),\n          log(2)+log(m)+log1m(m)\n          + poisson_log_lpmf(abd[i] | r*time),\n          2*log1m(m)\n          + poisson_log_lpmf(abd[i] | log(2) + r*time)\n        ]\n        );\n    } else {\n      target += log_sum_exp(\n        log(2)+log(m)+log1m(m)\n        + poisson_log_lpmf(abd[i] | r*time),\n        2*log1m(m)\n        + poisson_log_lpmf(abd[i] | log(2) + r*time)\n        );\n    }\n  }\n}\n\n\n\nmixture_growth_post &lt;- mixture_growth$sample(\n  data = list(n = nrow(aphid_data_surv), \n              time = 2,\n              abd = aphid_data_surv$obs_aphids),\n  refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nmixture_r &lt;- mixture_growth_post |&gt; \n  tidybayes::spread_rvars(r) |&gt; \n  mutate(m = \"mixture\") \n  \n\nbind_rows(no_mort_r, \n          N02_0s_r, \n          N02_drop0_r,\n          known_N0_r,\n          olre_r,\n          mixture_r)|&gt; \n  ggplot(aes(y = m, dist = r)) + \n  stat_halfeye() + \n  geom_vline(xintercept = r)"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#simulate-the-experimental-design",
    "href": "posts/2025-03-12-useful-zeros/index.html#simulate-the-experimental-design",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "Simulate the experimental design",
    "text": "Simulate the experimental design"
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#multiple-clones",
    "href": "posts/2025-03-12-useful-zeros/index.html#multiple-clones",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "multiple clones",
    "text": "multiple clones\nIn the actual experiment there will be a large number of clones, with 3 replicates in each. (The actual experiment also includes a block ID that is crossed with genotype, but I’m leaving that out of this simulation because here I’m just focused on correctly estimating the average growth rate. In a later post I might try to extend this model to include that detail also!)\n\nngenotypes &lt;- 300\n\nrep_per_geno &lt;- 3\n\nr_bar &lt;- 1.3\n\nr_sd &lt;- .5\n\nr_geno &lt;- rnorm(n = ngenotypes, mean = r_bar, sd = r_sd)\n\ncurve(1*exp(r_bar*x), xlim = c(0,2.5), lwd = 2)\nwalk(r_geno, ~curve(exp(.*x), add = TRUE, lwd = .5))\n\n\n\n\nVariation in growth rates among plant genotypes. These curves are drawn with the base r function curve, to illustrate how variation among genotypes creates differences in exponential growth.\n\n\n\n\nNow its time to simulate the design:\n\n## project the growth of each original female\naphid_clone_data &lt;- expand_grid(\n  rep_id = 1:rep_per_geno,\n  clone_id = 1:ngenotypes,\n  first_aphid = 1:2,\n  ) |&gt;\n mutate(\n   clone_r = r_geno[clone_id],\n   expect_aphids = 1*exp(clone_r*2),\n   obs_aphids = rpois(n = length(expect_aphids), lambda = expect_aphids)\n   )\n\n## sum the offspring of each initial female into a total!\naphid_clone_total &lt;- aphid_clone_data |&gt; \n  group_by(rep_id, clone_id) |&gt; \n  summarize(clone_r = unique(clone_r),\n            expect_aphids_tot = sum(expect_aphids),\n            obs_aphids_tot = sum(obs_aphids)) |&gt; \n  ungroup()\n\n`summarise()` has grouped output by 'rep_id'. You can override using the\n`.groups` argument.\n\nknitr::kable(head(aphid_clone_total))\n\n\n\n\nrep_id\nclone_id\nclone_r\nexpect_aphids_tot\nobs_aphids_tot\n\n\n\n\n1\n1\n0.8592557\n11.15244\n11\n\n\n1\n2\n1.4512208\n36.43715\n33\n\n\n1\n3\n2.4531072\n270.25379\n282\n\n\n1\n4\n1.0450358\n16.17099\n14\n\n\n1\n5\n1.4717657\n37.96553\n37\n\n\n1\n6\n1.1723096\n20.85860\n24\n\n\n\n\n\naphid_clone_total |&gt; \n  # filter(obs_aphids_tot&gt;0) |&gt; \n  mutate(clone_id = forcats::fct_reorder(\n    as.character(clone_id), obs_aphids_tot),\n    rgr = log(obs_aphids_tot/2)/2) |&gt; \n  ggplot(aes(x = clone_id, y = rgr)) + \n  geom_point()  + \n  labs(y = \"Relative growth rate\")\n\naphid_clone_total |&gt; \n  ggplot(aes(x = obs_aphids_tot)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nRelative growth rates as measured for each clone\n\n\n\n\n\n\n\nHistogram of population sizes\n\n\n\n\n\n\nResults of a model without mortality: Relative growth rates of aphids at the end of the experiment, where genotypes are different in their growth rates.\n\n\n\n\n\n\n\n\n\nMaybe not a Poisson after all\n\n\n\nThe figure above reveals a subtle problem with this Poisson model of population growth: it generates impossible predictions for population sizes. Specifically, its possible to draw a 0 or 1 by random chance from a Poisson distribution with a low mean. But we might want to consider that these aphid populations never actually decrease, for example because even the first-born aphids survive to the end of the experiment (which is only 2 weeks, in a controlled environment with no enemies and lots of resources). There are other ways of working with this, but the full exploration will have to be a different post!\n\n\n\naphid_clone_total |&gt; \n  group_by(clone_id) |&gt;\n  summarize(mean_abd = mean(obs_aphids_tot),\n            var_abd = var(obs_aphids_tot)) |&gt; \n  ggplot(aes(x = mean_abd, y = var_abd)) + geom_point()+ \n  geom_abline(slope = 1, intercept = 0) + \n  stat_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nthe mean and variance are kind of correlated, because these are observations drawn from a poisson distribution. Compare the figure below, which shows that small sample bias can make the trend look rather far from 1:1.\n\n\n\n\n\nModel fitting to recover parameters – no mortality\nThis calls for a model of growth that is hierarchical, so that individual host genotypes have thier own average growth rate:\n\nsimple_growth_hier &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2025-03-12-useful-zeros/simple_growth_hier.stan\"))\n\nsimple_growth_hier\n\ndata{\n  int n;\n  int nclone;\n  real time;\n  // real N0;\n  array[n] int abd;\n  array[n] int clone_id;\n}\nparameters {\n  real r_bar;\n  real&lt;lower=0,upper=1&gt; m;\n  real&lt;lower=0&gt; r_sd;\n  vector[nclone] r_z;\n}\ntransformed parameters {\n  vector[nclone] r_i = r_bar + r_z*r_sd;\n}\nmodel {\n  // priors\n  m ~ beta(4,4);\n  r_bar ~ normal(1.8, .2);\n  r_sd ~ exponential(3);\n  r_z ~ std_normal();\n\n  for (i in 1:n) {\n      target +=  poisson_log_lpmf(abd[i] | log(2) + r_i[clone_id[i]]*time);\n    }\n}\n\n\nThen we run the sampler on the data from the simulation. Remember that the true values for the parameters are\n\nr_bar: 1.3\nr_sd: 0.5\n\n\nsimple_growth_hier_post &lt;- simple_growth_hier$sample(\n  data = list(n = nrow(aphid_clone_total), \n              time = 2,\n              abd = aphid_clone_total$obs_aphids_tot,\n              nclone = max(aphid_clone_total$clone_id),\n              clone_id = aphid_clone_total$clone_id),\n  refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 4.5 seconds.\nChain 3 finished in 5.7 seconds.\nChain 1 finished in 5.8 seconds.\nChain 4 finished in 5.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 5.4 seconds.\nTotal execution time: 5.9 seconds.\n\nsimple_growth_hier_post$summary(variables = c(\"r_bar\", \"m\", \"r_sd\")) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nr_bar\n1.2937220\n1.294205\n0.0314846\n0.0339071\n1.2404100\n1.3444215\n1.048016\n60.30351\n126.6434\n\n\nm\n0.4961621\n0.494539\n0.1653054\n0.1791322\n0.2279260\n0.7654714\n0.999813\n6127.19256\n3414.3892\n\n\nr_sd\n0.5051465\n0.504177\n0.0216511\n0.0206089\n0.4716005\n0.5433092\n1.053516\n149.79032\n249.8677\n\n\n\n\n\nLooks like we’re getting pretty close!\n\n\n\n\n\n\nMean-variance relationships in the Poisson distribution\n\n\n\nWe often say things like “the mean and the variace of a Poisson are both equal to the parameter \\(\\lambda\\), AKA the rate parameter”. But what does this really look like in a sample? Here is a little bit of purrr code that demonstrates what this looks like:\nplot_poisson_meanvar &lt;- function(n){\n  1:35 |&gt; \n    rep(each = 4) |&gt; \n    map(~rpois(n=n, .x ))|&gt; \n    map_df(~tibble(mean = mean(.x),\n                   var = var(.x))) |&gt; \n    ggplot(aes(x = mean, y = var)) + \n    geom_point() + \n    geom_abline(intercept = 0, slope = 1) + \n    coord_fixed() + \n    # stat_smooth(method = \"lm\") + \n    NULL\n}\n\nplot_poisson_meanvar(n = 500)\nplot_poisson_meanvar(n = 3)\n\n\n\n\n\n\n500 samples in each Poisson\n\n\n\n\n\n\n\n3 samples in each Poisson\n\n\n\n\n\n\nSamples from the Poisson distribution, as \\(\\lambda\\) increases. Sample size makes a big difference in what the mean-variance relationship looks like, and how clear it is. In these simulations I change the \\(\\lambda\\) parameter from 1 to 35 in increments of 1, and repeat the sampling 4 times for each value of \\(\\lambda\\). The only thing that is different is the sample size of each sample: either 500 samples (left) or only 3 (right).\n\n\n\nIt seems it can be pretty hard to establish a clear “expected” mean-variance relationship in a Poisson! That makes sense to me, since a variance is pretty hard to measure accurately."
  },
  {
    "objectID": "posts/2025-03-12-useful-zeros/index.html#adding-mortality",
    "href": "posts/2025-03-12-useful-zeros/index.html#adding-mortality",
    "title": "Useful Zeros: poisson mixtures and custom families",
    "section": "Adding mortality",
    "text": "Adding mortality\nThis section uses the same simulation from above but adds in the mortality during the transfer process, with a 20% chance of killing the aphid.\nsurv_prob &lt;- .8\n\n# go back to original and remove some aphids before summing\n\naphid_clone_mort_dat &lt;- aphid_clone_data |&gt; \n  mutate(surv = rbinom(length(obs_aphids), size = 1, prob = surv_prob),\n         obs_aphids_alive = obs_aphids * surv)\n\naphid_clone_mort_sum &lt;- aphid_clone_mort_dat |&gt;\n  group_by(clone_id, rep_id) |&gt;\n  summarize(tot_aphids = sum(obs_aphids_alive))\n\n`summarise()` has grouped output by 'clone_id'. You can override using the\n`.groups` argument.\n\naphid_clone_mort_sum |&gt; \n  summarize(mean_abd = mean(tot_aphids),\n            var_abd = var(tot_aphids)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x = mean_abd, y =var_abd)) + geom_point() + \n  geom_abline(slope = 1, intercept = 0) + \n  stat_smooth(method = \"lm\") + \n  # coord_cartesian(xlim = c(0, 150), ylim = c(-20,400))\n  NULL\n\n`geom_smooth()` using formula = 'y ~ x'\n\naphid_clone_mort_sum |&gt; \n  ungroup() |&gt; \n  mutate(clone_id = forcats::fct_reorder(\n    as.character(clone_id), tot_aphids)) |&gt; \n  ggplot(aes(x = clone_id, y = log(tot_aphids))) + geom_point()\n\n\n\n\n\n\nSome initial mortality in aphids being transferred to plants means that the variance gets bigger faster than the average.\n\n\n\n\n\n\n\nSome initial mortality in aphids being transferred to plants means that the variance gets bigger faster than the average.\n\n\n\n\n\nThe mixture model from above only needs one extension to work for these data: add a random effect on the r parameter:\n\nmixture_growth_hier &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2025-03-12-useful-zeros/mixture_growth_hier.stan\"))\n\nmixture_growth_hier\n\ndata{\n  int n;\n  int nclone;\n  real time;\n  // real N0;\n  array[n] int abd;\n  array[n] int clone_id;\n}\nparameters {\n  real r_bar;\n  real&lt;lower=0,upper=1&gt; m;\n  real&lt;lower=0&gt; r_sd;\n  vector[nclone] r_z;\n}\ntransformed parameters {\n  vector[nclone] r_i = r_bar + r_z*r_sd;\n}\nmodel {\n  // priors\n  m ~ beta(4,4);\n  r_bar ~ normal(1.8, .2);\n  r_sd ~ exponential(3);\n  r_z ~ std_normal();\n\n  for (i in 1:n) {\n    if (abd[i] == 0) {\n      target += log_sum_exp(\n        [\n          2*log(m),\n          log(2)+log(m)+log1m(m)\n          + poisson_log_lpmf(abd[i] | r_i[clone_id[i]]*time),\n          2*log1m(m)\n          + poisson_log_lpmf(abd[i] | log(2) + r_i[clone_id[i]]*time)\n        ]\n        );\n    } else {\n      target += log_sum_exp(\n        log(2)+log(m)+log1m(m)\n        + poisson_log_lpmf(abd[i] | r_i[clone_id[i]]*time),\n        2*log1m(m)\n        + poisson_log_lpmf(abd[i] | log(2) + r_i[clone_id[i]]*time)\n        );\n    }\n  }\n}\n\n\nAnd we feed it precisely the same data as in the heierarcical model that didn’t know about transfer mortality:\n\nr_bar: 1.3\nm: 0.2\nr_sd: 0.5\n\n\nmixture_growth_post &lt;- mixture_growth_hier$sample(\n  data = list(n = nrow(aphid_clone_mort_sum), \n              time = 2,\n              abd = aphid_clone_mort_sum$tot_aphids,\n              nclone = max(aphid_clone_mort_sum$clone_id),\n              clone_id = aphid_clone_mort_sum$clone_id),\n  refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 21.3 seconds.\nChain 4 finished in 21.7 seconds.\nChain 1 finished in 21.9 seconds.\nChain 2 finished in 21.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 21.7 seconds.\nTotal execution time: 21.9 seconds.\n\nmixture_growth_post$summary()\n\nWarning: The ESS has been capped to avoid unstable estimates.\n\n\n# A tibble: 604 × 10\n   variable      mean    median      sd     mad       q5      q95  rhat ess_bulk\n   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lp__     -3254.    -3254.    20.2    20.2    -3.29e+3 -3.22e+3  1.09    45.7 \n 2 r_bar        1.30      1.30   0.0289  0.0284  1.26e+0  1.35e+0  1.03   196.  \n 3 m            0.203     0.203  0.0136  0.0135  1.81e-1  2.26e-1  1.05    55.6 \n 4 r_sd         0.509     0.508  0.0232  0.0229  4.72e-1  5.48e-1  1.01   270.  \n 5 r_z[1]      -1.31     -1.31   0.295   0.292  -1.79e+0 -8.35e-1  1.00  4352.  \n 6 r_z[2]       0.295     0.289  0.131   0.123   8.72e-2  5.15e-1  1.01   693.  \n 7 r_z[3]       2.40      2.32   0.240   0.181   2.10e+0  2.86e+0  1.53     7.50\n 8 r_z[4]      -0.663    -0.659  0.192   0.186  -9.83e-1 -3.54e-1  1.00  2770.  \n 9 r_z[5]       0.265     0.223  0.221   0.161  -7.14e-4  8.03e-1  1.01   656.  \n10 r_z[6]      -0.142    -0.147  0.187   0.162  -4.29e-1  1.46e-1  1.00  1957.  \n# ℹ 594 more rows\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nI THOUGHT this model was working poorly but it turns out that it was just fine – I was forgetting to sum the aphids before I ran the model. I only found my mistake because I spent a good amount of time digging back into my code to confirm.\nHere is one of the stripped-down models that I experimented with before finally realizing why it didn’t work.\n\nsimple_growth_NOThier &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2025-03-12-useful-zeros/simple_growth_NOThier.stan\"))\n\nsimple_growth_NOThier\n\ndata{\n  int n;\n  int nclone;\n  real time;\n  // real N0;\n  array[n] int abd;\n  array[n] int clone_id;\n}\nparameters {\n  // real r_bar;\n  real&lt;lower=0,upper=1&gt; m;\n  // real&lt;lower=0&gt; r_sd;\n  vector[nclone] r_i;\n}\n// transformed parameters {\n//   vector[nclone] r_i = r_bar + r_z*r_sd;\n// }\nmodel {\n  // priors\n  // m ~ beta(4,4);\n  // r_bar ~ normal(1.8, .2);\n  // r_sd ~ exponential(3);\n  // r_z ~ std_normal();\n  r_i ~ normal(1.8, .3);\n\n  for (i in 1:n) {\n      target +=  poisson_log_lpmf(abd[i] | log(2) + r_i[clone_id[i]]*time);\n    }\n}\n\n\n\n\n\nAttempting a brms model (doesn’t work)\n\n# sum(aphid_clone_mort_sum$tot_aphids&lt;1)\n# aphid_clone_mort_sum$tot_aphids |&gt; length()\n# \n# wt_avg_bf &lt;- brms::bf(\n#   tot_aphids ~ log(2*m*(1-m) + (1-m)^2) - log(1-m^2) + r*2,\n#   hu ~ m^2,\n#   nl = TRUE,\n#   r ~ 1 + (1|clone_id),\n#   m ~ 1,\n#   family = hurdle_poisson(link = \"log\", link_hu = \"identity\")\n#   )\n# \n# get_prior(wt_avg_bf, data = aphid_clone_mort_sum )\n# \n# wt_avg_priors &lt;- c(\n#   prior(beta(.2*45, (1-.2)*45),\n#         nlpar = \"m\",class = \"b\", coef = \"Intercept\", lb = 0, ub=1),\n#   prior(normal(1.8, .3), nlpar = \"r\", class = \"b\", coef = \"Intercept\"),\n#   prior(exponential(3), nlpar = \"r\", class = \"sd\")\n# )\n# \n# wt_avg_post &lt;- brm(wt_avg_bf, prior = wt_avg_priors, data = aphid_clone_mort_sum)\n\nTurns out, it isn’t really possible to use a “nonlinear” equation in both parts of the model – that is, in the expression for the poisson and also for the number of 0s.\n\n\nbrms custom distribution\nThis is the first time I’ve ever tried to make a custom family in brms. So I’ve broken the process down into steps.\n\nmake sure I have working Stan functions\nrun the brms code from the vignette with my new functions as a custom family\nfit to simulated data.\n\n\n\nWriting Stan program – with functions\nLet’s return to the simplest simulation I made: no variation among host plant clones, just replicates:\nOnce again, it was this model:\n\nmixture_growth &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2025-03-12-useful-zeros/mixture_growth.stan\"))\n\nmixture_growth \n\ndata{\n  int n;\n  real time;\n  // real N0;\n  array[n] int abd;\n}\nparameters {\n  real r;\n  real&lt;lower=0,upper=1&gt; m;\n}\nmodel {\n  m ~ beta(3,3);\n  r ~ normal(1.8, .2);\n\n  for (i in 1:n) {\n    if (abd[i] == 0) {\n      target += log_sum_exp(\n        [\n          2*log(m),\n          log(2)+log(m)+log1m(m)\n          + poisson_log_lpmf(abd[i] | r*time),\n          2*log1m(m)\n          + poisson_log_lpmf(abd[i] | log(2) + r*time)\n        ]\n        );\n    } else {\n      target += log_sum_exp(\n        log(2)+log(m)+log1m(m)\n        + poisson_log_lpmf(abd[i] | r*time),\n        2*log1m(m)\n        + poisson_log_lpmf(abd[i] | log(2) + r*time)\n        );\n    }\n  }\n}\n\n\nand these data\n\naphid_data_surv |&gt; \n  ggplot(aes(x = obs_aphids)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAnd the fit was excellent, recovering true parameters:\n\nmixture_growth_post &lt;- mixture_growth$sample(\n  data = list(n = nrow(aphid_data_surv), \n              time = 2,\n              abd = aphid_data_surv$obs_aphids),\n  refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nmixture_growth_post$summary()\n\n# A tibble: 3 × 10\n  variable      mean    median      sd     mad        q5      q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -1245.    -1244.    0.951   0.697   -1246.    -1.24e+3  1.00    2114.\n2 r            1.79      1.79  0.00407 0.00414     1.79   1.80e+0  1.00    4011.\n3 m            0.187     0.187 0.0157  0.0156      0.161  2.13e-1  1.00    1982.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\ntrue \\(r\\) was 1.7917595\n\nRewriting it with a function\nMy plan is to rewrite the Stan program with a function in it\n\nmixture_growth_fn &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2025-03-12-useful-zeros/mixture_growth_fn.stan\"))\n\nmixture_growth_fn \n\nfunctions {\n  real poisson_mix_mortality(int abd_i, real mu, real m) {\n    real ll;\n    if (abd_i == 0) {\n      ll = log_sum_exp(\n        [\n          2 * log(m),\n          log(2) + log(m) + log1m(m) + poisson_log_lpmf(abd_i | mu),\n          2 * log1m(m) + poisson_log_lpmf(abd_i | log(2) + mu)\n        ]\n      );\n    } else {\n      ll = log_sum_exp(\n        log(2) + log(m) + log1m(m) + poisson_log_lpmf(abd_i | mu),\n        2 * log1m(m) + poisson_log_lpmf(abd_i | log(2) + mu)\n      );\n    }\n    return ll;\n  }\n}\ndata{\n  int n;\n  real time;\n  // real N0;\n  array[n] int abd;\n}\nparameters {\n  real r;\n  real&lt;lower=0,upper=1&gt; m;\n}\nmodel {\n  real mu = r*time;\n  for (i in 1:n) {\n    target += poisson_mix_mortality(abd[i], mu, m);\n  }\n}\n\n\n\nmixture_growth_fn_post &lt;- mixture_growth_fn$sample(\n  data = list(n = nrow(aphid_data_surv), \n              time = 2,\n              abd = aphid_data_surv$obs_aphids),\n  refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n\nmixture_growth_fn_post$summary()\n\n# A tibble: 3 × 10\n  variable      mean    median      sd     mad        q5      q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -1241.    -1241.    0.957   0.712   -1243.    -1.24e+3  1.00    1676.\n2 r            1.79      1.79  0.00411 0.00406     1.79   1.80e+0  1.00    4098.\n3 m            0.185     0.185 0.0156  0.0159      0.160  2.11e-1  1.00    2216.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nresults are the same! Now to inject it into brms\n\n\n\ncustom brms distribuion\nThis is the code taken straight from the brms vignette linked above, and adapted for our situation:\n\npoisson_mix_mortality &lt;- custom_family(\n  \"poisson_mix_mortality\",\n  dpars = c(\"mu\", \"m\"),\n  links = c(\"identity\", \"identity\"),\n  lb = c(NA, 0), ub = c(NA,1),\n  type = \"int\", \n  #vars = \"vint1[n]\"\n  loop = TRUE\n)\n\nDefine Stan functions\n\npoisson_mix_mortality_fns &lt;- \"\nreal poisson_mix_mortality_lpmf(int abd_i, real mu, real m) {\n    real ll;\n    if (abd_i == 0) {\n      ll = log_sum_exp(\n        [\n          2 * log(m),\n          log(2) + log(m) + log1m(m) + poisson_log_lpmf(abd_i | mu),\n          2 * log1m(m) + poisson_log_lpmf(abd_i | log(2) + mu)\n        ]\n      );\n    } else {\n      ll = log_sum_exp(\n        log(2) + log(m) + log1m(m) + poisson_log_lpmf(abd_i | mu),\n        2 * log1m(m) + poisson_log_lpmf(abd_i | log(2) + mu)\n      );\n    }\n    return ll;\n  }\nint poisson_mix_mortality_rng(real mu, real m) {\n real p1 = square(m);  // Pr[0] component: both die\n real p2 = 2 * m * (1 - m);  // One dies, one lives\n real p3 = square(1 - m);    // Both live\n\n    // Normalize to ensure valid probabilities\n real total = p1 + p2 + p3;\n p1 /= total;\n p2 /= total;\n p3 /= total;\n\n    // Sample which mortality path to take\n    real u = uniform_rng(0, 1);\n    int n;\n    if (u &lt; p1) {\n      n = 0;  // both dead\n    } else if (u &lt; p1 + p2) {\n      n = poisson_log_rng(mu);  // one survives\n    } else {\n      n = poisson_log_rng(log(2) + mu);  // both survive\n    }\n\n    return n;\n  }\n\"\n\nstanvars &lt;- stanvar(scode = poisson_mix_mortality_fns,\n                    block = \"functions\")\n\nWith this objects created, the next step is to assemble the brms model formula:\n\npoisson_mix_bf &lt;- bf(\n  tot_aphids ~ 0 + time + (0 + time | clone_id), \n  family = poisson_mix_mortality)\n\npoisson_mix_bf\n\ntot_aphids ~ 0 + time + (0 + time | clone_id) \n\n\nUsing brms in this way lets us define a prior over parameters that I came up with myself!\n\nget_prior(poisson_mix_bf, data = aphid_clone_mort_sum |&gt; \n    mutate(time = 2))\n\n                 prior class coef    group resp dpar nlpar lb ub       source\n                (flat)     b                                          default\n                (flat)     b time                                (vectorized)\n                (flat)     m                                0  1      default\n student_t(3, 0, 20.8)    sd                                0         default\n student_t(3, 0, 20.8)    sd      clone_id                  0    (vectorized)\n student_t(3, 0, 20.8)    sd time clone_id                  0    (vectorized)\n\npois_mix_prior &lt;- c(\n  prior(normal(1.5, 2), class = \"b\", coef = \"time\"),\n  prior(beta(7*.3, 7*(1-.3)), class = \"m\", lb = 0, ub = 1),\n  prior(exponential(3), class=\"sd\", lb = 0)\n)\n\npois_mix_prior\n\n                        prior class coef group resp dpar nlpar   lb   ub source\n               normal(1.5, 2)     b time                       &lt;NA&gt; &lt;NA&gt;   user\n beta(7 * 0.3, 7 * (1 - 0.3))     m                               0    1   user\n               exponential(3)    sd                               0 &lt;NA&gt;   user\n\n\nNow instead of a Stan-style list we can use the regular dataframe:\n\nhead(aphid_clone_mort_sum)\n\n# A tibble: 6 × 3\n# Groups:   clone_id [2]\n  clone_id rep_id tot_aphids\n     &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n1        1      1          5\n2        1      2          3\n3        1      3          8\n4        2      1         33\n5        2      2         18\n6        2      3         41\n\n\nWe just need to add a time column (which is all the same number) and put it in the regular model syntax:\n\nhier_poisson_mix_brm &lt;- brm(\n  tot_aphids ~ 0 + time + (0 + time | clone_id), \n  data = aphid_clone_mort_sum |&gt; \n    mutate(time = 2),\n  family = poisson_mix_mortality,\n  stanvars = stanvars,\n  prior = pois_mix_prior,\n  chains = 2, cores = 2, refresh = 0\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\nWarning: The largest R-hat is 1.84, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nhier_poisson_mix_brm\n\nWarning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be\ncareful when analysing the results! We recommend running more iterations and/or\nsetting stronger priors.\n\n\n Family: poisson_mix_mortality \n  Links: mu = identity; m = identity \nFormula: tot_aphids ~ 0 + time + (0 + time | clone_id) \n   Data: mutate(aphid_clone_mort_sum, time = 2) (Number of observations: 900) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nMultilevel Hyperparameters:\n~clone_id (Number of levels: 300) \n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(time)     0.51      0.02     0.47     0.56 1.00      183      418\n\nRegression Coefficients:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntime     1.30      0.03     1.24     1.36 1.03       98      244\n\nFurther Distributional Parameters:\n  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nm     0.21      0.01     0.18     0.24 1.06       27      889\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/2022-11-21-growth-curve-known-age/index.html",
    "href": "posts/2022-11-21-growth-curve-known-age/index.html",
    "title": "Simple nonlinear growth",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(brms))"
  },
  {
    "objectID": "posts/2022-11-21-growth-curve-known-age/index.html#growth-when-you-know-the-age",
    "href": "posts/2022-11-21-growth-curve-known-age/index.html#growth-when-you-know-the-age",
    "title": "Simple nonlinear growth",
    "section": "Growth when you know the age",
    "text": "Growth when you know the age\nWe’re doing a lot of work with growth equations these days! This is how to use brms to fit the growth of an animal when we know:\n\nthe birth year\nsize at each year (measured as the length of a leg)\ntime\n\nWe’ll start with the classic VB growth equation that has been in several other posts:\n\\[\nL_t = L_0e^{-rt} + L_\\infty(1 - e^{-rt})\n\\tag{1}\\]\nThe model we use will resemble the others as well:\n\\[\n\\begin{align}\n\\text{Measurements} &\\sim \\text{Normal}(L_t, \\sigma_{meas})\\\\\nL_t &= L_0e^{-rt} + L_\\infty(1 - e^{-rt}) \\\\\nL_0 &\\sim ...\\\\\nL_\\infty &\\sim ...\\\\\nr &\\sim ...\n\\end{align}\n\\]\n\nSimulating data\nI’m starting off with a function to simulate data; this will make it easy to repeat experiments with this model.\n\nsimulate_one_growth_known_age &lt;- function(age, r,\n                                          Lmax,\n                                          size_at_first,\n                                          sd_obs){\n  tibble(age = 0:age,\n         size = size_at_first * exp(-r * age) + Lmax * (1 - exp(-r * age)),\n         obs_size = rnorm(n = length(age), mean = size, sd = sd_obs))\n}\n\n\none_animal &lt;- simulate_one_growth_known_age(9, Lmax = 550, size_at_first = 277, r = .7, sd_obs = 6)\n\none_animal |&gt; \n  ggplot(aes(x = age, y = obs_size)) + \n  geom_point() + \n  theme_bw() + \n  geom_line(aes(y = size)) + \n  labs(x = \"Age\", y = \"Size\") \n\n\n\n\nGrowth curve for a single individual. The curved line is the true size, and the dots are observations around it. The observations are taken in the field while the semi-tranquilized animal is struggling, so they show some slight variation."
  },
  {
    "objectID": "posts/2022-11-21-growth-curve-known-age/index.html#nonlinear-modelling-with-brms",
    "href": "posts/2022-11-21-growth-curve-known-age/index.html#nonlinear-modelling-with-brms",
    "title": "Simple nonlinear growth",
    "section": "Nonlinear modelling with BRMS",
    "text": "Nonlinear modelling with BRMS\nThere are three steps to defining and elementary model with brms:\n\nwrite the model\nwrite down some priors\ncondition the model on data\n\nIn practice there are many more steps, including prior predictive checks to make sure our priors make sense. In this post I’m going to focus on the mechanistic how-to of fitting a nonlinear model in brms and I’ll come back to Prior Predictive checks, which I love, in another post.\nFirst we define the model, here we need to indicate what are parameters by doing a ~1 after each. Yes it is a formula with multiple little formulae inside it! Feel the power flow through you.\n\nvb_form &lt;- bf(obs_size ~ startsize * exp(-growthrate * age) + maxsize * (1 - exp(-growthrate * age)),\n              startsize ~ 1, \n              growthrate ~ 1,\n              maxsize ~ 1,\n              nl = TRUE,\n              family = gaussian())\n\nget_prior(vb_form, data = one_animal)\n\n                 prior class      coef group resp dpar      nlpar lb ub\n student_t(3, 0, 22.6) sigma                                       0   \n                (flat)     b                           growthrate      \n                (flat)     b Intercept                 growthrate      \n                (flat)     b                              maxsize      \n                (flat)     b Intercept                    maxsize      \n                (flat)     b                            startsize      \n                (flat)     b Intercept                  startsize      \n       source\n      default\n      default\n (vectorized)\n      default\n (vectorized)\n      default\n (vectorized)\n\nvb_prior &lt;- c(\n  prior(exponential(1), class = \"sigma\"),\n  prior(normal(0,1), nlpar = \"growthrate\", lb = 0),\n  prior(normal(550, 20), nlpar = \"maxsize\", lb = 0),\n  prior(normal(200, 50), nlpar = \"startsize\", lb = 0)\n)\n\nvb_model &lt;- brm(formula = vb_form,\n                prior = vb_prior, \n                data = one_animal, \n                chains = 2, \n                file = here::here(\"posts/2022-11-21-growth-curve-known-age/vb_model.rds\"))\n\n\none_animal |&gt; \n  tidybayes::add_predicted_rvars(vb_model) |&gt; \n  ggplot(aes(x = age, dist = .prediction)) + \n  stat_dist_lineribbon() + \n  geom_point(aes(x = age, y = obs_size), inherit.aes = FALSE) \n\nWarning: Using the `size` aesthietic with geom_ribbon was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Unknown or uninitialised column: `linewidth`.\n\n\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\nWarning: Unknown or uninitialised column: `linewidth`.\nUnknown or uninitialised column: `linewidth`.\n\n\n\n\n\n\n\n\n\n\nsummary(vb_model)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: obs_size ~ startsize * exp(-growthrate * age) + maxsize * (1 - exp(-growthrate * age)) \n         startsize ~ 1\n         growthrate ~ 1\n         maxsize ~ 1\n   Data: one_animal (Number of observations: 10) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nstartsize_Intercept    275.83      4.47   266.81   284.49 1.00     1050\ngrowthrate_Intercept     0.66      0.03     0.61     0.72 1.00      918\nmaxsize_Intercept      551.62      2.43   546.94   556.62 1.00      814\n                     Tail_ESS\nstartsize_Intercept       866\ngrowthrate_Intercept      796\nmaxsize_Intercept         765\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.51      0.96     3.05     6.76 1.00      899      951\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/2023-11-03-power-law/index.html",
    "href": "posts/2023-11-03-power-law/index.html",
    "title": "Power laws on log-linear plots",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/2023-11-03-power-law/index.html#the-backgroung",
    "href": "posts/2023-11-03-power-law/index.html#the-backgroung",
    "title": "Power laws on log-linear plots",
    "section": "The backgroung",
    "text": "The backgroung\nPower laws are very common in ecology. One reason for this is surely how easy it is to fit and plot. There are also many reasons based on theory, for example famous work on metabolic scaling."
  },
  {
    "objectID": "posts/2023-11-03-power-law/index.html#the-math",
    "href": "posts/2023-11-03-power-law/index.html#the-math",
    "title": "Power laws on log-linear plots",
    "section": "The Math",
    "text": "The Math\n“Power law” is just a fancy name for a polynomial with an exponent between 0 and 1. This gives a declerating curve (figure below)\nPower laws look like this:\n\\[\ny = ax^b\n\\]\nIt becomes linear if you log both sides, like this:\n\\[\n\\ln{y} = \\ln{a} + b\\ln{x}\n\\]\nThis is a format we can fit with our favourite linear OLS function, lm()."
  },
  {
    "objectID": "posts/2023-11-03-power-law/index.html#simulate-data",
    "href": "posts/2023-11-03-power-law/index.html#simulate-data",
    "title": "Power laws on log-linear plots",
    "section": "Simulate data",
    "text": "Simulate data\nWe’re simulating data here to roughly imitate a dataset on bumblebees collected by Amélie Morin from Université Laval.\nThis is a very simplified look at only one part of the dataset. Here we are asking if there are more bumblebees in longer strips of habitat. The independent variable is the length of the strip, in meters. The response is the observed counts of bumblebees.\nWe begin by picking parameter values and a range of simulated X variables\n\na &lt;- 11\nb &lt;- 0.3\n# longeur &lt;- runif(104, min = 2, max = 25)\nlongeur &lt;- sample(c(2:7, 10, 12, 15, 25), size = 104, replace = TRUE)\n\nmoy_bourdons &lt;- a * longeur ^b\n\nplot(longeur, moy_bourdons)\n\n\n\n\n\n\n\n\nWe simulate observations of counts using a Poisson distribution, so that the data resemble biological reality.\n\nobs_bourdons &lt;- rpois(104, lambda = moy_bourdons)\n\n\ntibble(longeur, obs_bourdons) |&gt; \n  ggplot(aes(x = longeur, y = obs_bourdons)) + \n  geom_count()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere we are using a model which is similar to, but not identical to, the model we used to generate the data. That is to emphasize that while this technique (i.e. a linear model on log-transformed data) is not perfect, it still gets very close and is a great solution in many cases."
  },
  {
    "objectID": "posts/2023-11-03-power-law/index.html#fit-a-power-law",
    "href": "posts/2023-11-03-power-law/index.html#fit-a-power-law",
    "title": "Power laws on log-linear plots",
    "section": "Fit a power law",
    "text": "Fit a power law\nWe fit the model by log-transforming and then fitting a straightforward linear model.\n\nbourdon_data &lt;- tibble(longeur, obs_bourdons)\n\nbourdon_data_log &lt;- bourdon_data |&gt; \n  mutate(log_longeur = log(longeur),\n         log_obs_bourdons = log(obs_bourdons))\n\n\nlm_pwr &lt;- lm(log_obs_bourdons ~ log_longeur, data = bourdon_data_log)"
  },
  {
    "objectID": "posts/2023-11-03-power-law/index.html#predict",
    "href": "posts/2023-11-03-power-law/index.html#predict",
    "title": "Power laws on log-linear plots",
    "section": "Predict",
    "text": "Predict\nTo draw a line and confidence intervals we make predictions. However these are NOT predictions on the original data, but on a nice even sequence of values on the x-axis.\nA few points to note here:\n\nI take a range of x-values and then log-transform them, then transform them back. This imitates the same process that we went throught with the actual data\nI’m using broom::augment from the broom package. It is awesome! It takes a model and a dataset, and adds the prediction to the dataset as a new set of columns. See ?broom::augment for all the details.\nOnce we have the predictions, we need to back-transform everything by exponentiating with exp(): the average response (.fitted), the lower and upper confidence intervals (.lower and .upper) and of course the x-axis values. Here i use a little function from the tidyverse to apply exp() to all columns at once. Learn more by typing vignette(\"colwise\").\n\n\nnew_data &lt;- tibble(log_longeur = log(1:25))\n\npredictions_pwr &lt;- broom::augment(x = lm_pwr,\n                                  newdata = new_data,\n                                  interval = \"confidence\") |&gt;\n  mutate(across(everything(), exp))"
  },
  {
    "objectID": "posts/2023-11-03-power-law/index.html#plot",
    "href": "posts/2023-11-03-power-law/index.html#plot",
    "title": "Power laws on log-linear plots",
    "section": "Plot",
    "text": "Plot\nFinally we plot the transformed predictions using ggplot2:\n\npredictions_pwr |&gt; \n  ggplot(aes(x = log_longeur, y = .fitted)) + \n  geom_ribbon(\n    aes(x = log_longeur,\n        ymin = .lower,\n        ymax = .upper), \n    inherit.aes = FALSE, fill = \"lightgreen\") + \n  geom_line() + \n  geom_count(aes(x = longeur, y = obs_bourdons), data = bourdon_data) + \n  geom_vline(xintercept = 12, lty = 2, col = \"red\")"
  },
  {
    "objectID": "posts/2016-12-02-twitter-recommends-stats-books/index.html",
    "href": "posts/2016-12-02-twitter-recommends-stats-books/index.html",
    "title": "Twitter recommends stats books",
    "section": "",
    "text": "Yesterday I asked my beloved Twitter nerds to recommend to me their favourite quantitative texts in Ecology:\n\n\nmy postdoc super tells me I can BUY BOOKS! 💓📚📚💓 quick sweet friends, what are your fun statistical ecology book recommendations?\n\n— Andrew MacDonald (@polesasunder) December 2, 2016\n\n\nAs a way of saying “Thank you!” I thought that I would put all the books in a list for anyone who is curious.\n\nStats texts\n\nStatistics for Spatio-Temporal Data, by Noel Cressie, Christopher K. Wikle, Wiley-Blackwell (19 avril 2011)\nStatistical Rethinking: A Bayesian Course with Examples in R and Stan by Richard McElreath – This one was a HUGE favourite recommended by multiple people. Going straight to the top of my list!\nApplied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS, 1st Edition- by Kery & Royle, Academic Press on the strength of recs from two fine quantitative folks, Auriel Fournier and Petr Keil\nQuantitative Ecology and Evolutionary Biology: Integrating models with data (Oxford Series in Ecology and Evolution) 1st Edition by Otso Ovaskainen, Henrik Johan de Knegt, Maria del Mar Delgado\nBayesian Models - A Statistical Primer for Ecologists (Anglais) Relié – 25 août 2015 - de N Thompson Hobbs , Mevin Hooten\nGeneralized Additive Models: An Introduction with R, Second Edition (Chapman & Hall/CRC Texts in Statistical Science) 2nd Edition by Simon N. Wood\nRegression Modeling Strategies With Applications to Linear Models, Logistic Regression, and Survival Analysis by Harrell, Frank\nGraph Theory and Complex Networks: An Introduction by Maarten van Steen\nData Analysis Using Regression and Multilevel/Hierarchical Models 1st Edition by Andrew Gelman, Jennifer Hill – this one (which I had already) was another very frequent recommendation!\n\n\n\nClassic works:\n\nA Crude Look at the Whole: The Science of Complex Systems in Business, Life, and Society by John H. Miller\nDaedalus, or Science and the Future by JBS Haldane\nEvidence and Evolution: the Logic behind the Science by Elliott Sober\nBiology as Ideology, Lewontin\n\nSo, as Margaret Kosmala of Ec0l0gy b1ts says: what are we reading next?"
  },
  {
    "objectID": "posts/2023-12-14-contrasts/index.html",
    "href": "posts/2023-12-14-contrasts/index.html",
    "title": "Contrasts",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nmeans &lt;- c(3,4,9)\n\nneach &lt;-  25\nys &lt;- rnorm(n = neach*length(means), mean = rep(means, each = neach), sd = .3)\nxs &lt;- rep(letters[1:length(means)], each = neach)\n\nmod &lt;- lm(ys ~ xs)\nxs &lt;- as.factor(xs)\n\ncontrasts(xs) &lt;- contr.helmert(3)\ncontrasts(xs) &lt;- sweep(contr.helmert(3), MARGIN = 2, STATS = 2:3, FUN = `/`)\n\nsweep(matrix(1, nrow = 4, ncol = 3), MARGIN = 2,STATS = 1:3, FUN =  `/`)\n\n     [,1] [,2]      [,3]\n[1,]    1  0.5 0.3333333\n[2,]    1  0.5 0.3333333\n[3,]    1  0.5 0.3333333\n[4,]    1  0.5 0.3333333\n\nmod_helm &lt;- lm(ys ~ xs)\n\nsummary(mod_helm)\n\n\nCall:\nlm(formula = ys ~ xs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56666 -0.18801 -0.03627  0.17782  0.62273 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.29433    0.03218  164.50   &lt;2e-16 ***\nxs1          1.13516    0.07883   14.40   &lt;2e-16 ***\nxs2          5.48811    0.06827   80.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2787 on 72 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.989 \nF-statistic:  3335 on 2 and 72 DF,  p-value: &lt; 2.2e-16\n\nmean(ys)\n\n[1] 5.294333\n\nmeans[2] - means[1]\n\n[1] 1\n\nmeans[3] - mean(means[1:2])\n\n[1] 5.5"
  },
  {
    "objectID": "posts/2023-12-14-contrasts/index.html#helmert-contrasts",
    "href": "posts/2023-12-14-contrasts/index.html#helmert-contrasts",
    "title": "Contrasts",
    "section": "Helmert contrasts",
    "text": "Helmert contrasts\n\nsuppressPackageStartupMessages(library(tidyverse))\n\n# plot the means\n\nhh &lt;- coef(mod_helm)\n\ncc &lt;- RColorBrewer::brewer.pal(3, \"Set2\")\n\n\nhelmert &lt;- tibble(xs, ys) |&gt; \n  ggplot(aes(x = xs, y= ys)) + \n  geom_point() + \n  stat_summary(fun.data = mean_cl_normal, col = \"red\") + \n  geom_segment(aes(x = 1.5,\n                   xend = 1.5,\n                   y = means[1],\n                   yend = means[1] + hh[2]),\n               lwd = 4, col = cc[2]) + \n  geom_segment(x = .9, xend = 3.1, \n               yend = hh[1], \n               y = hh[1], lwd = 4, col = cc[1]) + \n  geom_segment(aes(x = 1, xend = 3,\n                   y = mean(means[1:2]),\n                   yend =  mean(means[1:2])), lty = 2) + \n  geom_segment(aes(x = 2.5, xend = 2.5,\n                   y = mean(means[1:2]), \n                   yend = mean(means[1:2]) + hh[3]),\n               col = cc[3], lwd = 4)\n\nhelmert\n\nWarning: Computation failed in `stat_summary()`\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required.\n\n\n\n\n\n\n\n\n\nbuild a group of 3 this way:\n\ncontr.helmert.unscaled &lt;- function(n){\n   sweep(contr.helmert(n), MARGIN = 2, STATS = 2:n, FUN = `/`)\n}\n\ncmat &lt;- contr.helmert.unscaled(3)\n\n\ngrand_mean &lt;- 7\ndiff_parents &lt;- .5\nnonadd_hybrid &lt;- 3\n\ngeno_simulation &lt;- tibble(geno_name = c(\"H\", \"N\", \"HN\"),\n       geno_id = 1:3,\n       n = 15) |&gt;\n  uncount(n) |&gt; \n  mutate(b0 = 1, \n         b1 = cmat[geno_id, 1], \n         b2 = cmat[geno_id, 2],\n         avg = b0 * grand_mean + b1 * diff_parents + b2*nonadd_hybrid,\n         obs = rnorm(length(avg), mean = avg, sd = .3))\n  \ngeno_simulation |&gt; \n  ggplot(aes(x = geno_name, y = obs)) + \n  geom_point()"
  },
  {
    "objectID": "posts/2023-12-14-contrasts/index.html#default-contrasts",
    "href": "posts/2023-12-14-contrasts/index.html#default-contrasts",
    "title": "Contrasts",
    "section": "default contrasts",
    "text": "default contrasts\n\nmeans &lt;- c(3,4,9)\n\nneach &lt;-  25\nys &lt;- rnorm(n = neach*length(means), mean = rep(means, each = neach), sd = .3)\nxs &lt;- rep(letters[1:length(means)], each = neach)\n\nmod &lt;- lm(ys ~ xs)\n\ncoefs_trt &lt;- coef(mod)\n\ntreatment &lt;- tibble(xs, ys) |&gt; \n  ggplot(aes(x = xs, y= ys)) + \n  geom_point() + \n  stat_summary(fun.data = mean_cl_normal, col = \"red\") + \n  geom_segment(x = .9, xend = 3.1, \n               yend = coefs_trt[1], \n               y = coefs_trt[1], lwd = 4, col = cc[1]) + \n  geom_segment(x = 2.1,\n               xend = 2.1,\n               y = coefs_trt[1],\n               yend = coefs_trt[1] + coefs_trt[2],\n               lwd = 4, col = cc[2]) + \n  geom_segment(x = 3.1, \n               xend = 3.1,\n               y = coefs_trt[1], \n               yend = coefs_trt[1] + coefs_trt[3],\n               col = cc[3], lwd = 4)\n\ntreatment\n\nWarning: Computation failed in `stat_summary()`\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required."
  },
  {
    "objectID": "posts/2023-12-14-contrasts/index.html#polynomial-contrasts",
    "href": "posts/2023-12-14-contrasts/index.html#polynomial-contrasts",
    "title": "Contrasts",
    "section": "Polynomial contrasts",
    "text": "Polynomial contrasts\n\nmeans &lt;- c(3,4,9)\n\nneach &lt;-  25\nys &lt;- rnorm(n = neach*length(means), mean = rep(means, each = neach), sd = .3)\nxs &lt;- ordered(rep(letters[1:length(means)], each = neach))\n\nmod &lt;- lm(ys ~ xs)\n\ncontr_vals &lt;- contrasts(xs)\n\ncoefs_lin &lt;- coef(mod)\n\npolyfig &lt;- tibble(xs, ys) |&gt; \n  ggplot(aes(x = xs, y= ys)) + \n  geom_point() + \n  stat_summary(fun.data = mean_cl_normal, col = \"red\") + \n  geom_segment(x = 1, y = coefs_lin[1], xend = 3, yend = coefs_lin[1], lwd = 4, col = cc[1]) + \n  geom_line(aes(x = x, y = y), \n            data = data.frame(x = 1:3,\n                              y = contr_vals[,1][]*coefs_lin[2] + coefs_lin[1]),\n            lwd = 4, col = cc[2]) + \n  geom_line(aes(x = x, y = y), \n            data = data.frame(x = 1:3,\n                              y = contr_vals[,2][]*coefs_lin[3] + coefs_lin[1]),\n            lwd = 4, col = cc[3])\n\npolyfig\n\nWarning: Computation failed in `stat_summary()`\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required.\n\n\n\n\n\n\n\n\n\n\nsum contrasts\n\nmeans &lt;- c(3,4,9)\n\nneach &lt;-  25\nys &lt;- rnorm(n = neach*length(means), mean = rep(means, each = neach), sd = .3)\nxs &lt;- factor(rep(letters[1:length(means)], each = neach))\ncontrasts(xs) &lt;- contr.sum(3)\nmod &lt;- lm(ys ~ xs)\nsummary(mod)\n\n\nCall:\nlm(formula = ys ~ xs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54245 -0.18690  0.02279  0.18022  0.88346 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.32356    0.03108  171.31   &lt;2e-16 ***\nxs1         -2.34976    0.04395  -53.47   &lt;2e-16 ***\nxs2         -1.28808    0.04395  -29.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2691 on 72 degrees of freedom\nMultiple R-squared:  0.9899,    Adjusted R-squared:  0.9896 \nF-statistic:  3523 on 2 and 72 DF,  p-value: &lt; 2.2e-16\n\ncoefs_sum &lt;- coef(mod)\n\ncontrsum &lt;- tibble(xs, ys) |&gt; \n  ggplot(aes(x = xs, y= ys)) + \n  geom_point() + \n  stat_summary(fun.data = mean_cl_normal, col = \"red\") + \n  geom_segment(x = .9, xend = 3.1, y = coefs_sum[1], yend = coefs_sum[1],\n               lwd = 4, col = cc[1]) +  \n  geom_segment(x = 1.1,\n               xend = 1.1,\n               y = coefs_sum[1],\n               yend = coefs_sum[1] + coefs_sum[2],\n               lwd = 4, col = cc[2]) + \n  geom_segment(x = 2.1, \n               xend = 2.1,\n               y = coefs_sum[1], \n               yend = coefs_sum[1] + coefs_sum[3],\n               col = cc[3], lwd = 4)\ncontrsum\n\nWarning: Computation failed in `stat_summary()`\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required.\n\n\n\n\n\n\n\n\n\n\nlibrary(patchwork)\n\n\n(\n  (treatment + \n     labs(title = \"Treatment\") + \n     theme(axis.title = element_blank())\n   ) + contrsum +\n     labs(title = \"Sum\") + \n    theme(axis.title = element_blank())\n) / (\n  (helmert +\n     labs(title = \"Helmert\") + \n     theme(axis.title = element_blank())) +\n    (polyfig +\n     labs(title = \"Polynomial\") + \n     theme(axis.title = element_blank()))\n)\n\nWarning: Computation failed in `stat_summary()`\nComputation failed in `stat_summary()`\nComputation failed in `stat_summary()`\nComputation failed in `stat_summary()`\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required."
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html",
    "href": "posts/2023-11-15-ives03-ts/index.html",
    "title": "Discrete-time population growth in Stan",
    "section": "",
    "text": "library(cmdstanr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html#background",
    "href": "posts/2023-11-15-ives03-ts/index.html#background",
    "title": "Discrete-time population growth in Stan",
    "section": "Background",
    "text": "Background\nHow do species populations change over time?\nHow much of that change is caused by interactions with their community?\n20 years ago a very influential paper was written by Ives, Dennis, Cottingham and Carpenter (Ives et al. 2003). Ives et al. present a method called a first-order Multivariate AutoRegressive model, also known as MAR(1). The core idea is that species are growing in a density-dependent way, but at a rate that is influenced by the per-capita effects of every other interacting population. A transition matrix captures the effects of each species on every other, and lets us predict how the vector species abundances changes over time.\nIves et al. advance this argument by beginning with univariate (single-species) approach, and that is what this post is about. Perhaps a future post will cover all the models.\n\\[\n\\begin{align}\nn_t &= n_{t - 1}e^{a + (b - 1)ln(n_{t-1})} \\\\\nln(n_t) = x_t &= a + bx_{t - 1}\n\\end{align}\n\\] We can predict the curve at any time \\(t\\) with this expression, from finite series:\n\\[\nx_t = x_\\infty + b^t(x_0 - x_\\infty)\n\\]\n\ncurve(200 + (.9)^x * (4 - 200), xlim = c(0, 100))\n\n\n\n\nCurve of a Gompertz growth model.\n\n\n\n\nHowever there might be variation every time step because growth rate differences are caused by lots of unmeasured things.\n\\[\n\\begin{align}\nX_t &= a + bX_{t-1} + \\epsilon_t \\\\\n\\epsilon &\\sim \\text{Normal}(0, \\sigma)\n\\end{align}\n\\]\nThis leads to a mean and variance at time \\(t = \\infty\\)\n\\[\n\\begin{align}\n\\mu_\\infty &= \\frac{a}{1 - b} \\\\\nv_\\infty &= \\frac{\\sigma^2}{1 - b^2}\n\\end{align}\n\\]\nand the mean and variance at time \\(t\\)\n\\[\n\\begin{align}\n\\mu_t &= \\mu_\\infty + b^t(x_0 - \\mu_\\infty) \\\\\nv_t &= \\sigma^2\\frac{1 - (b^2)^t}{1 - b^2} = v_\\infty(1 - b^{2t})\n\\end{align}\n\\] Everything here is on the log scale. The result is something that we can work with in a model for the likelihood – the mean and variance of a normal distribution."
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html#simulations",
    "href": "posts/2023-11-15-ives03-ts/index.html#simulations",
    "title": "Discrete-time population growth in Stan",
    "section": "Simulations",
    "text": "Simulations\nHere are simulations from a one-species AR-1 model that imitate Ives et al. figure 1.\nsimulate_pop_growth &lt;- function(\n    a = 0, \n    b, \n    sigma = 1, \n    tmax = 50, \n    x0 = -8) {\n  \n  xvec &lt;- numeric(tmax)\n  \n  xvec[1] &lt;- x0\n  \n  ## process error\n  eta &lt;- rnorm(tmax, mean = 0, sd = sigma)\n  \n  for(time in 2:tmax){\n    xvec[time] &lt;- a + b*xvec[time-1] + eta[time]\n  }\n  \n  return(xvec)\n}\n\nmap_dfr(1:10, ~ tibble(pop = simulate_pop_growth(b = 0.6, tmax = 50),\n                       time = 1:length(pop)), .id = \"sim\") |&gt; \n  ggplot(aes(x = time, y = pop, group = sim)) + \n  geom_line() + \n  geom_hline(yintercept = 0) + \n  theme_bw()\nmap_dfr(1:10, ~ tibble(pop = simulate_pop_growth(b = 0.95, tmax = 50),\n                       time = 1:length(pop)), .id = \"sim\") |&gt; \n  ggplot(aes(x = time, y = pop, group = sim)) + \n  geom_line() + \n  geom_hline(yintercept = 0)+ \n  theme_bw()\n\n\n\n\n\n\nGrowth with a = 0, b = .6, sigma = 1\n\n\n\n\n\n\n\nGrowth with a = 0, b = .95, sigma = 1\n\n\n\n\n\nIt’s fun to take a look at this curve after exponentiating it, so as to see the real population sizes.\n\nmap_dfr(1:10, \n        ~ tibble(pop = simulate_pop_growth(\n          a = 3, b = 0.4, tmax = 10, sigma = 0.03),\n          time = 1:length(pop)), .id = \"sim\") |&gt; \n  ggplot(aes(x = time, y = exp(pop), group = sim)) + \n  geom_line() + \n  geom_hline(yintercept = 0, lty = 2)\n\n\n\n\n\n\n\n\n\nFunctions for the mean and variance\nWe can also get a plot of the changing mean and variance over time. Just from playing with these, we can see what the simulations earlier showed: that variance depends on both the process error \\(\\sigma\\) and on the parameter that controls the amount of density dependence, \\(b\\).\n\ncalc_mean &lt;- function(a, b, time, n_start){\n  mu_max &lt;- a / (1 - b)\n  \n  mu_max + b^time * (n_start - mu_max)\n}\n\ncalc_var &lt;- function(b, time, sigma){\n  bsq &lt;- b^2\n  \n  var_max = sigma^2/(1 - bsq)\n  \n  var_max * (1 - bsq^time)\n}\n\ncurve(calc_mean(0, .8, n_start = -8, time = x), xlim = c(0, 50))\n\n\n\n\n\n\n\ncurve(calc_var(.9, time = x, sigma = 1),\n      xlim = c(0,50), ylim = c(0, 10))\n\ncurve(calc_var(.8, time = x, sigma = 1), add = TRUE)\n\n\n\n\n\n\n\n\nDo these numbers reflect the distribution we see in the simulations?\nset.seed(5002)\nsome_sims &lt;- map_dfr(1:300, ~ tibble(pop = simulate_pop_growth(b = 0.9, tmax = 30),\n                       time = 0:(length(pop)-1)), .id = \"sim\")\n\nsim_meanvar &lt;- some_sims |&gt; \n  group_by(time) |&gt; \n  summarize(sim_mean = mean(pop),\n            sim_var = var(pop))\n\nsome_sims |&gt; \n  filter(time == 22) |&gt; \n  pluck(\"pop\") |&gt; \n  hist(probability = TRUE, breaks = 30, xlab = \"population size (log)\", main = \"Simulated and predicted population size distribution\")\n\ncurve(dnorm(x, \n            mean = calc_mean(a = 0, b = .9, time = 22, n_start = -8),\n            sd = sqrt(calc_var(b = .9, time = 22, sigma = 1))), \n      add = TRUE)\na_fig &lt;- 0\nb_fig &lt;- 0.8\nnstart_fig &lt;- -8\nsigma_fig &lt;- 1\n\ntibble(\n  time = 0:25,\n  mean = calc_mean(\n    a = a_fig, b = b_fig, time = time,\n    n_start = nstart_fig),\n  sd = sqrt(calc_var(\n    b = b_fig, time = time, sigma = sigma_fig))) |&gt; \n  ggplot(aes(x = time,\n             ymin = mean - sd*2,\n             ymax = mean + sd*2, \n             y = mean)) + \n  geom_ribbon(fill = \"lightblue\")+\n  geom_line(col = \"darkblue\", lwd = 2) + \n  geom_line(\n    aes(x = time,\n        y = pop, \n        group = sim),\n    inherit.aes = FALSE,\n    data = map_dfr(1:10, \n                   ~ tibble(\n                     pop = simulate_pop_growth(\n                       a = a_fig, \n                       b = b_fig,\n                       tmax = 25, \n                       sigma = sigma_fig),\n                     time = 0:(length(pop)-1)\n                     ),\n                   .id = \"sim\"\n                   )\n  ) + \n  NULL + \n  labs(x = \"Time\", y = \"log population size\")\n\n\n\n\n\n\nSimulations match the theoretical predictions very closely. Math works!\n\n\n\n\n\n\n\nSimulations match the theoretical predictions very closely. Math works!"
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html#aphids",
    "href": "posts/2023-11-15-ives03-ts/index.html#aphids",
    "title": "Discrete-time population growth in Stan",
    "section": "Aphids",
    "text": "Aphids\nBelow I explore this model and try to fit some examples in Stan. I’m building this example to match work being done at UdeS by students of Matt Barbour. In these experiments, a single aphid is placed on a radish plant. Aphids are clonal, and give birth to live (!) young.\nEach colony, incredibly, starts with a single aphid. This simplifies the expressions for the average and the variance because the starting population size is \\(ln(1) = 0\\)\n\\[\n\\begin{align}\n\\mu_t &= a\\frac{1 - b^t}{1 - b} &= \\mu_\\infty(1 - b^t) \\\\\nv_t &= \\sigma^2\\frac{1 - (b^2)^t}{1 - b^2} &= v_\\infty(1 - b^{2t})\n\\end{align}\n\\]\nThe entire experiment fits on a single tray in a growth chamber, and Katerie replicated the experiment 6 times."
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html#math-a-model-for-one-clone",
    "href": "posts/2023-11-15-ives03-ts/index.html#math-a-model-for-one-clone",
    "title": "Discrete-time population growth in Stan",
    "section": "Math: a model for one clone",
    "text": "Math: a model for one clone\nWhen we are making replicates observations of a single clone, we only need to know four quantites to predict both the average and the variation around that average: \\(a\\), \\(b\\), \\(\\sigma\\) and the time that passed since the start, \\(t\\)\nHere is the full Bayesian model with priors. I used prior simulations to come up with these numbers (that’s the next section below!)\n\\[\n\\begin{align}\nx_{i} &\\sim \\text{Normal}(\\mu_t, \\sqrt{v_t})\\\\\n\\mu_t &= \\mu_\\infty(1 - b^t) \\\\\nv_t &= v_\\infty(1 - b^{2t}) \\\\\n\\mu_\\infty &= \\frac{a}{1 - b} \\\\\nv_\\infty &= \\frac{\\sigma^2}{1 - b^2} \\\\\na &\\sim \\text{Normal}(2, .5) \\\\\nb &\\sim \\text{Beta}(5, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html#make-fake-data",
    "href": "posts/2023-11-15-ives03-ts/index.html#make-fake-data",
    "title": "Discrete-time population growth in Stan",
    "section": "Make fake data",
    "text": "Make fake data\nSimulating from the data-generating model\n\nsim_one_gompertz &lt;- function(K = 1000, \n         a = 1, \n         b = 0.86,\n         s = .3,\n         ntime = 15){\n  \n  N = c(0, \n        rnorm(ntime - 1, \n              mean = a * (1 - b^(1:(ntime-1))) / (1 - b),\n              sd = s * (1 - (b^2)^(1:(ntime-1))) / (1 - b^2)\n        )\n  )\n        \n        \n  return(tibble(N = N, time = 0:(ntime-1)))\n}\n\nmap_df(1:6, ~ list(sim_one_gompertz()), .id = \"sim_id\") |&gt; \n  ggplot(aes(x = time, y = N, group = sim_id)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n# load the model in stan\nar_1 &lt;- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1.stan\"))\n\nar_1\n\ndata{\n  int n;\n  vector[n] time;\n  vector[n] x;\n}\n// transformed data {\n//   vector[n] x = log(pop);\n// }\nparameters {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0,upper=1&gt; b;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  real mu_max = a / (1 - b);\n  real sigma_max = sigma /sqrt(1 - b^2);\n}\nmodel {\n  a ~ normal(2, .5);\n  b ~ beta(5,2);\n  sigma ~ exponential(5);\n  x ~ normal(\n    mu_max .* (1 - pow(b, time)),\n    sigma_max .* sqrt(1 - pow(b^2, time))\n    );\n}\ngenerated quantities {\n  vector[15] x_pred;\n  x_pred[1] = 0;\n  for (j in 1:14) {\n    x_pred[j+1] = normal_rng(\n      mu_max * (1 - pow(b, j)),\n      sigma_max * sqrt(1 - pow(b^2, j))\n      );\n  }\n}\n\n\n\ngomp_one_pop_df &lt;- map_df(1:6, ~ list(\n  sim_one_gompertz()), .id = \"sim_id\")\n  \n  \ngomp_nozero &lt;- gomp_one_pop_df  |&gt; \n    filter(time != 0)\n\ngomp_ar_1_sample &lt;- ar_1$sample(data = list(n = nrow(gomp_nozero),\n                        x = gomp_nozero$N,\n                        time = gomp_nozero$time),\n                        parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 0.9 seconds.\nChain 1 finished in 1.0 seconds.\nChain 3 finished in 0.9 seconds.\nChain 4 finished in 0.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.9 seconds.\nTotal execution time: 1.2 seconds.\n\n\n\ngomp_ar_1_sample |&gt; \n  spread_rvars(x_pred[time]) |&gt; \n  ggplot(aes(x = time-1, ydist = x_pred)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  theme_bw() + \n  geom_line(aes(x = time, y = N, group = sim_id),\n            inherit.aes = FALSE, data = gomp_one_pop_df) + \n  labs(x = \"Time\", y = \"log population size\")\n\n\n\n\n\n\n\n\n\ngomp_ar_1_sample$summary(variables = c(\"a\", \"b\", \"sigma\",\n                                       \"mu_max\", \"sigma_max\")) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\na\n0.8755229\n0.8715515\n0.0653034\n0.0640476\n0.7744877\n0.9901877\n1.002237\n1274.784\n1329.060\n\n\nb\n0.8919885\n0.8926680\n0.0188091\n0.0186207\n0.8595486\n0.9219138\n1.002168\n1276.568\n1300.837\n\n\nsigma\n0.5386121\n0.5334915\n0.0482727\n0.0461511\n0.4668362\n0.6243722\n1.004019\n1438.492\n1641.133\n\n\nmu_max\n8.2620125\n8.1184950\n0.9562658\n0.8650823\n6.9728300\n10.0094000\n1.002092\n1446.378\n1782.139\n\n\nsigma_max\n1.2005219\n1.1903850\n0.1052177\n0.1015655\n1.0467550\n1.3880110\n1.002135\n1844.450\n1556.341"
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html#say-it-again-but-different-parameterizing-based-on-mu-and-sigma",
    "href": "posts/2023-11-15-ives03-ts/index.html#say-it-again-but-different-parameterizing-based-on-mu-and-sigma",
    "title": "Discrete-time population growth in Stan",
    "section": "Say it again but different: parameterizing based on \\(\\mu\\) and \\(\\sigma\\)",
    "text": "Say it again but different: parameterizing based on \\(\\mu\\) and \\(\\sigma\\)\nIt might be easier to set priors directly on equilibrium population size (\\(\\mu_\\infty\\)) and variance at equilibrium (\\(v_\\infty\\)) so I experimented with parameterizing the model directly that way. It works just as well!\n\n# load the model in stan\nar1_mu_sigma &lt;- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_mu_sigma.stan\"))\n\nar1_mu_sigma\n\ndata{\n  int n;\n  vector[n] time;\n  vector[n] x;\n}\n// transformed data {\n//   vector[n] x = log(pop);\n// }\nparameters {\n  real mu_max;\n  real&lt;lower=0,upper=1&gt; b;\n  real&lt;lower=0&gt; sigma_max;\n}\ntransformed parameters {\n  // real mu_max = a / (1 - b);\n  // real sigma_max = sigma /sqrt(1 - b^2);\n}\nmodel {\n  mu_max ~ normal(7, .5);\n  b ~ beta(5,2);\n  sigma_max ~ exponential(1);\n  x ~ normal(\n    mu_max .* (1 - pow(b, time)),\n    sigma_max .* sqrt(1 - pow(b^2, time))\n    );\n}\ngenerated quantities {\n  vector[15] x_pred;\n  x_pred[1] = 0;\n  for (j in 1:14) {\n    x_pred[j+1] = normal_rng(\n      mu_max * (1 - pow(b, j)),\n      sigma_max * sqrt(1 - pow(b^2, j))\n      );\n  }\n}\n\n\n\ngomp_ar1_mu_sigma_sample &lt;- ar1_mu_sigma$sample(\n  data = list(n = nrow(gomp_nozero),\n              x = gomp_nozero$N,\n              time = gomp_nozero$time),\n  parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.3 seconds.\nChain 2 finished in 1.3 seconds.\nChain 3 finished in 1.2 seconds.\nChain 4 finished in 1.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.2 seconds.\nTotal execution time: 1.5 seconds.\n\n\n\ngomp_ar1_mu_sigma_sample |&gt; \n  spread_rvars(x_pred[time]) |&gt; \n  ggplot(aes(x = time-1, ydist = x_pred)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  theme_bw() + \n  geom_line(aes(x = time, y = N, group = sim_id),\n            inherit.aes = FALSE, data = gomp_one_pop_df)\n\n\n\n\n\n\n\n\n\ngomp_ar1_mu_sigma_sample$summary(variables = c(\"mu_max\", \"b\", \"sigma_max\")) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nmu_max\n7.4195465\n7.408785\n0.3897996\n0.3887970\n6.8015800\n8.0852720\n1.003136\n1324.390\n1827.973\n\n\nb\n0.8758691\n0.876516\n0.0121230\n0.0118971\n0.8543245\n0.8948875\n1.002905\n1356.657\n1740.710\n\n\nsigma_max\n1.1726875\n1.167125\n0.0945322\n0.0938486\n1.0264060\n1.3328110\n1.000760\n2029.433\n1859.360"
  },
  {
    "objectID": "posts/2023-11-15-ives03-ts/index.html#more-than-one-model-at-once",
    "href": "posts/2023-11-15-ives03-ts/index.html#more-than-one-model-at-once",
    "title": "Discrete-time population growth in Stan",
    "section": "More than one model at once",
    "text": "More than one model at once\nHere is a no-pooling approach to modelling different clones.\n\nar1_multispp &lt;- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_multispp.stan\"))\nar1_multispp\n\ndata{\n  int n;\n  int nclone;\n  vector[n] time;\n  vector[n] x;\n  array[n] int&lt;lower=1, upper=nclone&gt; clone_id;\n}\n// transformed data {\n//   vector[n] x = log(pop);\n// }\nparameters {\n  vector[nclone] log_a;\n  vector[nclone] logit_b;\n  vector[nclone] log_sigma;\n}\ntransformed parameters {\n  vector[nclone] b = inv_logit(logit_b);\n  vector[nclone] mu_max = exp(log_a) ./ (1 - b);\n  vector[nclone] sigma_max = exp(log_sigma) ./ sqrt(1 - b^2);\n}\nmodel {\n  log_a ~ normal(.7, .2);\n  logit_b ~ normal(1, .2);\n  log_sigma ~ normal(-1.5, .5);\n  x ~ normal(\n    mu_max[clone_id] .* (1 - pow(b[clone_id], time)),\n    sigma_max[clone_id] .* sqrt(1 - pow(b[clone_id]^2, time))\n    );\n}\ngenerated quantities {\n  matrix[15, nclone] x_pred;\n  x_pred[1,] = rep_row_vector(0, nclone);\n  for (s in 1:nclone){\n    for (j in 1:14) {\n      x_pred[j+1,s] = normal_rng(\n        mu_max[s] .* (1 - pow(b[s], j)),\n        sigma_max[s] .* sqrt(1 - pow(b[s]^2, j))\n        );\n    }\n  }\n}\n\n\nA few things are different in this model compared to the previous one:\n\nI’m passing in ID numbers for each clone, and an integer number for maximum number of clones\nthe model parameters are now vectors, not scalars\nI’ve also put the parameters on different scale. This will come in handy later when the model becomes hierarchical. Whenever I personally do this, I also change the names (see Note below).\nThe predictions are now a matrix. This is because we need two different pieces of information for each observation: what time it is, and which clone we’re talking about.\n\n\n\n\n\n\n\nCall me by your name\n\n\n\nThis code shows off a habit I’ve developed over the last few months of working on Bayesian models. When I put a parameter on a link scale, I change the parameter name to add the name of that link scale.\nFor example, you might have a strictly positive parameter and model it like this:\nparameters {\n  real&lt;lower=0&gt; alpha;\n}\nmodel {\n  alpha ~ normal(1, .5);\n  ....\n}\nBut then you might decide to model that on a log scale, either to reparameterize or to prepare for making it hierarchical later. Using link functions like the log and logit is standard in hiearchical models because it allows us to calculate a different value for every group, above and below the overall average, while respecting any constraints\nparameters {\n  real alpha_log;\n}\nmodel {\n  alpha_log ~ normal(0, .2);\n}\nI don’t usually try to make sure the prior is EXACTLY equivalent, but its usually pretty straightforward to get kind of close.\n\n\nTo validate this model, I’m going to generate 6 aphid clones by choosing parameters for a and b from different\n\nas &lt;- runif(n = 6, min = 1.5, max = 2.5)\nbs &lt;- runif(n = 6, min = .4, max = .9)\ngomp_many_df &lt;- expand_grid(clone_id = 1:6, rep_id = 1:10) |&gt; \n  mutate(a = as[clone_id],\n         b = bs[clone_id]) |&gt; \n  rowwise() |&gt; \n  mutate(x = list(sim_one_gompertz(a = a, b = b)))\n\ngomp_nozero_many_df &lt;- gomp_many_df  |&gt; \n  unnest(x) |&gt; \n    filter(time != 0)\n\ngomp_nozero_many_df |&gt; \n  ggplot(aes(x = time, y = N, group = rep_id)) + \n  geom_line() + \n  facet_wrap(~clone_id)\n\n\n\n\n\n\n\n\n\nar1_multispp_samp &lt;- ar1_multispp$sample(\n  data = list(n = nrow(gomp_nozero_many_df),\n              nclone = max(gomp_nozero_many_df$clone_id),\n              time = gomp_nozero_many_df$time,\n              x = gomp_nozero_many_df$N,\n              clone_id = gomp_nozero_many_df$clone_id), \n  parallel_chains = 4,\n  refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 23.8 seconds.\nChain 1 finished in 24.2 seconds.\nChain 3 finished in 24.1 seconds.\nChain 4 finished in 24.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 24.0 seconds.\nTotal execution time: 24.3 seconds.\n\nar1_multispp_samp\n\n   variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n lp__       79.64  79.98 3.07 3.01 74.05 84.00 1.00     1728     2506\n log_a[1]    0.86   0.86 0.02 0.02  0.83  0.90 1.00     4228     2969\n log_a[2]    0.58   0.58 0.03 0.03  0.52  0.63 1.00     4768     2881\n log_a[3]    0.79   0.79 0.03 0.03  0.75  0.83 1.00     4648     2895\n log_a[4]    0.56   0.56 0.03 0.03  0.50  0.61 1.00     4467     2727\n log_a[5]    0.52   0.51 0.03 0.03  0.47  0.57 1.00     4103     3076\n log_a[6]    0.85   0.85 0.03 0.02  0.81  0.90 1.00     4921     2812\n logit_b[1]  1.17   1.17 0.04 0.04  1.10  1.24 1.00     4148     2972\n logit_b[2]  0.59   0.59 0.06 0.06  0.48  0.69 1.00     4790     2715\n logit_b[3]  0.59   0.59 0.05 0.05  0.51  0.67 1.00     4720     2849\n\n # showing 10 of 127 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n\nar1_multispp_samp |&gt; \n  gather_rvars(x_pred[time,clone_id]) |&gt; \n  ggplot(aes(x = time-1, ydist = .value)) + stat_lineribbon() + \n  facet_wrap(~clone_id) + \n  scale_fill_brewer(palette = \"Greens\") + \n  ## add data\n  geom_line(aes(x = time, y = N, group = rep_id), \n            data = gomp_nozero_many_df,\n            inherit.aes = FALSE) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime equals 🫖 - 1\n\n\n\nOne of the trickiest parts of working with these models has been adding and subtracting \\(1\\). Here’s some notes for the next time I get confused:\n\ntake out the first value We can’t use data including time=0 in a stan model because that implies 0 variance, which just causes lots of errors with the normal likelihood. Taking this point out is no loss, because it doesn’t give us any real information – there’s no variance at all.\nusing position in the matrix as data for predictions I generate predictions for new clonal lines, in order to generate the posterior predictive distribution and plot the figures. I store these predictions in a matrix. However, the first observation (time = 0) is actually stored in the first position of the matrix (i.e., row 1). I just evaluate time as all the integers between 0 and 14, which gives two weeks of experiment in this case. You can see in the generated quantities block that I actually use the index value in the for-loop (I called it j) as the value of time. This requires a weird trick: the value of j gets used, but then it is placed in the j+1 position in the vector. This is because there is no “row 0” to go with the observation at time 0, so everything is pushed up by 1.\nsubtract 1 to get the correct time This means that to get the right time, we need to subtract 1 from the value used in generated quantities. When I use tidybayes to extract the predictive distribution, I use gather_rvars(x_pred[time,clone_id]) to specify that I want to call the first dimension of the matrix “time”, and the second “clone_id”. But then when I plot these data, I need to subtract 1 from the row position of a prediction to get the actual time value. This is because time = 0 is in row 1, time = 1 is in row 2, etc. Doing this gives a figure where model predictions match\n\n\n\n\nHierarchical model\n\nar1_multilevel &lt;- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_multilevel.stan\"))\nar1_multilevel\n\n// modified from previous models with help from ChatGPT!\ndata {\n  int n;\n  int nclone;\n  vector[n] time;\n  vector[n] x;\n  array[n] int&lt;lower=1, upper=nclone&gt; clone_id;\n}\ntransformed data{\n  vector[n] twotime = 2 * time;\n}\nparameters {\n  vector[nclone] mu_log_a;\n  vector[nclone] mu_logit_b;\n  vector[nclone] mu_log_sigma;\n\n  cholesky_factor_corr[3] L_corr;  // Cholesky factorization of the correlation matrix\n  vector&lt;lower=0&gt;[3] sigma_params;  // Standard deviations for log_a, logit_b, log_sigma\n\n  matrix[nclone, 3] z_params_raw;  // Unconstrained parameters\n}\n\ntransformed parameters {\n  matrix[nclone, 3] z_params = z_params_raw * diag_pre_multiply(sigma_params, L_corr);\n\n  vector[nclone] log_a = mu_log_a + z_params[,1];\n  vector[nclone] logit_b = mu_logit_b + z_params[,2];\n  vector[nclone] log_sigma = mu_log_sigma + z_params[,3];\n\n  vector[nclone] b = inv_logit(logit_b);\n  vector[nclone] mu_max = exp(log_a - log1m_inv_logit(b));\n  vector[nclone] sigma_max = exp(log_sigma) ./ sqrt(1 - square(b));\n}\n\nmodel {\n  mu_log_a ~ normal(0.7, 0.2);\n  mu_logit_b ~ normal(1.7, 0.2);\n  mu_log_sigma ~ normal(-.7, 0.25);\n\n  L_corr ~ lkj_corr_cholesky(4);  // Prior on the Cholesky factor for the correlation matrix\n\n  sigma_params[1] ~ exponential(4);\n  sigma_params[2] ~ exponential(4);\n  sigma_params[3] ~ exponential(3.5);\n\n  to_vector(z_params_raw) ~ std_normal();\n\n  x ~ normal(\n    mu_max[clone_id] .* (1 - pow(b[clone_id], time)),\n    sigma_max[clone_id] .* sqrt(1 - pow(b[clone_id], twotime))\n  );\n}\n\ngenerated quantities {\n  matrix[15, nclone] x_pred;\n  x_pred[1,] = rep_row_vector(0, nclone);\n  for (s in 1:nclone){\n    for (j in 1:14) {\n      x_pred[j+1,s] = normal_rng(\n        mu_max[s] .* (1 - pow(b[s], j)),\n        sigma_max[s] .* sqrt(1 - pow(b[s]^2, j))\n      );\n    }\n  }\n}\n\n\n\nar1_multilevel_samp &lt;- ar1_multilevel$sample(\n  data = list(n = nrow(gomp_nozero_many_df),\n              nclone = max(gomp_nozero_many_df$clone_id),\n              time = gomp_nozero_many_df$time,\n              x = gomp_nozero_many_df$N,\n              clone_id = gomp_nozero_many_df$clone_id), \n  parallel_chains = 4,\n  refresh = 500\n)\n\nar1_multilevel_samp\n\nWe can fit to previous simulations no problem, but we can also simulate data directly from the model. Because this has gotten kind of big, I’m going to make a Stan program just for simulating data\n\nar1_multilevel_prior &lt;- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_multilevel_prior.stan\"))\nar1_multilevel_prior\n\n// modified from previous models with help from ChatGPT!\ndata {\n  int nclone;\n  int nrep;\n}\n\nparameters {\n  vector[nclone] mu_log_a;\n  vector[nclone] mu_logit_b;\n  vector[nclone] mu_log_sigma;\n\n  cholesky_factor_corr[3] L_corr;  // Cholesky factorization of the correlation matrix\n  vector&lt;lower=0&gt;[3] sigma_params;  // Standard deviations for log_a, logit_b, log_sigma\n\n  matrix[nclone, 3] z_params_raw;  // Unconstrained parameters\n}\n\ntransformed parameters {\n  matrix[nclone, 3] z_params = z_params_raw * diag_pre_multiply(sigma_params, L_corr);\n\n  vector[nclone] log_a = mu_log_a + z_params[,1];\n  vector[nclone] logit_b = mu_logit_b + z_params[,2];\n  vector[nclone] log_sigma = mu_log_sigma + z_params[,3];\n\n  vector[nclone] b = inv_logit(logit_b);\n  vector[nclone] mu_max = exp(log_a - log1m_inv_logit(b));\n  vector[nclone] sigma_max = exp(log_sigma) ./ sqrt(1 - square(b));\n}\n\nmodel {\n  mu_log_a ~ normal(0.7, 0.2);\n  mu_logit_b ~ normal(1.7, 0.2);\n  mu_log_sigma ~ normal(-.7, 0.25);\n\n  L_corr ~ lkj_corr_cholesky(4);  // Prior on the Cholesky factor for the correlation matrix\n\n  sigma_params[1] ~ exponential(4);\n  sigma_params[2] ~ exponential(4);\n  sigma_params[3] ~ exponential(3.5);\n\n  to_vector(z_params_raw) ~ std_normal();\n}\n\ngenerated quantities {\n  array[nclone, nrep, 15] real x_pred;\n  for (s in 1:nclone){\n    for (r in 1:nrep){\n      x_pred[s, r, 1] = 0;\n    }\n  }\n  for (s in 1:nclone){\n    for (r in 1:nrep){\n      for (j in 1:14) {\n        x_pred[s, r, j+1] = normal_rng(\n          mu_max[s] .* (1 - pow(b[s], j)),\n          sigma_max[s] .* sqrt(1 - pow(b[s]^2, j))\n          );\n      }\n    }\n  }\n}\n\n\nThis model code lacks the likelihood, but uses the same configuration as the previous non-centered model. It also has a bit more going on the Generated Quantities block. Here, we are simulating multiple replicates using each clone, which simulates the actual experiment.\nWe can sample from this prior distribution and visualize it to see what it says. This is really the only way to get a good idea of what priors really mean in a nonlinear model like this one!\n\nmultilevel_prior &lt;- ar1_multilevel_prior$sample(\n  data = list(nclone = 12, nrep = 10), \n  chains = 1,\n  refresh = 0)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 1.5 seconds.\n\nprior_pred_multilevel &lt;- multilevel_prior |&gt; \n  gather_draws(x_pred[clone_id, rep, time], ndraws = 1) |&gt; \n  mutate(time = time -1)\n\nprior_pred_multilevel |&gt; \n  ggplot(aes(x = time, y = .value, group = rep)) + \n  geom_line() + \n  facet_wrap(~clone_id)\n\n\n\n\n\n\n\n\nFinally we can put this model through a very similar process of fitting and plotting as previous. Here I’m doing data prep, sampling and plotting all in one plot.\n\nprior_df &lt;- prior_pred_multilevel |&gt; \n  filter(time != 0)\n\nar1_multilevel_samp &lt;- ar1_multilevel$sample(\n  data = list(n = nrow(prior_df),\n              nclone = max(prior_df$clone_id),\n              time = prior_df$time,\n              x = prior_df$.value,\n              clone_id = prior_df$clone_id), \n  parallel_chains = 4,\n  refresh = 0\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 466.2 seconds.\nChain 3 finished in 472.9 seconds.\nChain 4 finished in 473.9 seconds.\nChain 1 finished in 475.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 472.2 seconds.\nTotal execution time: 475.6 seconds.\n\n# ar1_multilevel_samp\n\nar1_multilevel_samp |&gt; \n  gather_rvars(x_pred[time,clone_id]) |&gt; \n  ggplot(aes(x = time-1, ydist = .value)) + stat_lineribbon() + \n  facet_wrap(~clone_id) + \n  scale_fill_brewer(palette = \"Greens\") + \n  ## add data\n  geom_line(aes(x = time, y = .value, group = rep), \n            data = prior_pred_multilevel,\n            inherit.aes = FALSE, col = \"darkorange\", alpha = .3)  + \n  labs(y = \"Population size (log)\", \n       x = \"Time\")\n\n\n\n\n\n\n\n\nNote that this did not work perfectly, even though we are fitting to the prior predictive distribution. There are divergent iterations!\n\nar1_multilevel_samp$diagnostic_summary()\n\n$num_divergent\n[1] 2 2 6 1\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.8415368 1.0193399 0.8426104 0.9076898"
  },
  {
    "objectID": "posts/2022-12-05-growth-increment/index.html",
    "href": "posts/2022-12-05-growth-increment/index.html",
    "title": "Uncertainty in growth increments",
    "section": "",
    "text": "The following is a simulation by Will Vieira which explores how process uncertainty and measurement error combine when we measure trees.\nThe model imagines a simple linear growth scenario: every year, individuals grow by a random amount \\(g_i\\). This growth increment is random and varies each year according to a normal distribution\n\\[\n\\begin{align}\nL_{\\text{year}[i]} \\sim N \\\\\n\\end{align}\n\\]\n\\[\nL = lo\n\\]\nHere is Will’s very clean and concise simulation code.\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nset.seed(0.0)\n\nnbInd &lt;- 2000\ndeltaYear = 14\nobsError = 5\n\n# Generate individual trees with random size in mm\nsizeGrowth_dt &lt;- tibble(\n    tree_id = 1:nbInd,\n    size_real0 = rgamma(nbInd, 190^2/1e4, 190/1e4)\n    ) |&gt;\n    # each indv increment every year with a N(2.3, 2.2) (values from dataset);\n    # here from size_t to size_t+1 is the sum of X years growth\n    mutate(\n        size_real1 = size_real0 + replicate(n(), sum(rnorm(deltaYear, 2.3, 2.2))),\n        size_real2 = size_real1 + replicate(n(), sum(rnorm(deltaYear, 2.3, 2.2))),\n        size_real3 = size_real2 + replicate(n(), sum(rnorm(deltaYear, 2.3, 2.2))),\n        size_real4 = size_real3 + replicate(n(), sum(rnorm(deltaYear, 2.3, 2.2)))\n    ) |&gt;\n    pivot_longer(\n        cols = contains('size'),\n        names_to = 'timeStep',\n        values_to = 'size_real'\n    ) |&gt;\n    # each observation has an error of measurement\n    mutate(\n        size_random = rnorm(n(), mean = size_real, sd = obsError)\n    ) |&gt;\n    # calculate growth from real and random (observed) values\n    group_by(tree_id) |&gt;\n    mutate(\n        growth_real = (size_real - lag(size_real, 1))/deltaYear,\n        growth_random = (size_random - lag(size_random, 1))/deltaYear\n    ) |&gt;\n    ungroup() |&gt;\n    pivot_longer(\n        cols = contains(c('real', 'random')),\n        names_to = c('var', 'type'),\n        names_sep = '_'\n    ) |&gt;\n    # remove NA for first time step\n    filter(!is.na(value))\n\np1 &lt;- sizeGrowth_dt |&gt;\n  pivot_wider(\n    names_from = type,\n    values_from = value\n  ) |&gt;\n  ggplot(aes(x = real, y = random)) +\n  geom_point(size = .5) +\n  facet_wrap(~var, scales = 'free') +\n  geom_abline(intercept = 0, slope = 1)\n\np2 &lt;- sizeGrowth_dt |&gt;\n  ggplot(aes(value, color = type)) +\n  geom_density() +\n  facet_wrap(~var, scales = 'free')\n\nlibrary(patchwork)\np1  + p2\nsizeGrowth_dt |&gt; \n  filter(var == \"growth\") |&gt; \n  ggplot(aes(x = value, colour = type)) + \n  geom_density() + \n  stat_function(fun = function(x) dnorm(x, mean = 2.3, sd = 2.2),\n                colour = \"black\") + \n  stat_function(fun = function(x) dnorm(x, mean = 2.3, sd = sqrt(deltaYear)/2.2),\n                colour = \"black\", lty = 2) + \n  stat_function(fun = function(x) dnorm(x, mean = 2.3, \n                                        sd = sqrt(deltaYear)/sqrt(2.2^2 + obsError^2)),\n                colour = \"black\", lty = 3)\nsd_g &lt;- 2.2\n\nsizeGrowth_dt |&gt; \n  filter(var == \"growth\") |&gt; \n  ggplot(aes(x = value, colour = type)) + \n  geom_density(lwd = 2) + \n  stat_function(fun = function(x) dnorm(x, mean = 2.3, sd = 2.2),\n                colour = \"black\", lty = 3) + \n  stat_function(fun = function(x) dnorm(x, mean = 2.3, sd = 2.2/sqrt(deltaYear)),\n                colour = \"black\", lty = 2) + \n  stat_function(fun = function(x) dnorm(x, mean = 2.3, \n                                        sd = sqrt(\n                                          (sd_g^2 * deltaYear + 2*obsError^2)/deltaYear^2\n                                          )\n                                        ),\n                colour = \"black\", lty = 1) + \n  theme_bw()\nbut why are these correlated?"
  },
  {
    "objectID": "posts/2022-12-05-growth-increment/index.html#distribution-of-a-difference",
    "href": "posts/2022-12-05-growth-increment/index.html#distribution-of-a-difference",
    "title": "Uncertainty in growth increments",
    "section": "Distribution of a difference",
    "text": "Distribution of a difference\n\nu1 &lt;- 8\nu2 &lt;- 15\nsdx &lt;- 1.5\nn &lt;- 1e5\nx1 &lt;- rnorm(n, mean = u1, sd = sdx)\nx2 &lt;- rnorm(n, mean = u2, sd = sdx)\ntibble(diff = x2 - x1) |&gt; \n  ggplot(aes(x = diff)) + \n  stat_function(fun = \\(x) dnorm(x, mean = u2 - u1, sd = sqrt(2) * sdx),\n                size = 3, col = \"darkgreen\") + \n  geom_density(size = 1, col = \"orange\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThis is what happens when you have two constant values which are measured with error and then contrasted.\nif another process adds to the variation, then the two variances add – THEN get scaled but sqrt(2)"
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html",
    "href": "posts/2023-07-05-breakpoint/index.html",
    "title": "Breakpoint regression in Stan",
    "section": "",
    "text": "library(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)"
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html#two-or-more-relationships",
    "href": "posts/2023-07-05-breakpoint/index.html#two-or-more-relationships",
    "title": "Breakpoint regression in Stan",
    "section": "Two (or more) relationships",
    "text": "Two (or more) relationships\nIn breakpoint regression, we think that the relationship between two things can be described by two lines which alternate at a specific point.\nWhen might this happen? One case is when a rate is determined by the minimum value of two functions1. For an example, consider cars driving down a series of roads. The roads vary in their speed limit, and each car is driven by very lawful drivers who always drive precisely the speed limit\n\ntibble(x = 0:1, y = 0:1) |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_abline(slope = 1, intercept = 0) + \n  labs(x = \"speed limit\", y = \"Car speed\")\n\n\n\n\n\n\n\n\nHowever, some of these highways have VERY high speed limits. The cars, however, can’t keep up: eventually they hit their max speed. When that happens, the lawful drivers hold steady at the fastest speed their car can maintain. Together, you get a kind of “hockey stick” shape:\n\ntribble(~ limit, ~ max,\n        0, 0,\n        200, 200) |&gt; \n  ggplot(aes(x = limit, y = max)) + \n  geom_point(col = \"white\") + \n  geom_abline(slope = 0, intercept = 100, lty = 2, lwd = 2, col = \"grey\") + \n  geom_label(aes(x = 10, y = 105, label = \"Max speed\")) + \n  geom_abline(slope = 1, intercept = 0, lty = 2, lwd = 2) + \n  theme_classic() + \n  labs(x = \"Speed limit\", y = \"Car speed\")\n\n\n\n\nCar speed equals the speed limit, right up to the car’s maximum velocity. Then, it doesn’t matter how high the limit is, the car has to stay at maximum speed.\n\n\n\n\n\nNot so fast!\nNot every car is going to go at the exact manufacturers maximum speed! When (in my imaginary example) manufacturers release a new kind of car, they test a sample of cars to measure their maximum speed. Therefore we know the max speed (with some error) for that brand of car. However, the realized max speed for any specific car in the wild will be lower than this value. This could be caused by a host of unmeasured factors, such as that particular car’s defects, the kind of care it has received, etc\n\ntribble(~ limit, ~ max,\n        0, 0,\n        200, 200) |&gt; \n  ggplot(aes(x = limit, y = max)) + \n  geom_point(col = \"white\") + \n  geom_abline(slope = 0, intercept = 100, lty = 2, lwd = 2, col = \"grey\") + \n  geom_label(aes(x = 25, y = 105, label = \"Factory Max speed\")) + \n  geom_abline(slope = 0, intercept = 80, lty = 2, lwd = 1, col = \"orange\") + \n  geom_label(aes(x = 25, y = 85, label = \"Actual max speed\")) + \n  geom_abline(slope = 1, intercept = 0, lty = 2, lwd = 2) + \n  theme_classic() + \n  labs(x = \"Speed limit\", y = \"Car speed\")\n\n\n\n\nCar speed equals the speed limit, right up to the car’s maximum velocity – decreased a little bit by particularities of this specific car."
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html#write-that-in-mathematics",
    "href": "posts/2023-07-05-breakpoint/index.html#write-that-in-mathematics",
    "title": "Breakpoint regression in Stan",
    "section": "Write that in mathematics",
    "text": "Write that in mathematics\nfor one car, driving on different roads.\n\nWe label the different roads \\(i\\)\n\\(\\tau\\) is the maximum speed of this particular kind of car\n\\(M\\) is a bunch of measurements of the car’s max speed (based on factory cars)\n\\(p\\) is the proportion of that maximum that our actual car has\n\n\\[\n\\begin{align}\nY_i &\\sim \\text{Normal}(\\mu_i, \\sigma)\\\\\nM &\\sim \\text{Normal}(\\tau, \\sigma_m)\\\\\n\\mu_i & = \\begin{cases}\n    X_i & \\text{if } X_i \\leq p\\tau \\\\\n    p\\tau & \\text{if } X_i &gt; p\\tau\n\\end{cases} \\\\\n\\text{logit}(p) &\\sim \\text{Normal}(2, .5) \\\\\n\\tau &\\sim \\text{Normal}(...) \\\\\n\\sigma_m &\\sim \\text{Exponential}(...) \\\\\n\\sigma &\\sim \\text{Exponential}(...)\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html#simple-stan-code-with-prior-predictions",
    "href": "posts/2023-07-05-breakpoint/index.html#simple-stan-code-with-prior-predictions",
    "title": "Breakpoint regression in Stan",
    "section": "Simple Stan code with prior predictions",
    "text": "Simple Stan code with prior predictions\n\nbreakpoint_B &lt;- cmdstan_model(here::here(\"posts/2023-07-05-breakpoint/breakpoint_B.stan\"))\nbreakpoint_B\n\ndata {\n  int n;\n  vector[n] x;\n  real B;\n}\n// transformed data {\n//   real M = max(x);\n// }\nparameters {\n real b2;\n real&lt;lower=0&gt; sigma;\n}\nmodel {\n  b2 ~ normal(1, 1);\n  sigma ~ exponential(1);\n}\ngenerated quantities {\n  vector[n] y;\n  for( i in 1:n){\n    if (x[i] &lt; B) {\n      y[i] = normal_rng(x[i]*b2, sigma);\n    } else {\n      y[i] = normal_rng(B*b2, sigma);\n    }\n  }\n}\n\n\n\nxvar &lt;- runif(42, min = 1, max=55)\n\nbreakpoint_B_prior &lt;- breakpoint_B$sample(chains=1,\n                                      data = list(x = xvar,\n                                                  n = 42,\n                                                  B = 25))\n\nRunning MCMC with 1 chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\n\nbreakpoint_B_prior\n\n variable  mean median    sd   mad     q5   q95 rhat ess_bulk ess_tail\n    lp__  -2.00  -1.69  0.99  0.71  -3.84 -1.05 1.00      423      391\n    b2     0.98   0.97  0.91  0.93  -0.56  2.57 1.00      589      609\n    sigma  0.99   0.64  1.02  0.66   0.05  3.15 1.00      609      346\n    y[1]  24.52  23.87 22.87 23.49 -14.30 64.14 1.00      592      572\n    y[2]  24.58  24.27 22.87 23.19 -13.87 63.73 1.00      585      609\n    y[3]   3.59   3.42  3.56  3.53  -2.18  9.59 1.00      573      617\n    y[4]  24.55  24.04 22.91 23.51 -14.53 63.83 1.00      599      581\n    y[5]  24.70  24.28 22.82 22.93 -14.38 64.15 1.00      598      501\n    y[6]  15.98  15.66 14.87 14.93  -9.47 42.02 1.00      595      607\n    y[7]  24.55  24.02 22.78 23.27 -13.91 63.94 1.00      586      572\n\n # showing 10 of 45 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nplot it\n\nprior_draws &lt;- breakpoint_B_prior |&gt; \n  tidybayes::gather_draws(y[i], ndraws = 12) |&gt; \n  mutate(x = xvar[i])\n\nprior_draws |&gt; \n  ggplot(aes(x = x, y = .value)) + geom_point() + \n  facet_wrap(~.draw)"
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html#rewriting-this-with-step",
    "href": "posts/2023-07-05-breakpoint/index.html#rewriting-this-with-step",
    "title": "Breakpoint regression in Stan",
    "section": "Rewriting this with step()",
    "text": "Rewriting this with step()\nI learned from PhD student Dominique Caron that breakpoint regression can be written in brms using step(). This is described in this forum discussion and demonstrated in Stan code here. The Stan manual discusses step() among other logical functions\n\nbreakpoint_step_prior &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-07-05-breakpoint/breakpoint_step_prior.stan\"))\nbreakpoint_step_prior\n\ndata {\n  int n;\n  vector[n] x;\n  real B;\n}\nparameters {\n vector[n] y;\n real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  vector[n] x2;\n  for (i in 1:n){\n    x2[i] = step(B - x[i]);\n  }\n}\nmodel {\n  vector[n] mu;\n  mu = B + (x - B) .* x2;\n  y ~ normal(mu, sigma);\n  sigma ~ exponential(1);\n}\n\n\n\nset.seed(4812)\n\nxvar &lt;- runif(42, min = 1, max=55)\n\nbreakpoint_step_prior_samples &lt;- breakpoint_step_prior$sample(chains=1,\n                                      data = list(x = xvar,\n                                                  n = 42, B = 33), refresh = 0L)\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.6 seconds.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\nprior_draws &lt;- breakpoint_step_prior_samples |&gt; \n  tidybayes::gather_draws(y[i], ndraws = 12) |&gt; \n  mutate(x = xvar[i])\n\nprior_draws |&gt; \n  ggplot(aes(x = x, y = .value)) + geom_point() + \n  facet_wrap(~.draw)\n\n\n\n\n\n\n\n\nestimating breakpoints with some prior knowledge\nWe assume here that we have some unmeasured maximum. in my car example, the maximum speed of each kind of car is measured at the factory, by taking a random sample of cars and measuring their max speeds\n\n## take just one \none_dataset &lt;- prior_draws |&gt; \n  arrange(.draw) |&gt; \n  head(42)\n\nbreakpoint_step_meas &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-07-05-breakpoint/breakpoint_step_meas.stan\"))\n\nbreakpoint_step_meas\n\ndata {\n  int n;\n  vector[n] speed_limit;\n  vector[n] obs_speed;\n  real max_avg;\n  real max_sd;\n}\nparameters {\n  real max_speed;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  vector[n] V2;\n  for (i in 1:n){\n    V2[i] = step(max_speed - speed_limit[i]);\n  }\n}\nmodel {\n  vector[n] mu;\n  mu = max_speed + (speed_limit - max_speed) .* V2;\n  max_speed ~ normal(max_avg, max_sd);\n  obs_speed ~ normal(mu, sigma);\n  sigma ~ exponential(1);\n}\n\n\n\nbreakpoint_step_meas$sample(data = list(n  = 42, speed_limit = one_dataset$x, \n                                        obs_speed = one_dataset$.value,\n                                        max_avg = 30,\n                                        max_sd = 5), refresh = 0L)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\n\nWarning: NAs introduced by coercion\n\nWarning: NAs introduced by coercion\n\n\n  variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n lp__      -18.53 -18.23 0.99 0.71 -20.57 -17.57 1.00     1745     1892\n max_speed  33.17  33.17 0.23 0.23  32.79  33.54 1.00     2427     2370\n sigma       0.93   0.91 0.11 0.10   0.77   1.11 1.00     3100     2272\n V2[1]       1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n V2[2]       1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n V2[3]       1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n V2[4]       0.00   0.00 0.00 0.00   0.00   0.00   NA       NA       NA\n V2[5]       1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n V2[6]       0.00   0.00 0.00 0.00   0.00   0.00   NA       NA       NA\n V2[7]       1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n\n # showing 10 of 45 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)"
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html#objects-on-the-road-are-slower-than-they-appear",
    "href": "posts/2023-07-05-breakpoint/index.html#objects-on-the-road-are-slower-than-they-appear",
    "title": "Breakpoint regression in Stan",
    "section": "Objects on the road are slower than they appear",
    "text": "Objects on the road are slower than they appear\nthe factory-measured speed is probably a lot higher than the top speed of the average car on the road. lots of factors intervene: the particular car’s defects, its maitenance history, the fuel used, the driver’s skill.\nAll of these factors will only DECREASE, and never increase, a car’s speed relative to the maximum possible.\n\nbreakpoint_step_p &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-07-05-breakpoint/breakpoint_step_p.stan\"))\nbreakpoint_step_p\n\ndata {\n  int n;\n  vector[n] speed_limit;\n  vector[n] obs_speed;\n  real max_avg;\n  real max_sd;\n}\nparameters {\n  real factory_speed;\n  real&lt;lower=0&gt; sigma;\n  real alpha;\n}\ntransformed parameters {\n  real max_speed;\n  max_speed = factory_speed * inv_logit(alpha);\n  vector[n] V2;\n  for (i in 1:n){\n    // step() is 0 if speed limit is over the maximum, 0 otherwise\n    // in other words, it gives the slope\n    V2[i] = step(max_speed - speed_limit[i]);\n  }\n}\nmodel {\n  vector[n] mu;\n  mu = max_speed + (speed_limit - max_speed) .* V2;\n  factory_speed ~ normal(max_avg, max_sd);\n  obs_speed ~ normal(mu, sigma);\n  alpha ~ normal(3, 2);\n  sigma ~ exponential(1);\n}\n\n\n\nbreakpoint_step_p$sample(data = list(n  = 42, \n                                     speed_limit = one_dataset$x, \n                                     obs_speed = one_dataset$.value,\n                                     max_avg = 30,\n                                     max_sd = 5), refresh = 0L)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.1 seconds.\n\n\nWarning: 80 of 4000 (2.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nWarning: NAs introduced by coercion\n\nWarning: NAs introduced by coercion\n\n\n      variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n lp__          -19.12 -18.82 1.21 0.95 -21.47 -17.84 1.01      985     1660\n factory_speed  33.84  33.68 0.66 0.57  33.03  35.09 1.01      232      109\n sigma           0.93   0.92 0.11 0.10   0.77   1.11 1.01      539      997\n alpha           4.45   4.24 1.25 1.28   2.82   6.74 1.01      237      135\n max_speed      33.16  33.17 0.24 0.23  32.76  33.55 1.00     2243      727\n V2[1]           1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n V2[2]           1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n V2[3]           1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n V2[4]           0.00   0.00 0.00 0.00   0.00   0.00   NA       NA       NA\n V2[5]           1.00   1.00 0.00 0.00   1.00   1.00   NA       NA       NA\n\n # showing 10 of 47 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nfirst thing that doesn’t work well (admittedly, there is nothing in these simulated data to support this)\nsuggestion that the probability prior is very important\nperhaps if the same kind of car travelled on different highways in several different countries? in all countries the car would have the same theoretical maximum, but the difference in country-level factors mean that the proportion \\(p\\) below that max, where the average car has its top speed, is going to vary.\nwhat other uncertainty is there to include? uncertainty in the speed limit perhaps?"
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html#switch-for-gamma-errors",
    "href": "posts/2023-07-05-breakpoint/index.html#switch-for-gamma-errors",
    "title": "Breakpoint regression in Stan",
    "section": "switch for gamma errors",
    "text": "switch for gamma errors\nBefore advancing model building much farther, I want to correct the error distribution. Speeds might be variable, but they can never be negative!\n\nbreakpoint_step_gamma_prior &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-07-05-breakpoint/breakpoint_step_gamma_prior.stan\"))\nbreakpoint_step_gamma_prior\n\ndata {\n  int n;\n  vector[n] x;\n  real B;\n}\nparameters {\n vector&lt;lower=0&gt;[n] y;\n real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  vector[n] x2;\n  for (i in 1:n){\n    x2[i] = step(B - x[i]);\n  }\n}\nmodel {\n  vector[n] mu;\n  mu = B + (x - B) .* x2;\n  vector[n] beta = mu/square(sigma);\n  y ~ gamma(mu .* beta, beta);\n  sigma ~ exponential(1);\n}\n\n\n\nset.seed(4812)\n\nxvar &lt;- runif(42, min = 1, max = 55)\n\nbreakpoint_step_gamma_prior_samples &lt;- breakpoint_step_gamma_prior$sample(\n  chains=1,\n  data = list(x = xvar,\n              n = 42, B = 33), refresh = 1000L)\n\nRunning MCMC with 1 chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Random variable[1] is inf, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Random variable[1] is inf, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Random variable[4] is inf, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Shape parameter[1] is 0, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Shape parameter[1] is 0, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Shape parameter[1] is 0, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Shape parameter[1] is 0, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Shape parameter[1] is inf, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Shape parameter[1] is inf, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: gamma_lpdf: Shape parameter[1] is inf, but must be positive finite! (in '/var/folders/x7/l08zn2396g797m5ws54np_6w0000gp/T/Rtmp5fMbyc/model-5cb742e1f51c.stan', line 20, column 2 to column 30)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.5 seconds.\n\n\nWarning: 1 of 1 chains had an E-BFMI less than 0.2.\nSee https://mc-stan.org/misc/warnings for details.\n\nprior_draws &lt;- breakpoint_step_gamma_prior_samples |&gt; \n  tidybayes::gather_draws(y[i], ndraws = 12) |&gt; \n  mutate(x = xvar[i])\n\nprior_draws |&gt; \n  ggplot(aes(x = x, y = .value)) + \n  geom_point() + \n  facet_wrap(~.draw)"
  },
  {
    "objectID": "posts/2023-07-05-breakpoint/index.html#footnotes",
    "href": "posts/2023-07-05-breakpoint/index.html#footnotes",
    "title": "Breakpoint regression in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee Leibig’s Law of the Minimum, and also Dune↩︎"
  },
  {
    "objectID": "posts/2023-11-14-multilevel-arima/index.html",
    "href": "posts/2023-11-14-multilevel-arima/index.html",
    "title": "Multilevel Arima model",
    "section": "",
    "text": "library(cmdstanr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nWe can vectorize an AR-1 Model for one species very easily. This is also the right way to work with missing data.\nBut how can we do it with multiple species? This is pretty key when working with long-format datasets, often with different species. I want it to be flexible – after all, not all species have datasets of equal length - some are only monitored in certain years."
  },
  {
    "objectID": "posts/2023-11-14-multilevel-arima/index.html#math-notation",
    "href": "posts/2023-11-14-multilevel-arima/index.html#math-notation",
    "title": "Multilevel Arima model",
    "section": "Math notation",
    "text": "Math notation"
  },
  {
    "objectID": "posts/2023-11-14-multilevel-arima/index.html#a-stan-model",
    "href": "posts/2023-11-14-multilevel-arima/index.html#a-stan-model",
    "title": "Multilevel Arima model",
    "section": "A Stan model",
    "text": "A Stan model\n\nlibrary(cmdstanr)\nsingle_spp_ar1 &lt;- cmdstan_model(\n  here::here(\"posts/2023-11-14-multilevel-arima/single_spp_ar1.stan\"),\n  pedantic = TRUE)\nsingle_spp_ar1\n\ndata {\n  int n;\n  vector[n] pop;\n  int&lt;lower=0,upper=1&gt; fit;\n  // array[n] int&lt;lower=1&gt; y_id;\n  // for predictions\n  int nyear;\n}\ntransformed data {\n  vector[n] log_pop = log(pop);\n}\nparameters {\n  real log_b0;\n  real log_rho;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  log_b0 ~ normal(0, 0.1);\n  log_rho ~ normal(0, 0.1);\n  sigma ~ exponential(1);\n  // likelihood\n  if (fit == 1){\n    log_pop[2:n] ~ normal(\n      exp(log_b0) + exp(log_rho) * log_pop[1:(n-1)],\n      sigma);\n  }\n}\ngenerated quantities {\n  vector[nyear] pred_pop_avg;\n  array[nyear] real pred_pop_obs;\n\n  pred_pop_avg[1] = 2.2;\n\n  for (j in 2:nyear) {\n    pred_pop_avg[j] = exp(log_b0) + exp(log_rho) * pred_pop_avg[j-1];\n\n  }\n\n  pred_pop_obs = normal_rng(pred_pop_avg, sigma);\n\n}\n\n\n\nsample_single_spp_ar1 &lt;- single_spp_ar1$sample(data = list(\n  nyear = 25, \n  n = 25,\n  pop = rep(0, times = 25), \n  fit = 0))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.8 seconds.\n\n\n\nsome_draws &lt;- sample_single_spp_ar1 |&gt; \n  tidybayes::spread_draws(pred_pop_avg[i], pred_pop_obs[i], ndraws = 12)\n\nsome_draws |&gt; \n  ggplot(aes(x = i, y = pred_pop_avg)) + \n  geom_line() + \n  geom_point(aes(y = pred_pop_obs))+ \n  facet_wrap(~.draw, ncol = 4) + \n  coord_cartesian(ylim = c(0, 100))\n\n\n\n\n\n\n\n\nThis is the prior predictive distribution of an AR-1 model, for a single species model. Each panel in the discussion refers to a single posterior sample for all parameters"
  },
  {
    "objectID": "posts/2023-11-14-multilevel-arima/index.html#multi-species-vectorization",
    "href": "posts/2023-11-14-multilevel-arima/index.html#multi-species-vectorization",
    "title": "Multilevel Arima model",
    "section": "multi-species vectorization",
    "text": "multi-species vectorization\n\nlibrary(cmdstanr)\nmultiple_spp_ar1 &lt;- cmdstan_model(\n  here::here(\n    \"posts/2023-11-14-multilevel-arima/multiple_spp_ar1.stan\"),\n                                pedantic = TRUE)\nmultiple_spp_ar1\n\ndata {\n  int n;\n  int S;\n  vector[n] pop;\n  array[n] int&lt;lower=1, upper=S&gt; Sp;\n  int&lt;lower=0, upper=1&gt; fit;\n  // for predictions\n  int nyear;\n}\ntransformed data {\n  vector[n] log_pop = log(pop);\n  array[n - S] int time;\n  array[n - S] int time_m1;\n  for (i in 2:n) {\n    if (Sp[i] == Sp[i-1]) {\n      time[i - Sp[i]] = i;\n      time_m1[i - Sp[i]] = i - 1;\n    }\n  }\n}\nparameters {\n  vector[S] log_b0;\n  vector[S] log_rho;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  log_b0 ~ normal(0, 0.1);\n  log_rho ~ normal(0, 0.1);\n  sigma ~ cauchy(0, 2);\n\n  if (fit == 1) {\n    log_pop[time] ~ normal(\n      exp(log_b0[Sp[time]])\n      + exp(log_rho[Sp[time]]) .* log_pop[time_m1],\n      sigma);\n  }\n}\ngenerated quantities {\n  array[S] vector[nyear] pred_pop_avg;\n  array[S,nyear] real pred_pop_obs;\n\n  for (s in 1:S){\n    pred_pop_avg[s][1] = 2.2;\n  }\n\n  for (s in 1:S){\n    for (j in 2:nyear){\n      pred_pop_avg[s][j] = exp(log_b0[s])\n       + exp(log_rho[s]) .* pred_pop_avg[s][j-1];\n    }\n  }\n\n  for (s in 1:S){\n    pred_pop_obs[s,] = normal_rng(pred_pop_avg[s], sigma);\n  }\n}\n\n\n\nsample_multiple_spp_ar1 &lt;- multiple_spp_ar1$sample(\n  data = list(\n    n = 5*7,\n    S = 5,\n    pop = rep(0, times = 5*7), \n    Sp = rep(1:5, each = 7),\n    fit = 0,\n    nyear = 7\n  )\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n\n\n\nsome_draws &lt;- sample_multiple_spp_ar1 |&gt; \n  tidybayes::spread_draws(pred_pop_avg[sp, year], pred_pop_obs[sp, year], ndraws = 8)\n\nsome_draws |&gt; \n  ggplot(aes(x = year, y = pred_pop_avg, group = sp)) + \n  geom_line() + \n  geom_point(aes(y = pred_pop_obs))+ \n  facet_wrap(~.draw, ncol = 4) + \n  coord_cartesian(ylim = c(0, 50))"
  },
  {
    "objectID": "posts/2022-12-01-cholesky-correlation/index.html",
    "href": "posts/2022-12-01-cholesky-correlation/index.html",
    "title": "The Cholesky decomposition",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nYou can give uncorrelated random numbers a specific correlation by using the Cholesky decomposition. This comes in handy when you’re modelling correlated random variables using MCMC.\neg &lt;- rethinking::rlkjcorr(1, 2, 1)\neg\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.5587252\n[2,] -0.5587252  1.0000000\n\ncc &lt;- chol(eg)\n\npurrr::rerun(30,{\n  zz &lt;- matrix(data = rnorm(500), ncol = 2)\n  # plot(zz)\n  rr &lt;- t(cc) %*% t(zz)\n  # plot(t(rr))\n  cor(t(rr))[1,2]\n}) |&gt; \n  purrr::flatten_dbl() |&gt; density() |&gt; plot(main = \"Simulated correlations\")\n\nWarning: `rerun()` was deprecated in purrr 1.0.0.\nℹ Please use `map()` instead.\n  # Previously\nrerun(30, {\nzz &lt;- matrix(data = rnorm(500), ncol = 2)\nrr &lt;- t(cc) %*% t(zz)\ncor(t(rr))[1, 2]\n})\n\n  # Now\nmap(1:30, ~ {\nzz &lt;- matrix(data = rnorm(500), ncol = 2)\nrr &lt;- t(cc) %*% t(zz)\ncor(t(rr))[1, 2]\n})\n\nabline(v=eg[1,2])\nHere we can see that the correlation we want is -0.5587252, and indeed we’re able to give exactly that correlation to uncorrelated random numbers.\n[,1]       [,2]\n[1,] 1.00000000 0.08358276\n[2,] 0.08358276 1.00000000\n\n\n           [,1]       [,2]\n[1,] 1.00000000 0.08358276\n[2,] 0.08358276 1.00000000\n\n\n\n\n\n\n\nuncorrelated numbers\n\n\n\n\n\n\n\nafter being given a correlation"
  },
  {
    "objectID": "posts/2022-12-01-cholesky-correlation/index.html#doing-it-by-hand",
    "href": "posts/2022-12-01-cholesky-correlation/index.html#doing-it-by-hand",
    "title": "The Cholesky decomposition",
    "section": "Doing it by hand",
    "text": "Doing it by hand\nBecause the case for only two random variables is pretty simple, we can actually write out the Cholesky decomposition by hand. Here I want to give some independent random numbers a correlation of -0.8:\n\np &lt;- -.8\nL &lt;- matrix(c(1,0, p, sqrt(1 - p^2)), ncol = 2, byrow = TRUE)\nzz &lt;- matrix(data = rnorm(1000), ncol = 2)\nyy &lt;- t(L %*% t(zz))\nplot(yy)\n\n\n\n\n\n\n\ncor(yy)\n\n           [,1]       [,2]\n[1,]  1.0000000 -0.8005567\n[2,] -0.8005567  1.0000000\n\n\nThis can be written without matrix multiplication like this:\n\nz1 &lt;- rnorm(1000)\nz2 &lt;- rnorm(1000)\n\ny1 &lt;- z1\ny2 &lt;- p*z1 + sqrt(1 - p^2)*z2\n\nplot(y1, y2)\n\n\n\n\n\n\n\ncor(y2, y1)\n\n[1] -0.8105853\n\n\nIn a Stan model, I might want to use a scaled beta distribution to model the correlation between, say, slopes and intercepts in a model:\n\nmu &lt;- .84\nphi &lt;- 80\ng &lt;- rbeta(1, mu*phi, (1 - mu)*phi)\np_trans &lt;- g*2 - 1\n\nsim_y_corr &lt;- function(n, p) {\n  z1 &lt;- rnorm(n)\n  z2 &lt;- rnorm(n)\n  \n  y1 &lt;- z1\n  y2 &lt;- p*z1 + sqrt(1 - p^2)*z2\n  \n  return(data.frame(y1, y2))\n}\n\nsim_y_corr(n = 1000, p = p_trans) |&gt; plot()\n\n\n\n\n\n\n\n\nor even work on the logit scale:\n\nq &lt;- rnorm(1, mean = -1, sd = .1)\np_trans_n &lt;- plogis(q)*2 - 1\nsim_y_corr(n = 1000, p = p_trans_n) |&gt; plot()\n\n\n\n\n\n\n\n\nThis is even a little easier to think about: because of all the transformations, 0 on the logit scale still means a 0 correlation; negative and positive numbers mean negative and positive correlations as well.\nAnother way to make correlation matrices is here: https://www.rdatagen.net/post/2023-02-14-flexible-correlation-generation-an-update-to-gencorgen-in-simstudy/"
  },
  {
    "objectID": "posts/2022-12-15-setting-priors/index.html",
    "href": "posts/2022-12-15-setting-priors/index.html",
    "title": "Setting priors on hierarchical multivariate models",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2022-12-15-setting-priors/index.html#what-is-this-post",
    "href": "posts/2022-12-15-setting-priors/index.html#what-is-this-post",
    "title": "Setting priors on hierarchical multivariate models",
    "section": "What is this post",
    "text": "What is this post\n\ndemo_data &lt;- structure(list(csize = c(5, 5, 5), \n                            age_morpho_indic = c(1, 1, \n1), mass = c(20.64, 25.58, 20.2), log_mass = c(3.02723094061336, \n3.2418107961507, 3.00568260440716), dponte = c(134, 135, 136), \n    annee = c(2005, 2006, 2005), ferme = c(9, 9, 9), idF1 = c(188197003, \n    188197004, 188197004), general_csize = c(8.972, 9.344, 8.972\n    ), coldsnap_csize = c(3.9, 7.15, 3.9), general_dponte = c(8.704651163, \n    10.48604651, 8.704651163), coldsnap_dponte = c(2.3, 4.7, \n    2.3), density_TRSW = c(8, 9, 8), density_HOSP = c(0, 0, 0\n    ), paysa_ext = c(0.224174146, 0.223943415, 0.224166189), \n    general_mean_csize = c(-0.482520728753587, -0.343044978530785, \n    -0.343044978530785), difference_general_csize = c(0, 0.186, \n    -0.186), coldsnap_mean_csize = c(-1.09248466640163, -0.156587649057671, \n    -0.156587649057671), difference_coldsnap_csize = c(0, 1.625, \n    -1.625), general_mean_dponte = c(-2.54912170305933, -1.57828050221117, \n    -1.57828050221117), difference_general_dponte = c(0, 0.890698, \n    -0.890698), coldsnap_mean_dponte = c(-1.63910623622377, -0.951828417972457, \n    -0.951828417972457), difference_coldsnap_dponte = c(0, 1.2, \n    -1.2), density_TRSW_mean = c(0.248353096875404, 0.484967912059555, \n    0.484967912059555), difference_density_TRSW = c(0, 0.5, -0.5\n    ), paysa_ext_mean = c(-0.306307397416911, -0.308050462721618, \n    -0.308050462721618), difference_paysa_ext = c(-0.000913607068423686, \n    -0.0152243604543345, 0.0133971463174872), density_HOSP_mean = c(-0.525930668145508, \n    -0.525930668145508, -0.525930668145508), difference_density_HOSP = c(0, \n    0, 0), noisenvol = c(5, 5, 0)), row.names = c(NA, -3L), class = c(\"tbl_df\", \n\"tbl\", \"data.frame\"))\n\n\nlibrary(brms)\ndponte_model_bf_2 = bf(dponte ~ 1 + age_morpho_indic + general_mean_dponte + difference_general_dponte + (1|annee) + (1|ferme) + \n                         (1 + difference_general_dponte|f|idF1),\n                       family = gaussian(), center = FALSE)\n\n \n\ncsize_model_bf_2 = bf(csize ~ 1 + age_morpho_indic + general_mean_csize + difference_general_csize + (1|annee) + (1|ferme) + \n                        (1 + difference_general_csize|f|idF1),\n                      family = poisson(), center = FALSE)\n\n \n\nsucess_model_bf_2 = bf(noisenvol ~ 1 + (1|f|idF1) + (1|annee) + (1|ferme),\n                       family = poisson(), center = FALSE)\n\n# combine all three into one model\nfull_model_bf &lt;- dponte_model_bf_2 + csize_model_bf_2 + sucess_model_bf_2\n\nget_prior(full_model_bf, data = demo_data)\n\nfull_model_prior &lt;- c(\n  ## individual level correlations\n  prior(lkj(3), class = \"cor\", group = \"idF1\"),\n  ## clutch size model\n  prior(normal(0,1),    class = \"b\", resp = \"csize\"),\n  # prior(normal(0,1),    class = \"Intercept\", resp = \"csize\"),\n  prior(exponential(1), lb = 0, class = \"sd\", resp = \"csize\"),\n  ## laying date model\n  prior(normal(0,1),    class = \"b\",         resp = \"dponte\"),\n  # prior(normal(0,1),    class = \"Intercept\", resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sd\",   resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sigma\", resp = \"dponte\"),\n  # fitness -- no slopes here\n  # prior(normal(0,1),    class = \"Intercept\", resp = \"noisenvol\"),\n  prior(exponential(1), lb = 0, class = \"sd\",        resp = \"noisenvol\")\n)\n\n# Run fuul model\nfull_model_prior_predict = brm(full_model_bf,\n                    data = demo_data,\n                    prior = full_model_prior,\n                    cores = 4, chains = 4, \n                    sample_prior = \"only\")\n\n \n\nsummary(full_model_prior_predict)\n\n\nlibrary(tidybayes)\n\n# noisenvol\nprior_predictions &lt;- demo_data |&gt; \n  add_predicted_rvars(object = full_model_prior_predict, resp = \"noisenvol\")\n\nprior_predictions |&gt; glimpse()\n\nlibrary()\nprior_predictions |&gt; \n  select(idF1, noisenvol, .prediction) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(y = idF1, xdist = .prediction)) + \n  stat_halfeye() + \n  coord_cartesian(xlim =c(0,1e5))\n\nalready we can see that this is probably way too wide!\n\nfull_model_smaller_noisenvol_prior &lt;- c(\n  ## individual level correlations\n  prior(lkj(3), class = \"cor\", group = \"idF1\"),\n  ## clutch size model\n  prior(normal(0,1),    class = \"b\", resp = \"csize\"),\n  prior(exponential(1), lb = 0, class = \"sd\", resp = \"csize\"),\n  ## laying date model\n  prior(normal(138,5),    class = \"b\",         resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sd\",   resp = \"dponte\"),\n  prior(exponential(1), lb = 0, class = \"sigma\", resp = \"dponte\"),\n  # fitness -- no slopes here\n  prior(normal(1, 0.1),    class = \"b\", resp = \"noisenvol\"),\n  prior(exponential(4), lb = 0, class = \"sd\",        resp = \"noisenvol\")\n)\n\n\n\n# Run fuul model\nfull_model_noisenvol_prior_predict = brm(full_model_bf,\n                    data = demo_data,\n                    prior = full_model_smaller_noisenvol_prior,\n                    cores = 4, chains = 4, \n                    sample_prior = \"only\")\n\n \n\ndemo_data |&gt; \n  add_predicted_rvars(object = full_model_noisenvol_prior_predict, resp = \"noisenvol\") |&gt; \n  select(idF1, noisenvol, .prediction) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(y = idF1, xdist = .prediction)) + \n  stat_halfeye() + \n  coord_cartesian(xlim =c(0,12))\n\n\n# age_morpho_indic + general_mean_dponte + difference_general_dponte\none_female &lt;- tibble(difference_general_dponte = c(-3,0, 3),\n                     age_morpho_indic = c(0,1,1))\n\nfake_data &lt;- expand_grid(one_female,\n                         nesting(general_mean_dponte = c(-2,0,2),\n                                 idF1 = letters[1:3])) |&gt; \n  arrange(general_mean_dponte) |&gt; \n  mutate(ferme = \"f\",\n         annee = \"y\")\n\ndponte_prior_pred &lt;- fake_data |&gt; \n  add_predicted_draws(object = full_model_noisenvol_prior_predict, \n                      resp = \"dponte\", allow_new_levels = TRUE, ndraws = 3)\n\n\ndponte_prior_pred |&gt; \n  ggplot(aes(x= difference_general_dponte, y = .prediction, group = .draw)) + \n  geom_line() + \n  facet_wrap(~general_mean_dponte)"
  },
  {
    "objectID": "posts/30-03-2023-growth-continuous-discrete/index.html",
    "href": "posts/30-03-2023-growth-continuous-discrete/index.html",
    "title": "Should we model growth as continuous or discrete",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/30-03-2023-growth-continuous-discrete/index.html#how-do-things-grow-or-decay",
    "href": "posts/30-03-2023-growth-continuous-discrete/index.html#how-do-things-grow-or-decay",
    "title": "Should we model growth as continuous or discrete",
    "section": "How do things grow (or: decay)",
    "text": "How do things grow (or: decay)\nLet’s start with a simple simulation:\n\nstart_size &lt;- 45\ndecay_rate &lt;- .3\ntibble(time = 0:20, \n       mass = start_size * exp(-decay_rate * time)) |&gt; \n  ggplot(aes(x = time, y = mass)) + \n  geom_point()"
  },
  {
    "objectID": "posts/2023-08-22-transformed-regression/index.html",
    "href": "posts/2023-08-22-transformed-regression/index.html",
    "title": "Transformed regression in Stan",
    "section": "",
    "text": "library(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)"
  },
  {
    "objectID": "posts/2023-08-22-transformed-regression/index.html#sometimes-things-slow-down",
    "href": "posts/2023-08-22-transformed-regression/index.html#sometimes-things-slow-down",
    "title": "Transformed regression in Stan",
    "section": "Sometimes things slow down",
    "text": "Sometimes things slow down\n\nThere are no straight lines in ecology\n\nA lot of relationships in ecology are curved lines, often because there’s a constraint on the thing we are modelling as a response variable. For example, animals might get used to a specific stimulus over repeat exposures, with the result that eventually the response time drops to 0. However, response time can never be less than 0, and so the relationship is constrained from below. Relationships can be bounded above as well. Consider how the mass of an individual organism scales with increasing resources: more food means a bigger body, but eventually that relationship will flatten out.\nIn ecology, its very common to model these nonlinear relationships by applying a transformation to the response variable and modelling the resulting relationship with a gaussian linear regression. Sometimes this works out fine. Increasingly, ecologists are trying to go beyond this with models that reflect the data-generating process more explicitly. For example, an ecologist might model growth with one of the many growth equations. However, even if we develop a nonlinear model with a non-gaussian likelihood and all the features, our colleagues will still want a comparison to the now-traditional transformed lines.\nI wanted to write some simple Stan models to fit these transformed-response models. I was curious about the specific shape of the relationship we are fitting when we use these models. I have a suspicion that these curves are often fit to data, not because the scientist wants to make this curving shape, but because it improves residual plots. Whatever the reason for their use, I find that making pictures of something really helps me to understand it. My colleagues use these models, so this post is a beginning exploration of them."
  },
  {
    "objectID": "posts/2023-08-22-transformed-regression/index.html#two-pictures-of-two-curves",
    "href": "posts/2023-08-22-transformed-regression/index.html#two-pictures-of-two-curves",
    "title": "Transformed regression in Stan",
    "section": "Two pictures of two curves",
    "text": "Two pictures of two curves\nI love the base R function curve(); it might be my favourite base function! Let’s look at the curve that results from these two kinds of transformations.\nWhen I say “results from”, I mean what happens if you reverse the transformation on the response variable. In the case of a log-transformation model:\n\\[\n\\begin{align}\n\\text{log}(y) &= a + bx \\\\\ny &= e^{a + bx}\n\\end{align}\n\\]\nAnd in the case of a square root model:\n\\[\n\\begin{align}\n\\sqrt{y} &= a + bx \\\\\ny &= (a + bx)^2\n\\end{align}\n\\]\nIn both cases, the effect is like having a link function on the response variable. To get the curve, you just reverse the transformation on both.\n\ncurve(exp(7 - 2*x), xlim = c(0, 4))\n\n\n\n\nLog-transformed responses are like fitting an exponential curve\n\n\n\n\n\ncurve((7-2*x)^2, xlim = c(0, 8))\n\n\n\n\nSquare root transformed models are like fitting a quadratic curve\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI find this a confusing way to think of the square root curve; I prefer:\n\nsteep &lt;- 2\nminmax &lt;- 4\nelev &lt;- 1\ncurve(steep * (x - minmax)^2 + elev, xlim  = c(0, 8))\n\n\n\n\n\n\n\n\nThis is far easier to set priors on – useful to remember for those (rare?) times when this is a relationship you actually want to work with."
  },
  {
    "objectID": "posts/2023-08-22-transformed-regression/index.html#writing-the-models-in-stan",
    "href": "posts/2023-08-22-transformed-regression/index.html#writing-the-models-in-stan",
    "title": "Transformed regression in Stan",
    "section": "Writing the models in Stan",
    "text": "Writing the models in Stan\nLet’s write out the model we’re fitting, using g() to refer to either the log or the square root:\n\\[\n\\begin{align}\ng(y) &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= a + bx \\\\\na &\\sim \\text{Normal}(...) \\\\\nb &\\sim \\text{Normal}(...) \\\\\n\\sigma &\\sim \\text{Exponential}(...) \\\\\n\\end{align}\n\\]\nYou can see from here that we’re putting the likelihood ( the top line, \\(\\text{Normal}(\\mu, \\sigma)\\) ) through the transformation function also. This means that the “errors” or variation around the curve also get transformed. In the case of the log transformation, we are putting that normal distribution through the exponential function – in other words, we’re using a Lognormal distribution. With the square root transformation, it seems like perhaps we’re modelling something like a generalized chi square.\n\nSquare root transformation\nI want to begin with a simple prior predictive simulation from the square-root model.\n\nsqrt_prior &lt;- cmdstan_model(\n  here::here(\"posts/2023-08-22-transformed-regression/sqrt_prior.stan\")\n  )\nsqrt_prior\n\ndata{\n  int n;\n  vector[n] x;\n}\nparameters{\n  real slope;\n  real intercept;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  slope ~ std_normal();\n  intercept ~ std_normal();\n  sigma ~ exponential(1);\n}\ngenerated quantities {\n  vector[n] yrep;\n  for (i in 1:n){\n    yrep[i] = square(normal_rng(slope * x[i] + intercept, sigma));\n  }\n}\n\n\n\nxseq &lt;- seq(from = 0, \n    to = 20, \n    length.out = 15)\n\nsqrt_prior_reps &lt;- sqrt_prior$sample(\n  data = list(x = xseq,\n              n = 15),\n  refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.8 seconds.\n\nsqrt_prior_plot &lt;- sqrt_prior_reps$draws() |&gt; \n  tidybayes::gather_draws(yrep[i], ndraws = 12) |&gt; \n  mutate(x = xseq[i])\n  \nsqrt_prior_plot |&gt; \n  ggplot(aes(x= x, y = .value)) + geom_point() + \n  facet_wrap(~.draw)\n\n\n\n\nsome prior predictive simulations from a square-root transformed model.\n\n\n\n\nTo me, this illustrates the difficulty of setting useful priors on a square-root transformed model."
  },
  {
    "objectID": "posts/2023-08-22-transformed-regression/index.html#log-transformation",
    "href": "posts/2023-08-22-transformed-regression/index.html#log-transformation",
    "title": "Transformed regression in Stan",
    "section": "Log transformation",
    "text": "Log transformation\nlet’s try with log-transformed curves.\n\nlog_trans &lt;- cmdstan_model(\n  here::here(\"posts/2023-08-22-transformed-regression/log_trans.stan\"))\nlog_trans\n\ndata{\n  int n;\n  vector[n] x;\n  vector[n] y;\n}\ntransformed data{\n  vector[n] ylog = log(y);\n}\nparameters{\n  real slope;\n  real intercept;\n  real sigma;\n}\nmodel{\n  ylog ~ normal(intercept + slope*x, sigma);\n}\ngenerated quantities{\n  vector[n] ybar = exp(intercept + slope*x);\n  vector[n] yrep;\n  for(i in 1:n){\n    yrep[i] = exp(normal_rng(intercept + slope*x[i], sigma));\n  }\n}\n\n\n\n# make a bit of fake data\nxx &lt;- 0:14\nyb &lt;- 5.7  - 0.8*xx\nyy &lt;- exp(rnorm(length(xx), mean = yb, sd = 1))\nplot(xx,yy)\n\n\n\n\n\n\n\nlog_trans_post &lt;- log_trans$sample(\n  data = list(n = length(xx), x = xx, y = yy),\n  refresh  = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\n\n\nlog_trans_post$draws() |&gt; tidybayes::gather_rvars(ybar[i]) |&gt; \n  mutate(xx = xx[i]) |&gt; \n  ggplot(aes(x = xx, ydist = .value)) + stat_lineribbon()\n\n\n\n\n\n\n\n\n\nlog_trans_post$draws() |&gt; tidybayes::gather_rvars(yrep[i]) |&gt; \n  mutate(xx = xx[i]) |&gt; \n  ggplot(aes(x = xx, ydist = .value)) + stat_lineribbon()"
  },
  {
    "objectID": "posts/2023-08-22-transformed-regression/index.html#square-root-transformed-model",
    "href": "posts/2023-08-22-transformed-regression/index.html#square-root-transformed-model",
    "title": "Transformed regression in Stan",
    "section": "Square root transformed model",
    "text": "Square root transformed model\nGoing back to a square root model, fitting it to the same data that I just used for the log-transformed model. I’m doing it this way because the toy dataset I made and used above is much easier to reason about. I also want to see how a square-root model fits data that comes from a different data-generating process.\n\nsqrt_trans &lt;- cmdstan_model(here::here(\"posts/2023-08-22-transformed-regression/sqrt_trans.stan\"))\nsqrt_trans\n\ndata{\n  int n;\n  vector[n] x;\n  vector[n] y;\n}\ntransformed data{\n  vector[n] ytrans = sqrt(y);\n}\nparameters{\n  real slope;\n  real intercept;\n  real sigma;\n}\nmodel{\n  ytrans ~ normal(intercept + slope*x, sigma);\n}\ngenerated quantities{\n  vector[n] ybar = square(intercept + slope*x);\n  vector[n] yrep;\n  for(i in 1:n){\n    yrep[i] = square(normal_rng(intercept + slope*x[i], sigma));\n  }\n}\n\n\n\nsqrt_trans_post &lt;- sqrt_trans$sample(\n  data = list(n = length(xx), x = xx, y = yy), \n  refresh=0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.9 seconds.\n\nsqrt_trans_post$draws() |&gt; tidybayes::gather_rvars(ybar[i]) |&gt; \n  mutate(xx = xx[i]) |&gt; \n  ggplot(aes(x = xx, ydist = .value)) + stat_lineribbon()\n\n\n\n\nsquare-root transformed model fit to data from a log-transformed model. Just to prove you can\n\n\n\n\n\nsqrt_trans_post$summary() |&gt; \n  slice(2:4)\n\n# A tibble: 3 × 10\n  variable    mean median    sd   mad    q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 slope     -0.924 -0.923 0.235 0.228 -1.31 -0.547  1.00    1617.    1727.\n2 intercept 10.1   10.1   1.93  1.85   6.98 13.3    1.00    1629.    1426.\n3 sigma      3.82   3.71  0.827 0.763  2.69  5.39   1.00    1409.    1677.\n\n\nUnintuitive as these parameters are, you at least can get your downwards and decelerating curve from this model!"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html",
    "href": "posts/2022-11-29-group-centering/index.html",
    "title": "Andrew tries Quarto",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#what-is-this-post",
    "href": "posts/2022-11-29-group-centering/index.html#what-is-this-post",
    "title": "Andrew tries Quarto",
    "section": "What is this post",
    "text": "What is this post\nThis post is meant to be a simple template for new posts – when I make a new post I’ll start by copying and pasting this current one over. Probably in the future there will be a plugin that does this for us, but until then this will be fine."
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#why-i-switched-to-quarto",
    "href": "posts/2022-11-29-group-centering/index.html#why-i-switched-to-quarto",
    "title": "Andrew tries Quarto",
    "section": "Why I switched to Quarto",
    "text": "Why I switched to Quarto\nA simple and frequent answer: overenthusiasm! I like seeing all the new things that the Rstudio team develop, and I know that this vibrant community will keep adding features and tutorials"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#quarto-resources",
    "href": "posts/2022-11-29-group-centering/index.html#quarto-resources",
    "title": "Andrew tries Quarto",
    "section": "quarto resources",
    "text": "quarto resources\n\nthe Ultimate Guide to starting a Quarto blog\nquarto discussions\nDanielle Navarro’s comments on the topic\nNick Tierney’s notes\nand of course Nicks exciting book project!"
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#surprise-its-targets",
    "href": "posts/2022-11-29-group-centering/index.html#surprise-its-targets",
    "title": "Andrew tries Quarto",
    "section": "surprise it’s targets",
    "text": "surprise it’s targets\n\n\n\nvia GIPHY\n\nI’m also using targets. Here are some observations on that so far:\n\nLast error: ! System command 'quarto' failed I get this error message more that 10x more often than any other. As I’m learning Quarto I keep making errors which break the Quarto process but targets doesn’t (yet?) communicate the specific error message. To find out what has gone wrong, I go over to the terminal and run quarto render .\nyou have to go into _metadata.yml and stop the posts from freezing, by setting freeze: false . Targets will handle all the rest of it.\n\nI’m using targets with quarto because I want to work with large bayesian models fit with stan and brms., and fitting them in a regular blog post – including compiling, sampling etc."
  },
  {
    "objectID": "posts/2022-11-29-group-centering/index.html#my-own-observations-and-questions",
    "href": "posts/2022-11-29-group-centering/index.html#my-own-observations-and-questions",
    "title": "Andrew tries Quarto",
    "section": "My own observations and questions",
    "text": "My own observations and questions\nso far it is very straightforward!\n\n~it seems like post folder cant begin with dates?~ you can start a folder with a date, and I do, so that the posts sort in at least approximately the right temporal sequence. However the date for a post comes from the YAML, not the folder name.\nlove LOVE the bibliography automatically appearing\nDO think about changing the working directory for chunk evaluations to the project root, as described here. This is really important because that is the directory that targets uses when sourcing files. For example tar_load(my_target_name) will load in an object from your pipeline. But run this same line from an .qmd file in a subfolder (say posts/yourpostname/index.qmd), then THAT will be the location the computer looks in.."
  },
  {
    "objectID": "posts/2022-11-24-updating-R-and-packages/index.html",
    "href": "posts/2022-11-24-updating-R-and-packages/index.html",
    "title": "Updating R and your R packages",
    "section": "",
    "text": "I teach a few R workshops a year, and I often send out the typical “make sure R and Rstudio is up-to-date, using these helpful links. And then of course the workshop starts, and all the class notices that I myself have not updated R in .. 7 months.\nWhen I do decide to do it, I find I often need to google the correct procedure. In the spirit of this blog, here is a short note detailing the process, inspired by this stackoverflow note"
  },
  {
    "objectID": "posts/2022-11-24-updating-R-and-packages/index.html#doctor-update-thyself",
    "href": "posts/2022-11-24-updating-R-and-packages/index.html#doctor-update-thyself",
    "title": "Updating R and your R packages",
    "section": "",
    "text": "I teach a few R workshops a year, and I often send out the typical “make sure R and Rstudio is up-to-date, using these helpful links. And then of course the workshop starts, and all the class notices that I myself have not updated R in .. 7 months.\nWhen I do decide to do it, I find I often need to google the correct procedure. In the spirit of this blog, here is a short note detailing the process, inspired by this stackoverflow note"
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)\nlibrary(stantargets)"
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#the-equation",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#the-equation",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "The equation",
    "text": "The equation\nPredators eat prey. They eat prey faster when there is more prey – though they do have to slow down to catch, kill, and chew.1\nIf a predator can eat multiple things, then they might end up eating less of any one prey because they spread their kills around among all their prey. In the very simplest case, they do this in proportion to how frequent the different prey are – the predator has no preference, it just goes around randomly and eats what it finds\nThe classic OG version of this model comes from Holling (1966)\n\\[\nA = \\frac{\\alpha N}{1 + h\\alpha N}\n\\tag{1}\\]\nwhere\n\nN is the number of prey\n\\(\\alpha\\) is the attack rate\n\\(h\\) is the handling time\n\nAnd altogether you get the number of attacks \\(A\\) on prey in some unit of time."
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#multiple-species",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#multiple-species",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "multiple species",
    "text": "multiple species\nSuppose you have more than one species in this system. You could then rewrite Equation 1 to allow multiple animals to be included in the predation:\n\\[\nA_i = \\frac{\\alpha N_i}{1 + h\\alpha \\sum_{j = 1}^s N_j}\n\\tag{2}\\]\nhere \\(\\sum_{j = 1}^s N_j\\) means the sum over the abundances of all the prey. The subscript \\(i\\) just means that we are talking about one particular prey, which we label \\(i\\). This prey is included in the summation in the denominator.\nIt’s common to consider that different prey species might be attacked or handled at different rates (Smith and Smith 2020) (Smout et al. 2010)"
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#one-species-model",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#one-species-model",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "One species model",
    "text": "One species model\nLet’s begin with the classic model and add complexity later.\nI think it helps to think of predation as a binomial trial: out of so many prey individuals (\\(N\\)), some of them get attacked (\\(A\\)).\n\\[\n\\begin{align}\nA &\\sim \\text{Binomial}(p, N) \\\\\np &= \\frac{\\alpha}{1 + h\\alpha N} \\\\\n\\alpha &\\sim \\text{Beta}(2, 4) \\\\\nh &\\sim \\text{LogNormal}(0,1)\n\\end{align}\n\\] some things to note:\n\nthe \\(N\\) now appears in the Binomial distribution as a separate parameter, not in the expression for the probability of attack. Remember that the mean of a Binomial is \\(pN\\), so in this case we will come back to Equation 1\nboth the parameters have constraints: \\(a\\) cannot be outside of \\([0,1]\\), and \\(h\\) cannot be negative. We choose priors that respect these constraints!\n\nLet’s translate this into Stan and take a look:\n\nsimple_type2 &lt;- cmdstan_model(here::here(\"posts\", \n                                         \"2022-11-11-multispecies-functional-response\",\n                                         \"simple_type2.stan\"))\n\nsimple_type2\n\n// simple predator-prey functional response for a binomial density\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0&gt; attacks;\n  array[N] int&lt;lower=0&gt; densities;\n}\nparameters {\n  real&lt;lower=0,upper=1&gt; a;\n  real&lt;lower=0&gt; h;\n}\ntransformed parameters{\n  vector&lt;lower=0, upper = 1&gt;[N] prob_attack;\n  prob_attack = a * inv(1 + a * h * to_vector(densities));\n}\nmodel {\n  a ~ beta(2,6);\n  h ~ lognormal(0, 1);\n  attacks ~ binomial(densities, prob_attack);\n}\n\n\nthe code above is mostly a direct translation of the equations. One technique is worth noting: the types of the input vectors. Binomial distributions deal in integers, and so we define densities and attacks as integers. However, in order to vectorize our calculations, we massage the input data from an array of integers to a vector of real numbers using to_vector. This highlights an important difference between R and Stan. Stan requires careful definition of the types of data, where R is much more informal."
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#simulate-from-a-model",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#simulate-from-a-model",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "simulate from a model:",
    "text": "simulate from a model:\n\ngenerate_one_spp_type_2 &lt;- function(){\n  true_a &lt;- stats::rbeta(n = 1, 8, 4)\n  true_h &lt;- stats::rlnorm(n = 1, -2, .5)\n  densities &lt;- seq(from = 5, to = 100, by =5)\n  prob &lt;- true_a/(1 + true_a * true_h * densities)\n  attacks &lt;- rbinom(n = length(densities), size = densities, prob = prob)\n  list(true_a = true_a,\n       true_h = true_h,\n       densities = densities,\n       attacks = attacks, \n       prob = prob)\n}\n\none_spp_sim &lt;- generate_one_spp_type_2()\n\none_spp_sim\n\n$true_a\n[1] 0.7568304\n\n$true_h\n[1] 0.1605559\n\n$densities\n [1]   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95\n[20] 100\n\n$attacks\n [1] 2 3 4 5 7 6 3 6 3 7 6 4 5 2 8 6 4 5 5 6\n\n$prob\n [1] 0.47079213 0.34166316 0.26812246 0.22063275 0.18743446 0.16292010\n [7] 0.14407649 0.12913995 0.11700947 0.10696221 0.09850396 0.09128539\n[13] 0.08505256 0.07961647 0.07483352 0.07059268 0.06680672 0.06340618\n[19] 0.06033505 0.05754769\n\nwith(one_spp_sim, rbinom(n = length(densities), size = densities, prob = prob))\n\n [1] 4 5 3 3 8 3 5 5 4 8 7 7 5 3 7 2 5 7 3 6\n\nwith(one_spp_sim, plot(densities, prob*densities))\n\n\n\n\n\n\n\nwith(one_spp_sim, plot(densities, attacks))\n\n\n\n\n\n\n\n\ntry it in targets:\n\n\ntar_option_set(packages = c(\"cmdstanr\",\n               \"ggplot2\", \"tidybayes\", \n               \"stantargets\"))\n\ngenerate_one_spp_type_too &lt;- function(){\n  true_a &lt;- stats::rbeta(n = 1, 8, 4)\n  true_h &lt;- stats::rlnorm(n = 1, -2, .5)\n  densities &lt;- seq(from = 5, to = 100, by =5)\n  prob &lt;- true_a/(1 + true_a * true_h * densities)\n  attacks &lt;- rbinom(n = length(densities), size = densities, prob = prob)\n  list(\n    N = length(attacks),\n    true_a = true_a,\n    true_h = true_h,\n    densities = densities,\n    attacks = attacks, \n    prob = prob\n    )\n}\n\nEstablish _targets.R and _targets_r/globals/some-globals.R.\n\n\n\nlist(\n  stantargets::tar_stan_mcmc(name = one_spp, \n                stan_files = \"simple_type2.stan\",\n                data = generate_one_spp_type_too(),\n                stdout = R.utils::nullfile(),\n                stderr = R.utils::nullfile()\n  )\n)\n\nEstablish _targets.R and _targets_r/targets/run_one_model.R.\n\n\n\ntar_visnetwork()\n\n\n\n\n\n\ntar_make()\n\n✔ skip target one_spp_data\n✔ skip target one_spp_file_simple_type2\n✔ skip target one_spp_mcmc_simple_type2\n✔ skip target one_spp_diagnostics_simple_type2\n✔ skip target one_spp_summary_simple_type2\n✔ skip target one_spp_draws_simple_type2\n✔ skip pipeline [0.17 seconds]"
  },
  {
    "objectID": "posts/2022-11-11-multispecies-functional-response/index.html#footnotes",
    "href": "posts/2022-11-11-multispecies-functional-response/index.html#footnotes",
    "title": "Fitting a multispecies functional response in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThings that eat plants have to do the same; replace “kill” with “peel” etc.↩︎"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html",
    "title": "The evolution of plasticity",
    "section": "",
    "text": "options(tidyverse.quiet = TRUE)\nlibrary(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(brms)\n\n## functions to make models and so on\nsimulate_many_moms &lt;- function(pop_average_dponte = 138,\n                               pop_average_csize = 4,\n                               mom_quality_max = 4,\n                               quality_on_dponte = 2,\n                               quality_on_csize = .2,\n                               n_females = 42,\n                               lifespan = 5,\n                               temp_range  = c(2, 12)) {\n\n  general_temp &lt;- runif(lifespan, temp_range[1], max = temp_range[2])\n\n  general_temp_c &lt;- general_temp - mean(general_temp)\n\n  mom_qualities &lt;- runif(n_females, min = 0, max = 4)\n\n  many_moms_temperature &lt;- expand_grid(year = 1:lifespan,\n                                       idF1 = 1:n_females) |&gt;\n    mutate(mom_quality = mom_qualities[idF1],\n           general_temp = general_temp[year],\n           general_temp_c = general_temp_c[year],\n           ## adding the biology\n           ## Effect of temperature -- does it depend on quality? let's say that it DOES (for now)\n           effet_temp_dponte_qual = -.7*mom_quality,\n           effet_temp_csize_qual = .1*log(mom_quality),\n           # csize\n           mom_avg_csize = log(pop_average_csize) +  quality_on_csize*log(mom_quality),\n           temp_avg_csize = exp(mom_avg_csize + effet_temp_csize_qual*general_temp_c),\n           # dponte\n           mom_avg_dponte = pop_average_dponte + quality_on_dponte*mom_quality,\n           temp_avg_dponte = mom_avg_dponte + effet_temp_dponte_qual*general_temp_c,\n           ## observations\n           obs_csize = rpois(n = length(year), lambda = temp_avg_csize),\n           obs_dponte = rnorm(n = length(year), mean = temp_avg_dponte, sd = 3) |&gt; round()\n    )\n  return(many_moms_temperature)\n}\n\n\nbrms_dponte_csize &lt;- function(many_moms_temperature) {\n  ## define formulae\n  csize_model_bf &lt;- bf(obs_csize ~ 1 + general_temp_c + (1 + general_temp_c|f|idF1),\n                       family = poisson())\n\n  dponte_model_bf &lt;- bf(obs_dponte ~ 1 + general_temp_c + (1 + general_temp_c|f|idF1),\n                        family = gaussian())\n\n\n  ## set priors\n\n  ## run full model\n  full_model &lt;- brm(csize_model_bf + dponte_model_bf,\n                    data = many_moms_temperature,\n                    cores = 2, chains = 2)\n}"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#study-question",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#study-question",
    "title": "The evolution of plasticity",
    "section": "Study question",
    "text": "Study question\nPhenotypic plasiticity is the"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#data-simulation",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#data-simulation",
    "title": "The evolution of plasticity",
    "section": "Data simulation",
    "text": "Data simulation\nRARE to have more than two years per female\nlet’s start with one female\n\nn &lt;- 1\navg_csize &lt;- 5\n\nlifespan &lt;- 5\n\ngeneral_temp &lt;- runif(lifespan, 2, 12)\n\ngeneral_temp_c &lt;- general_temp - mean(general_temp)"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#fecundity",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#fecundity",
    "title": "The evolution of plasticity",
    "section": "fecundity",
    "text": "fecundity\n\\[\n\\begin{align}\n\\text{eggs} &\\sim \\text{Poisson}(e^{\\beta_0 + \\beta_1*(x - \\bar{x})})\n\\end{align}\n\\]\n\neffet_temp &lt;- .1\n\none_bird &lt;- tibble(year = 1:lifespan,\n       general_temp,\n       general_temp_c,\n       expected_clutch = log(avg_csize) + effet_temp * general_temp_c,\n       observed_clutch = rpois(n = length(year), \n                               lambda = exp(expected_clutch)))\n\none_bird\n\nMake a simple model to measure\n\nsummary(glm(observed_clutch ~ general_temp_c, data = one_bird))\n\n\none_bird |&gt; \n  ggplot(aes(x = general_temp, y = observed_clutch)) + \n  geom_point()"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#date-of-laying",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#date-of-laying",
    "title": "The evolution of plasticity",
    "section": "Date of laying",
    "text": "Date of laying\nWhen do birds lay eggs?\n\navg_dponte &lt;- 138\n\neffet_temp_dponte &lt;- -3\n\none_bird_dponte &lt;- tibble(year = 1:lifespan,\n       general_temp,\n       general_temp_c,\n       expected_dponte = avg_dponte + effet_temp_dponte * general_temp_c,\n       observed_dponte = round(rnorm(n = length(year), \n                               mean = expected_dponte,\n                               sd = 5)))\n\n\none_bird_dponte |&gt; \n  ggplot(aes(x = general_temp, y= observed_dponte)) + \n  geom_point()\n\nsummary(lm(observed_dponte ~ general_temp_c, data = one_bird_dponte))"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#combine-the-two",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#combine-the-two",
    "title": "The evolution of plasticity",
    "section": "combine the two",
    "text": "combine the two\nBirds which lay earlier also lay larger eggs, possibly because they are High Quality Moms.\n\n## population averages\npop_average_dponte &lt;- 138\npop_average_csize &lt;- 4\n\n## Effect of quality\nmom_quality &lt;- 4\nquality_on_dponte &lt;- 2\nquality_on_csize &lt;- .2\n\nlet’s observe five years for the high-quality Mom:\n\nquality_effects &lt;- tibble(\n  year = 1:lifespan,\n  mom_quality = mom_quality,\n  general_temp,\n  general_temp_c,\n  ## Effect of temperature -- does it depend on quality? let's say that it DOES (for now) \n  effet_temp_dponte_qual = -.7*mom_quality,\n  effet_temp_csize_qual = .1*log(mom_quality),\n  # csize\n  mom_avg_csize = log(pop_average_csize) +  quality_on_csize*log(mom_quality),\n  temp_avg_csize = exp(mom_avg_csize + effet_temp_csize_qual*general_temp_c),\n  # dponte\n  mom_avg_dponte = pop_average_dponte + quality_on_dponte*mom_quality,\n  temp_avg_dponte = mom_avg_dponte + effet_temp_dponte_qual*general_temp_c,\n  ## observations\n  obs_csize = rpois(n = length(year), lambda = temp_avg_csize),\n  obs_dponte = rnorm(n = length(year), mean = temp_avg_dponte, sd = 3) |&gt; round()\n)\n\nSome of these values are unreasonable! we can adjust later\nquality_effects |&gt; \n  ggplot(aes(x = general_temp, y = obs_csize)) + \n  geom_point()\n\nquality_effects |&gt; \n  ggplot(aes(x = general_temp, y = obs_dponte)) + geom_point()"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#multiple-females",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#multiple-females",
    "title": "The evolution of plasticity",
    "section": "Multiple females",
    "text": "Multiple females\nWe can repeat this process for multiple females at once! let’s wrap it in a function to make it easier to work with.\n\nsimulate_many_moms\nmany_moms_temperature &lt;- simulate_many_moms()\n\nlet’s plot it!\nmany_moms_temperature |&gt; \n  ggplot(aes(x = general_temp, y = obs_dponte)) + \n  geom_point()\n\nmany_moms_temperature |&gt; \n  ggplot(aes(x = general_temp, y = obs_dponte, group = idF1)) + \n  stat_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nmany_moms_temperature |&gt; \n  ggplot(aes(x = general_temp, y = obs_csize)) + \n  geom_point()\n\nmany_moms_temperature |&gt; \n  ggplot(aes(x = general_temp, y = obs_csize, group = idF1)) + \n  stat_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#model-in-brms",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#model-in-brms",
    "title": "The evolution of plasticity",
    "section": "model in brms",
    "text": "model in brms\nThe model above describes a situation where female swallows have some underlying trait (“quality”). This trait determines if this female will be above or below the rest of her population in two different outcomes: the timeing of her laying and the size of her clutch. This is a model structure that can’t be easily fit in lme4 at least as far as I know. However we can specify it in a very straightforward way using brms:"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#fitness",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#fitness",
    "title": "The evolution of plasticity",
    "section": "fitness",
    "text": "fitness"
  },
  {
    "objectID": "posts/2022-11-23-evolution-of-plasticity/index.html#notes",
    "href": "posts/2022-11-23-evolution-of-plasticity/index.html#notes",
    "title": "The evolution of plasticity",
    "section": "notes",
    "text": "notes\nvery small sample sizes per female – experiment with this (2 years or more)\nThe data are 0-truncated: only nesting females are measured!\nI think it is particularly interesting that the data are already conditioned on reproduction. that is, females who fail to reproduce at all don’t get included in the dataset. What effects could this have on our ability to detect interactions?"
  },
  {
    "objectID": "posts/2024-09-23-discrete-growth-latent/index.html",
    "href": "posts/2024-09-23-discrete-growth-latent/index.html",
    "title": "Ricker Model with Allee effects",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nThe Ricker model is a discrete model of population growth. Let’s get to know this model with a few extensions, and fit a demo of it in Stan."
  },
  {
    "objectID": "posts/2024-09-23-discrete-growth-latent/index.html#simulating-the-ricker-model",
    "href": "posts/2024-09-23-discrete-growth-latent/index.html#simulating-the-ricker-model",
    "title": "Ricker Model with Allee effects",
    "section": "Simulating the Ricker model",
    "text": "Simulating the Ricker model\nThe model in its simplest form looks like this:\n\\[\nN_{t+1} = N_te^{r\\left(1 - \\frac{N_t}{K}\\right)}\n\\] You can see that there is density dependence: when \\(N_t &lt;&lt; K\\), the population grows by a factor of \\(e^r\\) every time step. When \\(N_t = K\\), the population hits carrying capacity and the population grows by a factor of 1 : e.g. no change.\nTo start to draw this function I’ll assign values to the parameters and simulate a curve:\n\nr &lt;- 0.05\nc &lt;- 4\nN0 &lt;- 5\nK &lt;- 500\n\ntime &lt;- 300\n\nN &lt;- numeric(time)\n\nN[1] &lt;- N0\n\nfor (t in 2:time){\n  ri &lt;- rnorm(1, mean = 0, sd = 1)\n  # print(ri)\n  N[t] &lt;- N[t-1]*exp(r * (1 - N[t-1]/K))\n}\n\nplot(N, type = \"l\")"
  },
  {
    "objectID": "posts/2024-09-23-discrete-growth-latent/index.html#extending-it-uncertainty-and-allee-effects",
    "href": "posts/2024-09-23-discrete-growth-latent/index.html#extending-it-uncertainty-and-allee-effects",
    "title": "Ricker Model with Allee effects",
    "section": "Extending it: uncertainty and Allee effects",
    "text": "Extending it: uncertainty and Allee effects\nNow let’s add some uncertainty every time step! This is a kind of process error: growth rates bounce around randomly from one year to the next, perhaps because of unmodelled processes that affect the average growth rate in the population:\n\\[\n\\begin{align}\nN_{t+1} &= N_te^{r_t\\left(1 - \\frac{N_t}{K}\\right)} \\\\\n\\log(r_t) &\\sim \\text{Normal}(\\mu,\\sigma)\n\\end{align}\n\\] Here I’m using a log link on \\(r_t\\) because I want to be able to give \\(r_t\\) a mean and a variance without worrying about the growth rate flipping sign and suddenly becoming negative.\nThe second modification I want to make is to add Allee effects to the model. This is another kind of density dependence, but at the other extreme of density: instead of the population getting so large that it grows ever more slowly, it is also possible for the density to be so small that it grows slowly. Its the same kind of thing, and we add it in the same kind of way:\n\\[\n\\begin{align}\nN_{t+1} &= N_te^{r_t\\left(1 - \\frac{N_t}{K}\\right)\\left(1 - \\frac{C}{N_t}\\right)} \\\\\n\\log(r_t) &\\sim \\text{Normal}(\\mu,\\sigma)\n\\end{align}\n\\]\nNow we’re ready to simulate it all again, and this time I want to wrap it in a function for easy repetition later:\n\nsim_ricker_allee &lt;- function(\n    r = 0.05,\n    C = 9,#44\n    N0 = 7,\n    K = 500,\n    time = 300,\n    sd_process = 1.2){\n  \n  N &lt;- numeric(time)\n  \n  N[1] &lt;- N0\n  \n  for (t in 2:time){\n    ri &lt;- rnorm(1, mean = 0, sd = sd_process)\n    # print(ri)\n    N[t] &lt;- N[t-1]*exp(exp(log(r) + ri) * (1 - N[t-1]/K) * (1 - C/N[t-1]))\n  }\n  \n  return(N)\n}\n\nWe can repeat it many times using a sprinkle of syntatic sugar thanks to the purrr package1:\n\nallee_sim &lt;- map_df(1:50, ~ tibble(\n  N = sim_ricker_allee(\n    time = 300,C = 6, N0 = 7),\n  time = 1:300,\n  7),\n  .id = \"sim\") |&gt; \n  bind_cols(type = \"allee\")\n\n\nno_allee_sim &lt;- map_df(1:50, ~ tibble(\n  N = sim_ricker_allee(\n    time = 300,\n    C = 0,\n    N0 = 7),\n  time = 1:300,\n  7),\n  .id = \"sim\") |&gt; \n  bind_cols(type = \"no allee\")\n\n\n\ntwo_sims &lt;- bind_rows(allee_sim, no_allee_sim)\n\ntwo_sims |&gt; \n  ggplot(aes(x = time, y = N, group = sim)) + \n  geom_line() + \n  coord_cartesian(ylim = c(0, 1000)) + \n  facet_wrap(~type)\n\n\n\n\nTwo sets of simulations showing population growth\n\n\n\n\n\ntwo_sims |&gt; \n  ggplot(aes(x = time, y = N, group = sim)) + \n  geom_line() + \n  facet_wrap(~type) +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 100))\n\n\n\n\n\n\n\n\nand if we look at these another way we can see the difference:\n\ndeltaN_data &lt;- two_sims |&gt; \n  filter(type == \"no allee\") |&gt; \n  group_by(sim) |&gt; \n  mutate(deltaN = log(N/lag(N))) |&gt; \n  drop_na(deltaN)\n\ndeltaN_data |&gt; \n  ggplot(aes(x = N, y = deltaN)) + \n  geom_point() + \n  coord_cartesian(ylim = c(-10, 5), xlim = c(0, 600))\n\n\n\n\n\n\n\ndeltaN_data |&gt; \n  ggplot(aes(x = N, y = deltaN)) + \n  geom_point() + \n  coord_cartesian(ylim = c(-.0,1), xlim = c(0, 600))\n\n\n\n\n\n\n\n\nThis really doesn’t seem right! The y intercept is supposed to be \\(r\\), and the X intercept is supposed to be \\(K\\)! Is the cause the variation in growth rate? that is, the parameter sd_process above?\nLet’s make a new simulation where this is set to a very low value: :swea\n\nmap_df(1:50, ~ tibble(\n  N = sim_ricker_allee(\n    time = 300,\n    C = 0,\n    N0 = 7,\n    sd_process = 0.01),\n  time = 1:300,\n  7),\n  .id = \"sim\") |&gt; \n  group_by(sim) |&gt; \n  mutate(deltaN = log(N/lag(N))) |&gt; \n  drop_na(deltaN) |&gt; \n  ggplot(aes(x = N, y = deltaN)) + \n  geom_point() + \n  coord_cartesian(ylim = c(0, .1), xlim = c(0, 600)) + \n  geom_hline(yintercept = .05) + \n  geom_vline(xintercept = 500)\n\n\n\n\n\n\n\n\nMuch better! so the process error makes this pretty traditional plot go kind of haywire.\nInterestingly, looking back at the previous figures, you can see that the error is not around the “correct” line at all but mostly below it. That suggests that trying to model error based on lagged growth is probably not going to give a useful answer for the parameters"
  },
  {
    "objectID": "posts/2024-09-23-discrete-growth-latent/index.html#observation-error",
    "href": "posts/2024-09-23-discrete-growth-latent/index.html#observation-error",
    "title": "Ricker Model with Allee effects",
    "section": "Observation error",
    "text": "Observation error\nSo far all of this has been in a perfect, imaginary world where we have perfect information on the population density. In reality, we’ll always have a sample of the population density. A simple model for this variation is that it follows a Poisson distribution:\n\\[\n\\begin{align}\nY_t &\\sim \\text{Poisson}(N_t)\\\\\nN_{t+1} &= N_t e^{r\\left(1 - \\frac{N_t}{K}\\right)} \\\\\n\\end{align}\n\\]\nLets do a simulation of several observations of one single time series:\n\nset.seed(1618)\navg_dens &lt;- sim_ricker_allee(C = 5, N0 = 7, time = 120)\n\nobs_dens &lt;- avg_dens |&gt; \n  imap(\n    ~tibble(\n      time_id = as.numeric(.y),\n      obs = rpois(5, lambda = .x),\n      obs_id = seq_along(obs)\n    )\n    \n  ) |&gt; \n  bind_rows()\n\n\nobs_dens |&gt; \n  ggplot(aes(x = time_id, y = obs,group = obs_id)) + \n  geom_line() + \n  geom_line(aes(x = time_id, y = avg), inherit.aes = FALSE, \n            col = \"red\",\n            data = tibble(\n              time_id = seq_along(avg_dens),\n              avg = avg_dens))\n\n\n\n\n\n\n\n\nThis shows a few things of interest: the wiggling red line, which shows variation in growth rate at each timestep. This is process error. We can also see variation around this; these is variation coming from a Poisson distribution centered on the true mean population size."
  },
  {
    "objectID": "posts/2024-09-23-discrete-growth-latent/index.html#coding-and-validating-a-model",
    "href": "posts/2024-09-23-discrete-growth-latent/index.html#coding-and-validating-a-model",
    "title": "Ricker Model with Allee effects",
    "section": "Coding and validating a model",
    "text": "Coding and validating a model\nWe’re going to torture the math a little bit, to make it more convenient to write it Stan:\nTake the expression for the average and log both sides:\n\\[\n\\begin{align}\nN_{t+1} &= N_te^{r\\left(1 - \\frac{N_t}{K}\\right)} \\\\\n\\ln(N_{t+1}) &= \\ln{N_t} + r(1 - \\frac{N_t}{K})  \\\\\nL_{t+1} &= L_t + e^{\\ln{r} + \\ln(1 - e^{L_t - \\ln{K}})} \\\\\nL_{t+1} &= L_t + e^{s + \\ln(1 - e^{L_t - J})} \\\\\n\\end{align}\n\\] Here to keep notation simple I’m just writing\n\n\\(s = \\ln{r}\\)\n\\(J = \\ln{K}\\)\n\\(Lt = \\ln{N_t}\\)\n\nI know what you’re thinking: get help, Andrew!\nThere’s a couple reasons for this violence:\n\nWorking on the log scale is easier for parameter estimation. it keeps values on similar scales, even though, for example \\(r\\) and \\(K\\) have very different magnitudes.\nWe can take advantage of Stan’s built-in composed functions\nit will be easier to add hierarchical effects if (when :P ) we want to do that!\n\nHere’s a more complete rendering of the model, which will set us up for writing Stan code in the next section:\n\\[\n\\begin{align}\nY_i &\\sim \\text{Poisson\\_log}(L_{t[i]}) \\\\\nL_{t+1} &= L_t + e^{\\left(s + \\ln\\left(1 - e^{L_t - J}\\right)\\right)} \\\\\nL_0 &\\sim \\text{Normal}(2,1) \\\\\ns &\\sim \\text{Normal}(-3, 0.5) \\\\\nJ &\\sim \\text{Normal}(6, 1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2024-09-23-discrete-growth-latent/index.html#footnotes",
    "href": "posts/2024-09-23-discrete-growth-latent/index.html#footnotes",
    "title": "Ricker Model with Allee effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m old enough to remember when we did everything with for loops and the *apply family and I just hope the youth are grateful for the advances they have now!↩︎"
  },
  {
    "objectID": "posts/2016-12-07-population-growth-functional-programming/index.html",
    "href": "posts/2016-12-07-population-growth-functional-programming/index.html",
    "title": "Population growth with functional programming",
    "section": "",
    "text": "Today I want to tell you about an approach for functional programming in R – and then apply it to studying population growth!\nI have been studying some of the purrr functions lately. They are a useful family of functions for performing two common tasks in R: manipulating lists and altering the behaviour of functions. If you’d like a high-quality guide to this group of functions, set aside some time to work through Jenny Bryan’s excellent tutorial and Hadley Wickham’s chapter on Lists.\nI was inspired to write this post after reading This StackOverflow question by jebyrnes. He asks:\nand there is! An answerer mentioned purr::accumulate(). In this post I’m going to expand on the approach they suggest. accumulate, and its twin reduce, are examples of functionals – functions that take functions as their arguments, and manipulate their behaviour in some way. purrr::accumulate is a wrapper around to Reduce from the base package, with the argument accumulate = TRUE.\naccumulate is normally used when you want to do some cumulative function all along a vector. For example, we can reproduce the cumulative some of a vector like this (same output as cumsum(1:10))\nlibrary(purrr)\naccumulate(1:10, ~ .x + .y)\n\n [1]  1  3  6 10 15 21 28 36 45 55\n.x and .y here are just a handy way of writing “the first thing” and “the second thing”. Then accumulate goes down the vector 1:10, and takes the first thing (1) adds it to the second thing (2) and so on….\nHowever, this is not the only way it works! accumulate can take an initial value (.init) and can work on a dummy variable. If its starting function does nothing but modify an element, it will just keep modifying it: so instead of f(1, 2), f(f(1, 2), 3) we get f(.init), f(f(.init)) etc:\ndummy &lt;- numeric(10)\ndummy\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\naccumulate(dummy, ~ .* 1.5, .init = 50)\n\n [1]   50.0000   75.0000  112.5000  168.7500  253.1250  379.6875  569.5312\n [8]  854.2969 1281.4453 1922.1680 2883.2520\nClearly, the 0s are not involved in any calculation (the answer would be 0!). instead, you just get the starting value multiplied by 1.5 each time!\nThis already suggests an awesome biological interpretation: logistic population growth.\npop_size &lt;- accumulate(dummy, ~ .* 1.5, .init = 50)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndata_frame(time = 1:11, pop_size) %&gt;% \n  ggplot(aes(x = time, y = pop_size)) + \n  geom_point() + geom_line()\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\n\n\n\n\n\n\nFigure 1\nOn thing I like about this is that is is to much easier to look at – it look like the common biological equation for population growth:\n\\[\nN_{t+1} = r*N_t\n\\]"
  },
  {
    "objectID": "posts/2016-12-07-population-growth-functional-programming/index.html#so-is-this-useful",
    "href": "posts/2016-12-07-population-growth-functional-programming/index.html#so-is-this-useful",
    "title": "Population growth with functional programming",
    "section": "So, is this useful?",
    "text": "So, is this useful?\nI wonder if this might be an interesting pedagogical tool. I feel like it might place the emphasis a bit differently to for-loops. Perhaps a for loop emphasizes the passage of time – how, at each time step (each i for example) certain things happen in a certain order (the Resource grows, the Predator kills some, then some predators die, etc). On the other hand, I feel like the functional programming approach emphasizes how a population (or pair of populations) is transformed. Each little function has some parameters – which are either, constant, varying, and/or influenced by something besides population size – and each little function does only one thing – transform the population between one time step and the next."
  },
  {
    "objectID": "posts/2023-08-24-imputation-nonnormal/index.html",
    "href": "posts/2023-08-24-imputation-nonnormal/index.html",
    "title": "Missing data in non-normal distributions",
    "section": "",
    "text": "library(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)"
  },
  {
    "objectID": "posts/2023-08-24-imputation-nonnormal/index.html#working-with-missingness",
    "href": "posts/2023-08-24-imputation-nonnormal/index.html#working-with-missingness",
    "title": "Missing data in non-normal distributions",
    "section": "working with missingness",
    "text": "working with missingness\nI wish the topic of missingness was introduced much earlier in statistical ecology! Most ecological datasets have some examples of this. Working directly with missing data has many advantages, including letting us use ALL the information we have. There are many arguments, but that’s the one I find most compelling. In our science, each datapoint costs dearly in money and effort – the least we can do is learn the tools to use them well!"
  },
  {
    "objectID": "posts/2023-08-24-imputation-nonnormal/index.html#roadmap",
    "href": "posts/2023-08-24-imputation-nonnormal/index.html#roadmap",
    "title": "Missing data in non-normal distributions",
    "section": "Roadmap",
    "text": "Roadmap\n\nnormal distributions with missing data\nnonnormal distributions with missing data\nregression with missing information in just the x\nregression with missing data in both\nnonlinear, nonnormal missing data in both"
  },
  {
    "objectID": "posts/2023-08-24-imputation-nonnormal/index.html#normal-distributions-with-missing-data",
    "href": "posts/2023-08-24-imputation-nonnormal/index.html#normal-distributions-with-missing-data",
    "title": "Missing data in non-normal distributions",
    "section": "Normal distributions with missing data",
    "text": "Normal distributions with missing data\nHere is code from Flavio Affinito, adapting the code in the Stan Guide for continuous missing data:\n\nMissingDataImputation2 &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-08-24-imputation-nonnormal/MissingDataImputation2.stan\"))\nMissingDataImputation2\n\ndata {\n  int&lt;lower=0&gt; N_tot;          // Total number of observations\n  int&lt;lower=0&gt; N_miss;        // Number of missing values\n  int&lt;lower=0&gt; N_obs;        // Number of observed values\n  array[N_obs] int&lt;lower=1, upper=N_tot&gt; ii_obs;  // Position of observed values in the column\n  array [N_miss] int&lt;lower=1, upper=N_tot&gt; ii_mis; // Position of the missing values in the column\n  vector[N_obs] y_obs;            // Observed values\n  //int&lt;lower=0&gt; N_year;     // Number of years in the dataset\n}\n\n\nparameters {\n  real mu;              // Population mean\n  real&lt;lower=0&gt; sigma;     // Common standard deviation\n  vector[N_miss] y_imputed;       // Imputed outcomes for missing data\n}\n\ntransformed parameters {\n  vector[N_tot] y;      // create the dataset to fit the likelihood\n  y[ii_obs] = y_obs;        // assign observations to the positions with observations\n  y[ii_mis] = y_imputed;    // assign parameters (y missing) to the positions without observations\n}\n\nmodel {\n  // Priors\n  mu ~ normal(50, 10);\n  sigma ~ exponential(.1);\n\n  // Likelihood for observed and imputated data\n  y ~ normal(mu, sigma);\n}\n\ngenerated quantities {\n  vector[N_tot] y_pred;\n\n  for (i in 1:N_tot) {\n    y_pred[i] = normal_rng(mu, sigma);\n  }\n}\n\n\nLet’s try it out with 42 numbers, of which 6 are missing:\n\nset.seed(1234)\nxx&lt;- rnorm(42, mean = 50, sd = 10)\nxx2 &lt;- xx\nxx2[sample(42, 6, replace = FALSE)] &lt;- NA\n\nhist(xx2)\n\n\n\n\n\n\n\n\n\nnormal_missing &lt;- MissingDataImputation2$sample(\n  data = list(\n    N_tot = 42,\n    N_miss = 6,\n    N_obs = 42-6,\n    ii_obs = which(!is.na(xx2)),\n    ii_mis = which(is.na(xx2)),\n    y_obs = xx2[which(!is.na(xx2))]), \n  parallel_chains = 4,\n  refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.6 seconds.\n\n\n\nposterior_and_original &lt;- normal_missing$draws() |&gt; \n  gather_rvars(y_imputed[i]) |&gt; \n  mutate(xx = xx[which(is.na(xx2))])\n\n\n\nlibrary(tidybayes)\nposterior_and_original |&gt; \n  ggplot(aes(x = xx, ydist = .value)) + \n  stat_pointinterval()\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nunsurprisingly this is the same distribution for all parameters"
  },
  {
    "objectID": "posts/2023-08-24-imputation-nonnormal/index.html#with-a-linear-relationship",
    "href": "posts/2023-08-24-imputation-nonnormal/index.html#with-a-linear-relationship",
    "title": "Missing data in non-normal distributions",
    "section": "with a linear relationship",
    "text": "with a linear relationship\nlet’s imagine there is a clear linear relationship but we still have missing values:\n\nyy_bar &lt;- 12 + 2*(xx - 50)\n\nyy &lt;- rnorm(42, yy_bar, sd = 3)\n\nplot(xx, yy)\n\n\n\n\n\n\n\n\nwith the same 6 datapoints missing\n\nregression_imputation &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-08-24-imputation-nonnormal/regression_imputation.stan\"))\nregression_imputation\n\ndata {\n  int&lt;lower=0&gt; N_tot;          // Total number of observations\n  int&lt;lower=0&gt; N_miss;        // Number of missing values\n  int&lt;lower=0&gt; N_obs;        // Number of observed values\n  array[N_obs] int&lt;lower=1, upper=N_tot&gt; ii_obs;  // Position of observed values in the column\n  array [N_miss] int&lt;lower=1, upper=N_tot&gt; ii_mis; // Position of the missing values in the column\n  vector[N_obs] x_obs;            // Observed values\n  vector[N_tot] y;\n}\n\n\nparameters {\n  real x_mu;              // Population mean\n  real&lt;lower=0&gt; x_sigma;     // Common standard deviation\n  vector[N_miss] x_imputed;       // Imputed values for missing data\n  real slope;\n  real intercept;\n  real&lt;lower=0&gt; y_sigma;\n}\n\ntransformed parameters {\n  vector[N_tot] x;      // create the dataset to fit the likelihood\n  x[ii_obs] = x_obs;        // assign observations to the positions with observations\n  x[ii_mis] = x_imputed;    // assign parameters (y missing) to the positions without observations\n}\n\nmodel {\n  // Priors\n  x_mu ~ normal(50, 10);\n  x_sigma ~ exponential(.1);\n  intercept ~ normal(10, 4);\n  y_sigma ~ exponential(.1);\n\n  // Likelihood for observed and imputated data (x)\n  x ~ normal(x_mu, x_sigma);\n  // LIkelihood for the response variable\n  // y ~ normal(intercept + slope * (x - 50), y_sigma);\n  y ~ normal(intercept + slope * (x - x_mu), y_sigma);\n}\n\ngenerated quantities {\n  vector[N_tot] x_pred;\n\n  for (i in 1:N_tot) {\n    x_pred[i] = normal_rng(x_mu, x_sigma);\n  }\n}\n\n\n\nregression_missing &lt;- regression_imputation$sample(\n  data = list(\n    N_tot = 42,\n    N_miss = 6,\n    N_obs = 42 - 6,\n    ii_obs = which(!is.na(xx2)),\n    ii_mis = which(is.na(xx2)),\n    x_obs = xx2[which(!is.na(xx2))],\n    y = yy\n    ),\n  parallel_chains = 4,\n  refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.8 seconds.\nChain 4 finished in 0.7 seconds.\nChain 2 finished in 0.8 seconds.\nChain 3 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.9 seconds.\n\n\nposterior_and_original &lt;- regression_missing$draws() |&gt; \n  gather_rvars(x_imputed[i]) |&gt; \n  mutate(xx = xx[which(is.na(xx2))],\n         yy = yy[which(is.na(xx2))]\n  )\n\n\nposterior_and_original |&gt; \n  ggplot(aes(x = xx, ydist = .value)) + \n  stat_pointinterval() + \n  geom_abline(intercept = 0, slope = 1)\nposterior_and_original |&gt; \n  ggplot(aes(xdist = .value, y = yy)) + \n  stat_pointinterval() + \n  geom_point(aes(x = xx, y = yy), \n             col = \"red\",\n             inherit.aes = FALSE,\n             data = tibble(\n               xx = xx[which(!is.na(xx2))],\n               yy = yy[which(!is.na(xx2))]\n               ))\n\n\n\n\n\n\nimputed values (y axis) vs the real values (x axis), and the 1:1 line for comparison. The posterior distributions are close to the truth, because the regression equation lets information flow in both directions.\n\n\n\n\n\n\n\nThe original relationship, with posterior distributions for missing x variables.\n\n\n\n\n\nThe major takeaway from this model is that once we have created our merged parameter and data vector x, within the transformed parameters block, we can use it just like a vector made entirely of observations. The model structure causes information to “flow both ways” and automatically gives us the posterior distribution that is most consistent with our data and model. From the point of view of the model, there is no difference between a missing observation and any other unknown number, like a standard deviation or average.\nI also enjoy that we are modelling an average for the independent \\(x\\) variable – and then using that parameter to center the vector before modelling! This is useful if you want to set a prior on the intercept for what the average X value should be. Normally it would be tricky to center a variable with missing data (if you don’t know all the values, how can you know their average?) but Bayes makes it effortless."
  },
  {
    "objectID": "posts/2023-08-24-imputation-nonnormal/index.html#count-data-with-missing-numbers",
    "href": "posts/2023-08-24-imputation-nonnormal/index.html#count-data-with-missing-numbers",
    "title": "Missing data in non-normal distributions",
    "section": "Count data with missing numbers",
    "text": "Count data with missing numbers\nTo extend this model further, I want to try modelling count data for both an independent and dependent variable.\nIn this example, there will be missing data an independent variable. However, we’re not going to be able to model the missing counts as counts, because Stan does not allow discrete missing data. Instead we’ll treat the unobserved data as lognormal, and see how wrong we are."
  },
  {
    "objectID": "posts/2023-11-24-how-to-model-growth/index.html",
    "href": "posts/2023-11-24-how-to-model-growth/index.html",
    "title": "How to model discrete growth",
    "section": "",
    "text": "library(cmdstanr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(targets)\ntar_load(one_time)\nplot(one_time, type = \"l\")"
  },
  {
    "objectID": "posts/2023-11-24-how-to-model-growth/index.html#controversy",
    "href": "posts/2023-11-24-how-to-model-growth/index.html#controversy",
    "title": "How to model discrete growth",
    "section": "Controversy!",
    "text": "Controversy!"
  },
  {
    "objectID": "posts/2023-11-24-how-to-model-growth/index.html#section",
    "href": "posts/2023-11-24-how-to-model-growth/index.html#section",
    "title": "How to model discrete growth",
    "section": "",
    "text": "# load the model in stan\ntransition &lt;- cmdstan_model(here::here(\"posts/2023-11-24-how-to-model-growth/ar1.stan\"))\n\ntransition\n\ndata{\n  int n;\n  vector[n] time;\n  vector[n] x;\n}\n// transformed data {\n//   vector[n] x = log(pop);\n// }\nparameters {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0,upper=1&gt; b;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  real mu_max = a / (1 - b);\n  real sigma_max = sigma /sqrt(1 - b^2);\n}\nmodel {\n  a ~ normal(2, .5);\n  b ~ beta(5,2);\n  sigma ~ exponential(5);\n  x ~ normal(\n    mu_max .* (1 - pow(b, time)),\n    sigma_max .* sqrt(1 - pow(b^2, time))\n    );\n}\ngenerated quantities {\n  vector[15] x_pred;\n  x_pred[1] = 0;\n  for (j in 1:14) {\n    x_pred[j+1] = normal_rng(\n      mu_max * (1 - pow(b, j)),\n      sigma_max * sqrt(1 - pow(b^2, j))\n      );\n  }\n}\n\n\nHere is another approach, using a lagged population growth model\n\n# load the model in stan\nlagged_growth &lt;- cmdstan_model(here::here(\"posts/2023-11-24-how-to-model-growth/multiple_spp_ar1.stan\"))\n\nlagged_growth\n\ndata {\n  int n;\n  int nclone;\n  vector[n] x;\n  array[n] int&lt;lower=1, upper=nclone&gt; clone_id;\n  // for predictions\n  int&lt;lower=0, upper=1&gt; fit;\n  int nyear;\n}\ntransformed data {\n  array[n - nclone] int time;\n  array[n - nclone] int time_m1;\n  for (i in 2:n) {\n    if (clone_id[i] == clone_id[i-1]) {\n      time[i - clone_id[i]] = i;\n      time_m1[i - clone_id[i]] = i - 1;\n    }\n  }\n}\nparameters {\n  real&lt;lower=0&gt; a;\n  real&lt;lower=0,upper=1&gt; b;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n\n  a ~ normal(2, .5);\n  b ~ beta(5,2);\n  sigma ~ exponential(5);\n\n  if (fit == 1) {\n\n    x[time] ~ normal(\n      a + b * x[time_m1],\n      sigma);\n  }\n}\ngenerated quantities {\n  vector[nyear] x_pred;\n\n  x_pred[1] = 0;\n\n  for (j in 2:nyear){\n    x_pred[j] =  a + b * x_pred[j-1] + normal_rng(0, sigma);\n  }\n}"
  },
  {
    "objectID": "posts/2023-11-24-how-to-model-growth/index.html#simulations",
    "href": "posts/2023-11-24-how-to-model-growth/index.html#simulations",
    "title": "How to model discrete growth",
    "section": "Simulations",
    "text": "Simulations\nHere are simulations from a one-species AR-1 model that imitate Ives et al. figure 1.\n\nsimulate_pop_growth &lt;- function(\n    a = 0, \n    b, \n    sigma = 1, \n    tmax = 50, \n    x0 = -8) {\n  \n  xvec &lt;- numeric(tmax)\n  \n  xvec[1] &lt;- x0\n  \n  ## process error\n  eta &lt;- rnorm(tmax, mean = 0, sd = sigma)\n  \n  for(time in 2:tmax){\n    xvec[time] &lt;- a + b*xvec[time-1] + eta[time]\n  }\n  \n  return(xvec)\n}\n\nI’m going to simulate a modest number of time series, and choose parameters to make the time series slightly resemble the aphid experiment.\n\na_fig = 1\nb_fig = .8\nsigma_fig = .7\n\nts_data &lt;- map_dfr(1:12, \n        ~ tibble(\n          pop = simulate_pop_growth(\n            a = a_fig, \n            b = b_fig,\n            tmax = 16, \n            sigma = sigma_fig, \n            x0 = 0\n            ),\n          time = 0:(length(pop)-1)\n        ),\n        .id = \"sim\"\n)\n\nts_data |&gt; \n  ggplot(aes(x =time, y = pop, group = sim)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\nknitr::kable(head(ts_data))\n\n\n\n\nsim\npop\ntime\n\n\n\n\n1\n0.0000000\n0\n\n\n1\n0.9548568\n1\n\n\n1\n2.8629644\n2\n\n\n1\n2.8740721\n3\n\n\n1\n2.5695696\n4\n\n\n1\n3.2455612\n5"
  },
  {
    "objectID": "posts/2023-11-24-how-to-model-growth/index.html#transition-distribution",
    "href": "posts/2023-11-24-how-to-model-growth/index.html#transition-distribution",
    "title": "How to model discrete growth",
    "section": "Transition distribution",
    "text": "Transition distribution\n\nts_data_nozero &lt;- filter(ts_data, time != 0)\n\n\ntransition_sample &lt;- transition$sample(\n  data = list(n = nrow(ts_data_nozero),\n              x = ts_data_nozero$pop,\n              time = ts_data_nozero$time),\n  parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.7 seconds.\nChain 3 finished in 1.7 seconds.\nChain 2 finished in 1.8 seconds.\nChain 4 finished in 1.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.7 seconds.\nTotal execution time: 2.0 seconds.\n\n\n\ntransition_sample |&gt; \n  spread_rvars(x_pred[time]) |&gt; \n  ggplot(aes(x = time-1, ydist = x_pred)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  theme_bw() + \n  geom_line(aes(x = time, y = pop, group = sim),\n            inherit.aes = FALSE, data = ts_data) + \n  labs(x = \"Time\", y = \"log population size\")\n\n\n\n\n\n\n\n\n\ntransition_sample |&gt;\n  gather_rvars(a, b, sigma) |&gt; \n  ggplot(aes(y = .variable, dist = .value)) + \n  stat_halfeye() + \n  geom_point(\n    aes(y = .variable,  x = .value),\n    inherit.aes = FALSE,\n    data = tribble(\n      ~ .variable, ~.value,\n      \"a\", a_fig, \n      \"b\", b_fig,\n      \"sigma\", sigma_fig), col = \"red\", size = 2)"
  },
  {
    "objectID": "posts/2023-11-24-how-to-model-growth/index.html#lagged-model",
    "href": "posts/2023-11-24-how-to-model-growth/index.html#lagged-model",
    "title": "How to model discrete growth",
    "section": "Lagged model",
    "text": "Lagged model\nThis time there is no need to drop 0s\n\nts_data &lt;- ts_data |&gt; \n  mutate(sim = readr::parse_number(sim))\n\nlagged_growth_sample &lt;- lagged_growth$sample(\n  data = list(n = nrow(ts_data),\n              nclone = max(ts_data$sim),\n              x = ts_data$pop,\n              time = ts_data$time,\n              clone_id = ts_data$sim,\n              fit = 1,\n              nyear = 15),\n  parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.5 seconds.\n\n\n\npredictions\n\nlagged_growth_sample |&gt; \n  spread_rvars(x_pred[time]) |&gt; \n  ggplot(aes(x = time-1, ydist = x_pred)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  theme_bw() + \n  geom_line(aes(x = time, y = pop, group = sim),\n            inherit.aes = FALSE, data = ts_data) + \n  labs(x = \"Time\", y = \"log population size\")\n\n\n\n\n\n\n\n\n\n\nparameters\n\nlagged_growth_sample |&gt;\n  gather_rvars(a, b, sigma) |&gt; \n  ggplot(aes(y = .variable, dist = .value)) + \n  stat_halfeye() + \n  geom_point(\n    aes(y = .variable,  x = .value),\n    inherit.aes = FALSE,\n    data = tribble(\n      ~ .variable, ~.value,\n      \"a\", a_fig, \n      \"b\", b_fig,\n      \"sigma\", sigma_fig), col = \"red\", size = 2)"
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html",
    "title": "Validating a model of selection on plasticity",
    "section": "",
    "text": "library(cmdstanr)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html#the-challenge",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html#the-challenge",
    "title": "Validating a model of selection on plasticity",
    "section": "The challenge",
    "text": "The challenge\nStudying selection on phenotypic plasticity is challenging. First, because phenotypic plasticity is a slope – it is the change in a trait when an environmental variable changes. Secondly, because many traits of animals also cause other traits. For example, arriving later at a breeding site causes an individual to lay fewer eggs (because less food is available)."
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html#the-system",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html#the-system",
    "title": "Validating a model of selection on plasticity",
    "section": "The system",
    "text": "The system\nLet’s begin just with a simulation of three interrelated traits:\n\nThe date when a bird arrives, which determines\nHow many eggs they lay, out of which there is\nSome number of surviving offspring\n\n\n## how many birds\nnbirds &lt;- 57\n# simulate arrival dates -- two weeks before and after whatever the average is\ndarrive &lt;- runif(nbirds, min = -14, max = 14) |&gt; round()\n\n## simulate clutch sizes -- decrease by 4% each day\navg_clutch &lt;- 4.5\neffect_per_day &lt;- .97\n\nclutch &lt;- rpois(nbirds, exp(log(avg_clutch) + log(effect_per_day)*darrive))\n\nplot(darrive, clutch)\n\n\n\n\n\n\n\n\nAs an aside, this would be 0-truncated, since birds who don’t lay eggs don’t get observed at all.\n\n# simulate hatching success\nsuccess &lt;- rbinom(nbirds, size = clutch, prob = .86)\n\nplot(darrive, success)\n\n\n\n\n\n\n\n\nwhat’s important to see here is that there is a negative correlation, even though the arrival date has no direct effect on the outcome\n\nsummary(glm(success ~ darrive, family = \"poisson\"))\n\n\nCall:\nglm(formula = success ~ darrive, family = \"poisson\")\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.330746   0.068901  19.314  &lt; 2e-16 ***\ndarrive     -0.039128   0.008791  -4.451 8.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 78.184  on 56  degrees of freedom\nResidual deviance: 58.402  on 55  degrees of freedom\nAIC: 232.38\n\nNumber of Fisher Scoring iterations: 5\n\n\nBut, if we use a binomial model that knows about the number of possible successful chicks, then we see what we expect:\n\nbin_mod &lt;- (glm(cbind(success, clutch - success) ~ 1, family = binomial(link = \"logit\")))\n\nplogis(coef(bin_mod))\n\n(Intercept) \n  0.8842975 \n\n\nWhich matches the simulation above.\nif we put darrive in the model, the effect should be very close to 0 with overlap.\nIf we imagine that the laying date effects the survival p, then we should see an effect close to 0\n\nsummary(glm(\n  cbind(success, clutch - success) ~ 1 + darrive,\n            family = binomial(link = \"logit\")))\n\n\nCall:\nglm(formula = cbind(success, clutch - success) ~ 1 + darrive, \n    family = binomial(link = \"logit\"))\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.03026    0.20204  10.049   &lt;2e-16 ***\ndarrive     -0.03052    0.02683  -1.137    0.255    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 63.900  on 55  degrees of freedom\nResidual deviance: 62.595  on 54  degrees of freedom\nAIC: 102.23\n\nNumber of Fisher Scoring iterations: 4\n\n\nOne Stan model can model all of these at the same time\n\none_indiv &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv.stan\"))\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = one_indiv_model_namespace::one_indiv_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-2870599e2fe2.hpp:349: note:   by 'one_indiv_model_namespace::one_indiv_model::write_array'\n  349 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\n\nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = one_indiv_model_namespace::one_indiv_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \n\n\nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-2870599e2fe2.hpp:349: note:   by 'one_indiv_model_namespace::one_indiv_model::write_array'\n  349 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\none_indiv\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n}\nmodel {\n  success ~ binomial_logit(clutch, logit_psuccess);\n  clutch ~ poisson_log(log_avgclutch + log_b_date * darrive);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n}\n\n\n\none_indiv_post &lt;- one_indiv$sample(\n  data = list(nbirds = nbirds,\n              clutch = clutch, \n              success = success, \n              darrive = darrive))\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 1.0 seconds.\n\none_indiv_post\n\n       variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n lp__           19.63  19.97 1.28 1.02 17.07 20.96 1.00     1891     2648\n logit_psuccess  1.56   1.56 0.13 0.13  1.35  1.77 1.00     3807     2735\n log_avgclutch   1.41   1.41 0.06 0.06  1.30  1.51 1.00     3253     2731\n log_b_date     -0.04  -0.04 0.01 0.01 -0.05 -0.02 1.00     4038     2929\n\n\nreasonably close to true values:\n\nplogis(1.44)\n\n[1] 0.8084547\n\nlog(avg_clutch)\n\n[1] 1.504077\n\nlog(effect_per_day)\n\n[1] -0.03045921\n\n\n\nsimulate_some_birds &lt;- function(nbirds = 57, \n         log_b_date = log(.97),\n         log_avgclutch = log(4.5),\n         logit_psuccess = qlogis(.84)){\n  \n  # simulate arrival dates -- two weeks before and after whatever the average is\n  darrive &lt;- runif(nbirds, min = -14, max = 14) |&gt; round()\n  \n  ## simulate clutch sizes -- decrease  each day\n  clutch &lt;- rpois(nbirds, exp(log_avgclutch + log_b_date*darrive))\n  \n  ## simulate success\n  success &lt;- rbinom(nbirds, size = clutch, prob = plogis(logit_psuccess))\n  \n  return(list(\n    data_list = list(\n      nbirds = nbirds,\n      darrive = darrive, \n      clutch = clutch, \n      success = success \n    ),\n    true_values = tribble(\n      ~variable, ~true_value,\n      \"log_b_date\", log_b_date,\n      \"log_avgclutch\", log_avgclutch,\n      \"logit_psuccess\", logit_psuccess\n    )\n  ))\n}\n\n\ndata_for_simulation &lt;- simulate_some_birds()\n\none_indiv_post &lt;- one_indiv$sample(data = data_for_simulation$data_list,\n                                   refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.7 seconds.\n\ncomparison &lt;- one_indiv_post |&gt; \n  # tidybayes::gather_rvars(logit_psuccess, log_avgclutch, log_b_date) |&gt; \n  tidybayes::tidy_draws() |&gt; tidybayes::gather_variables() |&gt; \n  right_join(data_for_simulation$true_values, by = c(\".variable\" = \"variable\"))\n\n\ncomparison |&gt; \n  ggplot(aes(y = .variable, x = .value)) + \n  stat_halfeye() + \n  geom_point(aes(y = variable, x = true_value),\n             col = \"orange\",\n             pch = \"|\",\n             size = 10, data = data_for_simulation$true_values)"
  },
  {
    "objectID": "posts/2023-02-02-selection-on-plasticity/index.html#no-0-birds",
    "href": "posts/2023-02-02-selection-on-plasticity/index.html#no-0-birds",
    "title": "Validating a model of selection on plasticity",
    "section": "No 0 birds",
    "text": "No 0 birds\nThis system is a little challenging, since we never observe 0 eggs per bird – if a bird cannot lay eggs (e.g. it does not find a nest spot) then it goes uncounted\nTo simulate this, I’ll drop the 0 clutches before doing the rest of the simulations. This means that sample size will be less than or equal to the “nbirds” argument.\n\nsimulate_some_birds_nonzero &lt;- function(nbirds = 57, \n         log_b_date = log(.97),\n         log_avgclutch = log(4.5),\n         logit_psuccess = qlogis(.84)){\n  \n  # simulate arrival dates -- two weeks before and after whatever the average is\n  darrive &lt;- runif(nbirds, min = -14, max = 14) |&gt; round()\n  \n  ## simulate clutch sizes -- decrease  each day\n  clutch &lt;- rpois(nbirds, exp(log_avgclutch + log_b_date*darrive))\n  # drop 0 nests\n  nonzero_clutch &lt;- which(clutch &gt; 0)\n  \n  ## simulate success\n  success &lt;- rbinom(nbirds, size = clutch, prob = plogis(logit_psuccess))\n  \n  return(list(\n    data_list = list(\n      nbirds = length(nonzero_clutch),\n      darrive = darrive[nonzero_clutch], \n      clutch  =  clutch[nonzero_clutch], \n      success = success[nonzero_clutch] \n    ),\n    true_values = tribble(\n      ~variable, ~true_value,\n      \"log_b_date\", log_b_date,\n      \"log_avgclutch\", log_avgclutch,\n      \"logit_psuccess\", logit_psuccess\n    )\n  ))\n}\n\nset.seed(1234)\nsome_nonzeros &lt;- simulate_some_birds_nonzero(nbirds = 200)\n\na plot to confirm that it works:\n\nsome_nonzeros$data_list |&gt; \n  as.data.frame() |&gt; \n  ggplot(aes(x = darrive, y = clutch)) + geom_point()\n\n\n\n\n\n\n\n\nAnd fit the posterior\n\nplot_posterior_true &lt;- function(simdata, stanmodel){\n  model_post &lt;- stanmodel$sample(data = simdata$data_list,\n                                     refresh = 0, parallel_chains = 4)\n  \n  comparison &lt;- model_post |&gt; \n    tidybayes::tidy_draws() |&gt; \n    tidybayes::gather_variables() |&gt; \n    right_join(simdata$true_values, by = c(\".variable\" = \"variable\"))\n  \n  \n  comparison |&gt; \n    ggplot(aes(y = .variable, x = .value)) + \n    stat_halfeye() + \n    geom_point(\n      aes(y = variable, x = true_value),\n      col = \"orange\",\n      pch = \"|\",\n      size = 10, data = simdata$true_values)\n}\n\n\nplot_posterior_true(some_nonzeros, one_indiv)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.5 seconds.\n\n\n\n\n\n\n\n\n\nThere’s already some bias happening! let’s try what happens when the average is lower (and gives more 0s)\n\nset.seed(420)\nsimulate_some_birds_nonzero(log_avgclutch = log(2.4),\n                            log_b_date = log(.7),\n                            nbirds = 300) |&gt; \n  plot_posterior_true(one_indiv)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.6 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n\n\n\n\n\n\n\n\n\nsome preliminary repetitions show that it usually misses either the average or the hatching success, frequently both.\n\none_indiv_noZero &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv_noZero.stan\"))\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = one_indiv_noZero_model_namespace::one_indiv_noZero_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-2870114b1a0.hpp:363: note:   by 'one_indiv_noZero_model_namespace::one_indiv_noZero_model::write_array'\n  363 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = one_indiv_noZero_model_namespace::one_indiv_noZero_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-2870114b1a0.hpp:363: note:   by 'one_indiv_noZero_model_namespace::one_indiv_noZero_model::write_array'\n  363 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\none_indiv_noZero\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n}\nmodel {\n  vector[nbirds] alpha = log_avgclutch + log_b_date * darrive;\n  success ~ binomial_logit(clutch, logit_psuccess);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n  clutch ~ poisson_log(alpha);\n  // no zeros -- this normalizes the poisson density for a 0-truncated variable\n  target += -log1m_exp(-exp(alpha));\n}\n\n\n\nset.seed(420)\nsimulate_some_birds_nonzero(log_avgclutch = log(4.4),\n                            log_b_date = log(.9),\n                            nbirds = 300) |&gt; \n  plot_posterior_true(one_indiv_noZero)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.5 seconds.\nChain 2 finished in 1.5 seconds.\nChain 3 finished in 1.6 seconds.\nChain 4 finished in 1.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.5 seconds.\nTotal execution time: 1.7 seconds.\n\n\n\n\n\n\n\n\n\n\nTruncating using a different syntax\nThe manual uses a different syntax. To write the equation above this way, I found I needed to make a few changes:\n\npoisson_log has to be replaced with poisson and the exp() link function\n\n… that was actually the only change. It runs at the same speed as the previous way of writing it, and gets the same answer:\n\none_indiv_zerotrunc &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv_zerotrunc.stan\"))\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = one_indiv_zerotrunc_model_namespace::one_indiv_zerotrunc_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-287024f02b4b.hpp:369: note:   by 'one_indiv_zerotrunc_model_namespace::one_indiv_zerotrunc_model::write_array'\n  369 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = one_indiv_zerotrunc_model_namespace::one_indiv_zerotrunc_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-287024f02b4b.hpp:369: note:   by 'one_indiv_zerotrunc_model_namespace::one_indiv_zerotrunc_model::write_array'\n  369 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\none_indiv_zerotrunc\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n}\nmodel {\n  success ~ binomial_logit(clutch, logit_psuccess);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n  vector[nbirds] alpha = log_avgclutch + log_b_date * darrive;\n  clutch ~ poisson(exp(alpha)) T[1,];\n}\n\n\n\nset.seed(420)\nsimulate_some_birds_nonzero(log_avgclutch = log(4.4),\n                            log_b_date = log(.9),\n                            nbirds = 300) |&gt; \n  plot_posterior_true(one_indiv_zerotrunc)\n\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 1.9 seconds.\nChain 4 finished in 2.0 seconds.\nChain 1 finished in 2.1 seconds.\nChain 2 finished in 2.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.0 seconds.\nTotal execution time: 2.2 seconds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo parameterizations diverged in a yellow wood\n\n\n\nThese two ways of writing the model both work. Which to choose? Well, I was pleased with myself for manually normalizing the Poisson likelihood in the first model. However the second is clearer to the reader. In the first, it takes two lines of code – not even necessarily next to each other. In the second, the big T for Truncation indicates clearly what is going on. Code is communication; clarity wins.\n\n\n\n\nZero inflated binomial success\nOnce in a while, a nest will just be completely destroyed by, say, a predator. This has nothing to do with anything, probably, and is just a catastrophic Act of Weasel. Let’s imagine that some small proportion of the nests just die:\n\nsimulate_some_birds_nonzero_zeroinflated &lt;- function(nbirds = 57, \n         log_b_date = log(.97),\n         log_avgclutch = log(4.5),\n         logit_psuccess = qlogis(.84),\n         logit_pfail = qlogis(.1)){\n  \n  # simulate arrival dates -- two weeks before and after whatever the average is\n  darrive &lt;- runif(nbirds, min = -14, max = 14) |&gt; round()\n  \n  ## simulate clutch sizes -- decrease  each day\n  clutch &lt;- rpois(nbirds, exp(log_avgclutch + log_b_date*darrive))\n  # drop 0 nests\n  nonzero_clutch &lt;- which(clutch &gt; 0)\n  n_laid &lt;- length(nonzero_clutch)\n  \n  # simulate success -- among birds which laid at least 1 egg, there is a chance of failing completely\n  success_among_nonzero &lt;- rbinom(n_laid,\n                                  size = clutch[nonzero_clutch],\n                                  prob = plogis(logit_psuccess))\n  failed_nests &lt;- rbinom(n_laid, size = 1, prob = plogis(logit_pfail))\n\n  success_zi &lt;- success_among_nonzero * (1 - failed_nests)\n  \n  # success &lt;- rbinom(nbirds, \n  #                   size = clutch,\n  #                   prob = plogis(logit_psuccess))\n  # failed_nests &lt;- rbinom(nbirds, size = 1, prob = plogis(logit_pfail))\n  # \n  # success_zi &lt;- success * (1 - failed_nests)\n  \n  return(list(\n    data_list = list(\n      nbirds = n_laid,\n      darrive = darrive[nonzero_clutch], \n      clutch  =  clutch[nonzero_clutch], \n      success = success_zi#[nonzero_clutch]\n    ),\n    true_values = tribble(\n      ~variable, ~true_value,\n      \"log_b_date\", log_b_date,\n      \"log_avgclutch\", log_avgclutch,\n      \"logit_psuccess\", logit_psuccess,\n      \"logit_pfail\", logit_pfail\n    )\n  ))\n}\n\n\none_indiv_ztrunc_zinf &lt;- cmdstanr::cmdstan_model(\n  here::here(\"posts/2023-02-02-selection-on-plasticity/one_indiv_ztrunc_zinf.stan\"))\n\nIn file included from stan/src/stan/model/model_header.hpp:11:\nstan/src/stan/model/model_base_crtp.hpp:198: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, std::vector&lt;int&gt;&, std::vector&lt;double, std::allocator&lt;double&gt; &gt;&, bool, bool, std::ostream*) const [with M = one_indiv_ztrunc_zinf_model_namespace::one_indiv_ztrunc_zinf_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  198 |   void write_array(boost::ecuyer1988& rng, std::vector&lt;double&gt;& theta,\n      | \n\n\nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-2870469c7a24.hpp:420: note:   by 'one_indiv_ztrunc_zinf_model_namespace::one_indiv_ztrunc_zinf_model::write_array'\n  420 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \nstan/src/stan/model/model_base_crtp.hpp:136: warning: 'void stan::model::model_base_crtp&lt;M&gt;::write_array(boost::random::ecuyer1988&, Eigen::VectorXd&, Eigen::VectorXd&, bool, bool, std::ostream*) const [with M = one_indiv_ztrunc_zinf_model_namespace::one_indiv_ztrunc_zinf_model; boost::random::ecuyer1988 = boost::random::additive_combine_engine&lt;boost::random::linear_congruential_engine&lt;unsigned int, 40014, 0, 2147483563&gt;, boost::random::linear_congruential_engine&lt;unsigned int, 40692, 0, 2147483399&gt; &gt;; Eigen::VectorXd = Eigen::Matrix&lt;double, -1, 1&gt;; std::ostream = std::basic_ostream&lt;char&gt;]' was hidden [-Woverloaded-virtual=]\n  136 |   void write_array(boost::ecuyer1988& rng, Eigen::VectorXd& theta,\n      | \nC:/Users/UTILIS~1/AppData/Local/Temp/RtmpAv8s8z/model-2870469c7a24.hpp:420: note:   by 'one_indiv_ztrunc_zinf_model_namespace::one_indiv_ztrunc_zinf_model::write_array'\n  420 |   write_array(RNG& base_rng, std::vector&lt;double&gt;& params_r, std::vector&lt;int&gt;&\n      | \n\none_indiv_ztrunc_zinf\n\ndata{\n  int nbirds;\n  vector[nbirds] darrive;\n  array[nbirds] int clutch;\n  array[nbirds] int success;\n}\nparameters {\n  real logit_psuccess;\n  real log_avgclutch;\n  real log_b_date;\n  real logit_pfail;\n}\nmodel {\n  logit_pfail ~ normal(-1, .5);\n  logit_psuccess ~ normal(1, .2);\n  log_avgclutch ~ normal(1, .2);\n  log_b_date ~ normal(0, .2);\n\n  // Eggs laid -- at least one\n  vector[nbirds] alpha = log_avgclutch + log_b_date * darrive;\n  clutch ~ poisson(exp(alpha)) T[1,];\n\n  // nestling success\n  for (n in 1:nbirds) {\n    if (success[n] == 0) {\n      target += log_sum_exp(\n        log_inv_logit(logit_pfail),\n        log1m_inv_logit(logit_pfail) + binomial_logit_lpmf(0 | clutch[n], logit_psuccess)\n        );\n    } else {\n      target += log1m_inv_logit(logit_pfail) + binomial_logit_lpmf(success[n] | clutch[n], logit_psuccess);\n    }\n  }\n\n}\n\n\n\nset.seed(477)\nsome_zi_birds &lt;- simulate_some_birds_nonzero_zeroinflated(log_avgclutch = log(4.4),\n                            log_b_date = log(.9),\n                            nbirds = 300) \n\nsome_zi_birds|&gt; \n  plot_posterior_true(one_indiv_ztrunc_zinf)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 5.0 seconds.\nChain 4 finished in 5.1 seconds.\nChain 2 finished in 5.3 seconds.\nChain 3 finished in 5.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 5.2 seconds.\nTotal execution time: 5.6 seconds.\n\n\n\n\n\n\n\n\n\nI initially failed to recover the parameter for logit_pfail. Here I some things I learned:\n\nSimulating zero-inflated numbers can be tricky! I flipped back and forth between simulating 0-inflation for all nests, and simulating for only those with at least 1 egg. In retrospect, it is clear that these are equivalent. There are two independent things here: the probability of a clutch having 0 eggs and the probability of a 0 coming from the zero-inflated binomial.\nthis zero-inflated model is quite sensitive to the prior. That’s because there are not very many zeros being inflated: in my simulation at least, failed nests are rare and success is naturally low. it would be pretty important to decide in advance if sudden nest failure (e.g. by predation) is rare or common"
  },
  {
    "objectID": "posts/2024-09-27-phylo/index.html",
    "href": "posts/2024-09-27-phylo/index.html",
    "title": "Phylogeny",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nI’ve always wanted to learn more about phylogenetic regressions, and thanks to my colleauge Alex Fuster I recently had the opportunity to sit down and work on them. The literature on the topic is confusing, large, and not always clear about what model is being fit. I relied heavily on two resources:"
  },
  {
    "objectID": "posts/2024-09-27-phylo/index.html#why-a-phylogenetic-regression",
    "href": "posts/2024-09-27-phylo/index.html#why-a-phylogenetic-regression",
    "title": "Phylogeny",
    "section": "Why a phylogenetic regression?",
    "text": "Why a phylogenetic regression?\nSuppose you have two traits, measured across many different species – say, social group size (Trait X) and brain size (Trait Y). You want to test the hypothesis that bigger social groups mean a bigger brain. However there’s a catch: some of the species are closely related, and others are not. Its entirely possible that any apparent correlation between Trait X and Trait Y comes from random chance: both traits change randomly along evolutionary time. That means that distantly related species have more time to become different to each other, and close relatives have less “time apart” and are therefore less likely to be different in their two traits.\nBecause every kind of cross-species comparison involves a group of species with a phylogenetic structure, “controlling for phylogeny” has become very common in these kinds of studies. Also, because we are usually missing traits for at least some species in our studies, people often use phylogeny as a guide for guessing what trait values are present in the animals that we haven’t measured."
  },
  {
    "objectID": "posts/2024-09-27-phylo/index.html#recipe-for-phylogeny",
    "href": "posts/2024-09-27-phylo/index.html#recipe-for-phylogeny",
    "title": "Phylogeny",
    "section": "Recipe for phylogeny",
    "text": "Recipe for phylogeny\nI love the large and flexible toolbox of Bayesian methods because it can be adapted to fit such a huge array of models – virtually all the models that ecologists want to fit! However, there’s a catch: to fit a model using Stan (or something similar) you have to know exactly what model you’re fitting. However, because these regressions are usually fit using custom software, it can be a challenge to dig to find the exact equations being fit!\nUsing the two resources mentioned above, I was able to write down (I hope!) the equation for the model like this:\n\\[\n\\begin{align}\ny_i &= \\bar{y} + \\beta_1 x_i + a_{s[i]} \\\\\na_{s} &\\sim \\text{MVNormal}(0, \\Sigma_a)\\\\\n\\Sigma_a &= \\begin{bmatrix}\n\\sigma_a^2 & \\lambda_a \\cdot \\sigma_{12} & \\cdots & \\lambda_a \\cdot \\sigma_{1,s} \\\\\n\\lambda_a \\cdot \\sigma_{21} & \\sigma_a^2 & \\cdots & \\lambda_a \\cdot \\sigma_{2,s} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\lambda_a \\cdot \\sigma_{s,1} & \\lambda_a \\cdot \\sigma_{s,2} & \\cdots & \\sigma_a^2\n\\end{bmatrix} \\\\\nx_i &= \\bar{x} + b_{s[i]} \\\\\nb_{s} &\\sim \\text{MVNormal}(0, \\Sigma_b)\\\\\n\\Sigma_b &= \\begin{bmatrix}\n\\sigma_b^2 & \\lambda_b \\cdot \\sigma_{12} & \\cdots & \\lambda_b \\cdot \\sigma_{1,s} \\\\\n\\lambda_b \\cdot \\sigma_{21} & \\sigma_b^2 & \\cdots & \\lambda_b \\cdot \\sigma_{2,s} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\lambda_b \\cdot \\sigma_{s,1} & \\lambda_b \\cdot \\sigma_{s,2} & \\cdots & \\sigma_b^2\n\\end{bmatrix}\n\\end{align}\n\\tag{1}\\]\n\n\n\n\n\n\nNote\n\n\n\nYou can see that there is no likelihood for the \\(y_i\\) and \\(x_i\\) values. That’s because I’m starting from a simple case where we know the true values for each species. The only thing to estimate is how variable these traits are among species, and how much of that variation correlates with phylogeny. Later I’ll show an example that is closer to real life.\n\n\nYou can see that there are two big variance-covariance matrices here, for the effects of phylogeny on \\(y\\) and \\(x\\). These covariance matrices have three ingredients that are all put together:\n\nthe base How far apart are species on the phylogeny? Many ecologists work with trees where all the tips end at the present day – so all species have the same amount of time going back to their last common ancestor. For trees like this, the diagonal is 1 (i.e., 100% of the evolutionary time). The off-diagonals are the proportion of this total time which is shared between species.\nthe flavour This is a model of species averages. If there were no effect of phylogeny at all, we would still expect species to be a little different. But how different are species from each other? That is controlled by a standard deviation, \\(\\sigma\\), which we multiply the whole matrix by to scale it.\nthe secret sauce The off-diagnal elements of \\(\\Sigma\\) are multiplied by another number between 0 and 1: this is “Pagel’s Lambda”. It acts like a tuning knob, adjusting the amount of phylogenetic flavour that makes it into the model. When \\(\\lambda\\) is 1, we have the maximum amount of covariance coming from the phylogeny. When \\(\\lambda\\) is 0, we are back to an identity matrix and the species are independent.\n\nThere’s another way to write this equation that makes these three parts more clear to see. First we have to make \\(V_{phy}\\), which is the phylogenetic variance-covariance matrix. This has variances and covariances for each species on our tree. For example, for 3 species the phylogenetic variance covariance matrix is:\n\\[\nV_{phy} = \\begin{bmatrix}\n\\sigma_1^2 & \\sigma_{12} & \\sigma_{1,3} \\\\\n\\sigma_{2,1} & \\sigma_2^2 & \\sigma_{2,3} \\\\\n\\sigma_{3,1} & \\sigma_{3,2} & \\sigma_3^2\n\\end{bmatrix}\n\\] The covariances are equal to the proportion of the tree that is shared between two species. The diagonal is the amount of time between the tree’s start and each species. This means that, for a tree where all the tips end at the present day, the diagonal is 1 and the off-diagonal is between 0 and 1.\nThen, we can write the expression for \\(\\Sigma\\) like this:\n\\[\n\\Sigma = \\sigma^2 \\lambda V_{phy} + \\sigma^2 (1 - \\lambda) \\mathbf{I}\n\\] This is equation 4 in Pearse, Davies, and Wolkovich (n.d.).\nI can rewrite Equation 1 in this style:\n\\[\n\\begin{align}\ny_i &= \\bar{y} + \\beta_1 x_i + a_{s[i]} \\\\\na_{s} &\\sim \\text{MVNormal}(0, \\Sigma_a)\\\\\n\\Sigma_a &= \\sigma_a^2 \\lambda_a V_{phy} + \\sigma_a^2 (1 - \\lambda_a) \\mathbf{I} \\\\\nx_i &= \\bar{x} + b_{s[i]} \\\\\nb_{s} &\\sim \\text{MVNormal}(0, \\Sigma_b)\\\\\n\\Sigma_b &= \\sigma_b^2 \\lambda_b V_{phy} + \\sigma_b^2 (1 - \\lambda_b) \\mathbf{I} \\\\\n\\end{align}\n\\tag{2}\\]\n\\[\n\\begin{align}\ny_i &\\sim \\text{Normal}(\\bar{y} + \\beta_1 x_i + a_{s[i]}, \\sigma_y) \\\\\na_{s} &\\sim \\text{MVNormal}(0, \\Sigma_a)\\\\\n\\Sigma_a &= \\sigma_a^2 \\lambda_a V_{phy} + \\sigma_a^2 (1 - \\lambda_a) \\mathbf{I} \\\\\nx_i &\\sim \\text{Normal}(\\bar{x} + b_{s[i]}, \\sigma_x) \\\\\nb_{s} &\\sim \\text{MVNormal}(0, \\Sigma_b)\\\\\n\\Sigma_b &= \\sigma_b^2 \\lambda_b V_{phy} + \\sigma_b^2 (1 - \\lambda_b) \\mathbf{I} \\\\\n\\end{align}\n\\tag{3}\\]\nYou can see I’m using two different trait variances (\\(\\sigma_a\\) and \\(\\sigma_b\\)) and two different amounts of phylogenetic signal (\\(\\lambda_a\\) and \\(\\lambda_b\\)), one for each trait."
  },
  {
    "objectID": "posts/2024-09-27-phylo/index.html#data-simulation",
    "href": "posts/2024-09-27-phylo/index.html#data-simulation",
    "title": "Phylogeny",
    "section": "Data simulation",
    "text": "Data simulation\nHere is simulation code from Ives (n.d.) , which generates a dataset where there is a signal for phylogeny and also a relationship between two traits of interest. I’ll use this code to generate a dataset and then estimate the known parameters with a Stan model:\n## simulate data\nset.seed(1618)\nn &lt;- 20\nb0 &lt;- 0\nb1 &lt;- 0\nlambda_x &lt;- .98\nlambda_y &lt;- .8\nsigma_y &lt;- .2\nsigma_x &lt;- .2\n\nphy &lt;- ape::compute.brlen(\n  ape::rtree(n=n),\n  method = \"Grafen\",\n  power = 1)\n\nplot(phy)\nphy.x &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lambda_x))$tree\n\nphy.e &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lambda_y))$tree\n\nx &lt;- ape::rTraitCont(phy.x, model = \"BM\", sigma = sigma_x)\ne &lt;- ape::rTraitCont(phy.e, model = \"BM\", sigma = sigma_y)\nx &lt;- x[match(names(e), names(x))]\nY &lt;- b0 + b1 * x + e\nY &lt;- array(Y)\nrownames(Y) &lt;- phy$tip.label\n\nplot(x, Y)\n\n\n\n\n\n\na simulated phylogeny\n\n\n\n\n\n\n\nsimulated data, with both a phylogenetic signal and a causal relationship between trait X and trait Y.\n\n\n\n\n\nHere’s a simple Stan program which fits the model in Equation 3 to these simulated data.\n\nphylo &lt;- cmdstanr::cmdstan_model(here::here(\"posts/2024-09-27-phylo/phylo.stan\"))\n\nphylo\n\ndata {\n  int n;\n  int s;\n  vector[n] x;\n  vector[n] y;\n  matrix[s, s] phyvcv;\n}\nparameters {\n  real b0;\n  real b1;\n  real sigma_x;\n  real sigma_y;\n  real logit_lambda_x;\n  real logit_lambda_y;\n}\ntransformed parameters {\n  real&lt;lower=0,upper=1&gt; lambda_x;\n  lambda_x = inv_logit(logit_lambda_x);\n  // y\n  real&lt;lower=0,upper=1&gt; lambda_y;\n  lambda_y = inv_logit(logit_lambda_y);\n}\nmodel {\n  b0 ~ std_normal();\n  b1 ~ normal(.5, .5);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  logit_lambda_x ~ normal(3, .2);\n  logit_lambda_y ~ normal(0, .2);\n\n  matrix[s, s] vcv_x;\n  vcv_x = add_diag(sigma_x^2*lambda_x*phyvcv, sigma_x^2*(1 - lambda_x));\n\n\n  matrix[s, s] vcv_y;\n  vcv_y = add_diag(sigma_y^2*lambda_y*phyvcv, sigma_y^2*(1 - lambda_y));\n\n\n  x ~ multi_normal(rep_vector(0, n), vcv_x);\n  y ~ multi_normal(b0 + b1*x, vcv_y);\n}\n\n\nNow we’ll sample the model and plot the posterior distribution of some parameters against the truth:\n\nphylo_sample &lt;- phylo$sample(data = list(\n  n = n,\n  s = n,\n  x = x,\n  y = Y,\n  phyvcv = ape::vcv(phy)\n),parallel_chains = 4, refresh = 1000)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \n\n\nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \n\n\nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \n\n\nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 finished in 0.3 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.5 seconds.\n\n\n\nmake_rvar_df &lt;- function(post_draws){\n  post_draws |&gt; \n    posterior::as_draws_rvars() |&gt; \n    # list any parameter that isn't a scalar\n    map_if(\\(x) length(x)&gt;1, list) |&gt; \n    tibble::as_tibble()\n}\n\n\nplot_true_post &lt;- function(truth_df, post_draws_df){\n\n  true_post_df &lt;- truth_df |&gt; \n    left_join(post_draws_df, by = \"name\")\n  \n  true_post_df |&gt; \n    ggplot(aes(y = name, dist = posterior))+ \n    tidybayes::stat_dist_slab() + \n    geom_vline(aes(xintercept = value)) + \n    facet_wrap(~name, scales=\"free\")\n}\n\n\ntruth &lt;- data.frame(sigma_x, sigma_y, b0, \n           b1, lambda_x, lambda_y) |&gt; \n  pivot_longer(cols = everything())\n\n\nposterior_dist_long &lt;- make_rvar_df(phylo_sample) |&gt; \n  select(b0:lambda_y) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth, post_draws_df = posterior_dist_long)\n\n\n\n\n\n\n\n\nWe can see that, at least for these values, parameter recovery isn’t bad, especially for the coefficients \\(\\beta_0\\) and \\(\\beta_1\\). However, at least in this simulation, the parameters describing the phylogenetic signal are all underestimated."
  },
  {
    "objectID": "posts/2024-09-27-phylo/index.html#tips-from-the-forum",
    "href": "posts/2024-09-27-phylo/index.html#tips-from-the-forum",
    "title": "Phylogeny",
    "section": "Tips from the forum",
    "text": "Tips from the forum\nI posted about this model in the Stan Discourse forum and I had the good luck to get feedback from Bob Carpenter! Here is the model after including those suggested changes:\n\nphylo_forum &lt;- cmdstanr::cmdstan_model(here::here(\"posts/2024-09-27-phylo/phylo_forum.stan\"))\n\nphylo_forum\n\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0&gt; s;\n  vector[n] x;\n  vector[n] y;\n  cov_matrix[s] phyvcv;\n}\ntransformed data {\n  vector[n] zero_vec = rep_vector(0, n);\n}\nparameters {\n  real b0;\n  real&lt;offset=0.5, multiplier=0.5&gt; b1;\n  real&lt;lower=0&gt; sigma_x;\n  real&lt;lower=0&gt; sigma_y;\n  real&lt;offset=3, multiplier=0.2&gt; logit_lambda_x;\n  real&lt;multiplier=0.2&gt; logit_lambda_y;\n}\ntransformed parameters {\n  real&lt;lower=0, upper=1&gt; lambda_x = inv_logit(logit_lambda_x);\n  real&lt;lower=0, upper=1&gt; lambda_y = inv_logit(logit_lambda_y);\n}\nmodel {\n  b0 ~ std_normal();\n  b1 ~ normal(0.5, 0.5);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  logit_lambda_x ~ normal(3, .2);\n  logit_lambda_y ~ normal(0, .2);\n\n  matrix[s, s] vcv_x\n    = sigma_x^2 * add_diag(lambda_x * phyvcv, 1 - lambda_x);\n  matrix[s, s] vcv_y\n    = sigma_y^2 * add_diag(lambda_y * phyvcv, 1 - lambda_y);\n  x ~ multi_normal(zero_vec, vcv_x);\n  y ~ multi_normal(b0 + b1 * x, vcv_y);\n}\n\n\n\nphylo_forum_sample &lt;- phylo_forum$sample(data = list(\n  n = n,\n  s = n,\n  x = x,\n  y = Y,\n  phyvcv = ape::vcv(phy)\n),parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.5 seconds.\n\ntruth &lt;- data.frame(sigma_x, sigma_y, b0, \n           b1, lambda_x, lambda_y) |&gt; \n  pivot_longer(cols = everything())\n\n\nphylo_forum_sample_long &lt;- make_rvar_df(phylo_forum_sample) |&gt; \n  select(b0:lambda_y) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth, post_draws_df = phylo_forum_sample_long)\n\n\n\n\n\n\n\n\nWe get get pretty similar results to the above!\nand an even simpler strategy, replacing the lambda parameter on the logit scale with a beta:\n\nphylo_beta &lt;- cmdstanr::cmdstan_model(here::here(\"posts/2024-09-27-phylo/phylo_beta.stan\"))\n\nphylo_beta\n\ndata {\n  int&lt;lower=0&gt; n;\n  int&lt;lower=0&gt; s;\n  vector[n] x;\n  vector[n] y;\n  cov_matrix[s] phyvcv;\n}\ntransformed data {\n  vector[n] zero_vec = rep_vector(0, n);\n}\nparameters {\n  real b0;\n  real&lt;offset=0.5, multiplier=0.5&gt; b1;\n  real&lt;lower=0&gt; sigma_x;\n  real&lt;lower=0&gt; sigma_y;\n  real&lt;lower=0,upper=1&gt; lambda_x;\n  real&lt;lower=0,upper=1&gt; lambda_y;\n}\nmodel {\n  b0 ~ std_normal();\n  b1 ~ normal(0.5, 0.5);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  lambda_x ~ beta(9, 1);\n  lambda_y ~ beta(5, 5);\n\n  matrix[s, s] vcv_x\n    = sigma_x^2 * add_diag(lambda_x * phyvcv, 1 - lambda_x);\n  matrix[s, s] vcv_y\n    = sigma_y^2 * add_diag(lambda_y * phyvcv, 1 - lambda_y);\n  x ~ multi_normal(zero_vec, vcv_x);\n  y ~ multi_normal(b0 + b1 * x, vcv_y);\n}\n\n\n\nphylo_beta_sample &lt;- phylo_beta$sample(data = list(\n  n = n,\n  s = n,\n  x = x,\n  y = Y,\n  phyvcv = ape::vcv(phy)\n),parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 2 finished in 0.5 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.6 seconds.\n\nphylo_beta_sample\n\n variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n lp__     48.08  48.42 1.84 1.68 44.56 50.43 1.00     1586     2164\n b0        0.08   0.09 0.07 0.07 -0.04  0.20 1.00     4003     2241\n b1        0.07   0.06 0.23 0.22 -0.30  0.45 1.00     4309     3115\n sigma_x   0.19   0.19 0.04 0.04  0.14  0.27 1.00     3146     2817\n sigma_y   0.16   0.16 0.03 0.03  0.12  0.22 1.00     3296     3381\n lambda_x  0.92   0.93 0.06 0.06  0.80  0.99 1.00     3008     1832\n lambda_y  0.54   0.55 0.13 0.14  0.32  0.75 1.00     3309     2876\n\n\n\nphylo_beta_sample_long &lt;- make_rvar_df(phylo_beta_sample) |&gt; \n  select(b0:lambda_y) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth, post_draws_df = phylo_beta_sample_long)"
  },
  {
    "objectID": "posts/2024-09-27-phylo/index.html#repeated-sampling-of-these-traits",
    "href": "posts/2024-09-27-phylo/index.html#repeated-sampling-of-these-traits",
    "title": "Phylogeny",
    "section": "Repeated sampling of these traits",
    "text": "Repeated sampling of these traits\nThe simulation above is giving species means. However in our study we have more than one measurement per species. Measurements of “Trait X” and “Trait Y” are measured on different individuals. In fact, are coming from two completely different datasets! Of course, in the real-world application there will be all kinds of quirky differences between the two datasets: different amounts of effort per species and different species measured in each dataset.\n\nlibrary(tidyverse)\nsuppressPackageStartupMessages(library(ape))\n\nset.seed(1618)\n\n# set true parameter values\nn &lt;- 20\nb0_x &lt;- 4\nb0_y &lt;- .5\nb_xy &lt;- -.1\nlam.x &lt;- .98\nlam.e &lt;- .5\nsigma_x &lt;- .4\nsigma_y &lt;- .3\n\n\n\n# simulate phylogeny\nphy &lt;- ape::compute.brlen(\n  ape::rtree(n=n),\n  method = \"Grafen\",\n  power = 1.5)\n\nplot(phy)\n\n\n\n\n\n\n\n# get names from this matrix! needs to line up perfectly\nphyvcv &lt;- ape::vcv(phy)\n\ndistmat_names &lt;- dimnames(phyvcv)[[1]]\n\n# observations per species\nn_obs &lt;- 15\n\n\nphy.x &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lam.x))$tree\n\nphy.e &lt;- phylolm::transf.branch.lengths(\n  phy=phy, model=\"lambda\",\n  parameters=list(lambda = lam.e))$tree\n\nx &lt;- ape::rTraitCont(phy.x, model = \"BM\", sigma = sigma_x)\ne &lt;- ape::rTraitCont(phy.e, model = \"BM\", sigma = sigma_y)\nx &lt;- x[match(names(e), names(x))]\n\n## calculate Y\nY &lt;- b0_y + b_xy * x + e\n## calculate X\nX &lt;- b0_x + x\n\n# Y &lt;- array(Y)\nnames(Y) &lt;- phy$tip.label\n\nplot(X, Y)\n\n\n\n\n\n\n\nobs_xy_df &lt;- tibble(X, Y, sp_name = names(x)) |&gt; \n  mutate(\n    sp_id = as.numeric(\n      factor(sp_name, \n             levels = distmat_names))) |&gt; \n  rowwise() |&gt; \n  mutate(obs_x = list(\n    rnorm(n_obs, mean = X, sd = .3)),\n    obs_y = list(rnorm(n_obs, mean = Y, sd = .3)))\n\n\nx_obs_df &lt;- obs_xy_df |&gt; \n  select(sp_id, obs_x) |&gt; unnest(obs_x)\n\n\ny_obs_df &lt;- obs_xy_df |&gt; \n  select(sp_id, obs_y) |&gt; unnest(obs_y)\n\nfit a model that is ready for replication per species:\n\nphylo_obs_cen &lt;- cmdstanr::cmdstan_model(here::here(\"posts/2024-09-27-phylo/phylo_obs_cen.stan\"))\n\nphylo_obs_cen\n\ndata {\n  int&lt;lower=0&gt; s;\n  // x trait\n  int&lt;lower=0&gt; n_x;\n  vector[n_x] x_obs;\n  array[n_x] int&lt;lower=1,upper=s&gt; sp_id_x;\n  // y trait\n  int&lt;lower=0&gt; n_y;\n  vector[n_y] y_obs;\n  array[n_y] int&lt;lower=1,upper=s&gt; sp_id_y;\n  cov_matrix[s] phyvcv;\n}\ntransformed data {\n  vector[s] zero_vec = rep_vector(0, s);\n}\nparameters {\n  real&lt;offset=2,multiplier=2&gt; b0_x;\n  real&lt;offset=.5,multiplier=.8&gt; b0_y;\n  real&lt;offset=0.5, multiplier=0.5&gt; b_xy;\n  real&lt;lower=0&gt; sigma_x;\n  real&lt;lower=0&gt; sigma_y;\n  real&lt;lower=0, upper=1&gt; lambda_x;\n  real&lt;lower=0, upper=1&gt; lambda_y;\n  vector[s] x_avg;\n  vector[s] y_avg;\n  real&lt;lower=0&gt; sigma_x_obs;\n  real&lt;lower=0&gt; sigma_y_obs;\n}\nmodel {\n  b0_x ~ normal(2, 2);\n  b0_y ~ normal(.5, .8);\n  b_xy ~ normal(0, .2);\n  sigma_x ~ exponential(1);\n  sigma_y ~ exponential(1);\n  lambda_x ~ beta(9, 1);\n  lambda_y ~ beta(5, 5);\n\n  matrix[s, s] vcv_x\n    = sigma_x^2 * add_diag(lambda_x * phyvcv, 1 - lambda_x);\n  matrix[s, s] vcv_y\n    = sigma_y^2 * add_diag(lambda_y * phyvcv, 1 - lambda_y);\n\n  sigma_x_obs ~ exponential(1);\n  sigma_y_obs ~ exponential(1);\n  // species averages\n  x_avg ~ multi_normal(zero_vec, vcv_x);\n  y_avg ~ multi_normal(b_xy * x_avg, vcv_y);\n  // observations of these\n  x_obs ~ normal(b0_x + x_avg[sp_id_x], sigma_x_obs);\n  y_obs ~ normal(b0_y + y_avg[sp_id_y], sigma_y_obs);\n}\n\n\nSampling the model – this produces some warnings that are safe to ignore at this point.\n\nphylo_obs_cen_samp &lt;- phylo_obs_cen$sample(data = list(\n  s = n,\n  # trait x\n  n_x = nrow(x_obs_df),\n  x_obs = x_obs_df$obs_x,\n  sp_id_x = x_obs_df$sp_id,\n  # trait y\n  n_y = nrow(y_obs_df),\n  y_obs = y_obs_df$obs_y,\n  sp_id_y = y_obs_df$sp_id,\n  # phylogeny\n  phyvcv = phyvcv\n),parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 finished in 3.1 seconds.\nChain 3 finished in 3.1 seconds.\nChain 2 finished in 3.4 seconds.\nChain 4 finished in 3.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.3 seconds.\nTotal execution time: 3.6 seconds.\n\nphylo_obs_cen_samp$summary(variables = c(\n  \"b0_x\", \"b0_y\", \"b_xy\", \"sigma_x\", \"sigma_y\", \"lambda_x\", \"lambda_y\", \"sigma_x_obs\", \"sigma_y_obs\"\n))\n\n# A tibble: 9 × 10\n  variable       mean  median     sd    mad     q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b0_x         4.07    4.08   0.210  0.206   3.71  4.40   1.01     325.     412.\n2 b0_y         0.620   0.624  0.138  0.131   0.384 0.834  1.01     340.     458.\n3 b_xy        -0.0456 -0.0454 0.161  0.157  -0.312 0.216  1.00    2158.    2701.\n4 sigma_x      0.368   0.355  0.0861 0.0802  0.249 0.529  1.00    1610.    2143.\n5 sigma_y      0.293   0.285  0.0665 0.0636  0.202 0.414  1.00    1150.    2272.\n6 lambda_x     0.975   0.984  0.0275 0.0170  0.922 0.999  1.00    1866.    1631.\n7 lambda_y     0.470   0.470  0.140  0.148   0.238 0.700  1.00    1688.    2455.\n8 sigma_x_obs  0.288   0.288  0.0122 0.0122  0.269 0.309  1.00    2010.    2422.\n9 sigma_y_obs  0.313   0.313  0.0130 0.0126  0.293 0.336  1.00    2201.    2530.\n\n\n\ntruth_df &lt;- tribble(\n  ~name, ~value,\n  \"b0_x\", b0_x,\n  \"b0_y\", b0_y,\n  \"b_xy\", b_xy,\n  \"sigma_x\", sigma_x,\n  \"sigma_y\", sigma_y,\n  \"lambda_x\", lambda_x,\n  \"lambda_y\", lambda_y\n)\n\nphylo_obs_cen_samp_long &lt;- make_rvar_df(phylo_obs_cen_samp) |&gt; \n  select(-x_avg, -y_avg) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth_df = truth_df, \n               post_draws_df = phylo_obs_cen_samp_long)\n\n\n\n\n\n\n\n\nspecies averages\n\nrvar_list &lt;- phylo_obs_cen_samp |&gt; posterior::as_draws_rvars() |&gt; \n  pluck(\"b0_x\")\n\n\nx_avg_post_long &lt;- make_rvar_df(phylo_obs_cen_samp) |&gt; \n  # calculate averages\n  select(x_avg) |&gt; \n  unnest(x_avg) |&gt; \n  rownames_to_column(var = \"sp_id\") |&gt; \n  mutate(sp_id = readr::parse_number(sp_id),\n         x_total_avg = rvar_list + x_avg )\n\n  \n  \nobs_xy_df |&gt; \n  left_join(x_avg_post_long) |&gt; \n  ggplot(aes(x = sp_name, dist = x_total_avg))+ \n  tidybayes::stat_dist_slab() + \n    geom_point(aes(x = sp_name, y = X))\n\nJoining with `by = join_by(sp_id)`\n\n\n\n\n\n\n\n\n\n\nMissing data\nMany people use phylogenetic information to help when a dataset is missing a lot of traits.\nHere I’m using the same model as above but imagining that a few species are never measured for trait X, but are measured for trait y. There’s also phylogenetic information on both traits.\nNotice that there’s no need to rewrite the model for this! all I need to do is take out some observations from the dataset:\n\n# remove some from the output\n\nabsent_sp &lt;- sample(x_obs_df$sp_id |&gt; unique(), size = 7, replace = FALSE)\n  \nx_obs_NA_df &lt;- x_obs_df |&gt; \n  filter(!(sp_id %in% absent_sp))\n\n\nphylo_obs_NA_samp &lt;- phylo_obs_cen$sample(data = list(\n  s = n,\n  # trait x\n  n_x = nrow(x_obs_NA_df),\n  x_obs = x_obs_NA_df$obs_x,\n  sp_id_x = x_obs_NA_df$sp_id,\n  # trait y\n  n_y = nrow(y_obs_df),\n  y_obs = y_obs_df$obs_y,\n  sp_id_y = y_obs_df$sp_id,\n  # phylogeny\n  phyvcv = phyvcv\n),parallel_chains = 4, refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 2 finished in 3.2 seconds.\nChain 4 finished in 3.8 seconds.\nChain 1 finished in 3.8 seconds.\nChain 3 finished in 3.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.7 seconds.\nTotal execution time: 3.9 seconds.\n\nrvar_list &lt;- phylo_obs_NA_samp |&gt; \n  posterior::as_draws_rvars() |&gt; \n  pluck(\"b0_x\")\n\n\nx_avg_post_long &lt;- make_rvar_df(phylo_obs_NA_samp) |&gt; \n  # calculate averages\n  select(x_avg) |&gt; \n  unnest(x_avg) |&gt; \n  rownames_to_column(var = \"sp_id\") |&gt; \n  mutate(sp_id = readr::parse_number(sp_id),\n         x_total_avg = rvar_list + x_avg )\n\n  \n  \nobs_xy_df |&gt; \n  left_join(x_avg_post_long) |&gt; \n  mutate(absent = sp_id %in% absent_sp) |&gt; \n  ggplot(aes(x = sp_name, dist = x_total_avg))+ \n  tidybayes::stat_dist_slab() + \n    geom_point(aes(x = sp_name, y = X, col = absent))\n\n\n\n\n\n\n\n## scalar parameters\nphylo_obs_NA_samp_long &lt;- make_rvar_df(phylo_obs_NA_samp) |&gt; \n  select(-x_avg, -y_avg) |&gt; \n  pivot_longer(cols = everything(), values_to = \"posterior\")\n\nplot_true_post(truth_df = truth_df, \n               post_draws_df = phylo_obs_NA_samp_long)\n\n\n\n\n\n\n\n\nThe model estimates latent parameters for species averages, which are then measured with error. This makes it easy to model unmeasured values. In Bayesian inference, unmeasured quantities are all treated the same, and called “parameters”. So here, we’re modelling all species averages as latent parameters, and saying that most, but not all, actually get measured. The result is posterior samples, not only for slopes and other values of interest, but also for the unmeasured species averages.\nYou can see that the distributions are much flatter for these unmeasured species averages, compared to those that were measured. However, you can also see that the unmeasured averages are moving around, influenced by information coming from Pagel’s Lambda and the other parameters of the model as well."
  },
  {
    "objectID": "posts/2023-11-01-standalone-gq/index.html",
    "href": "posts/2023-11-01-standalone-gq/index.html",
    "title": "Stan-dalone generated quantites",
    "section": "",
    "text": "library(cmdstanr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)"
  },
  {
    "objectID": "posts/2023-11-01-standalone-gq/index.html#breaking-apart-stan-programs",
    "href": "posts/2023-11-01-standalone-gq/index.html#breaking-apart-stan-programs",
    "title": "Stan-dalone generated quantites",
    "section": "Breaking apart Stan programs",
    "text": "Breaking apart Stan programs\nSometimes we don’t want the output of a Stan model to become enormous. However, Stan models can be very convenient for calculating generated quantities. Of course this can be done in R, but sometimes it is just easier to have all the outputs presented in the same way.\nSee a description of this in the User’s guide and in the CmdStanR help file"
  },
  {
    "objectID": "posts/2023-11-01-standalone-gq/index.html#example-marginal-effects-in-multiple-regression",
    "href": "posts/2023-11-01-standalone-gq/index.html#example-marginal-effects-in-multiple-regression",
    "title": "Stan-dalone generated quantites",
    "section": "example: Marginal effects in multiple regression",
    "text": "example: Marginal effects in multiple regression\nSuppose there is a plant which, when growing in N-rich soil, is able to generate chemical defenses to prevent damage by a herbivorous insect. On poor soil the herbivore eats much more of the plant\n\nset.seed(4812)\nsoil_quality &lt;- runif(200, min = -3, max = 3)\ninsect_biomass &lt;- runif(200, min = -10, max = 10)\n# each gram of insect biomass eats 1.2 grams of plant biomass\ninsect_eff_per_g &lt;- 2\n\nsoil_quality_eff_per_unit &lt;- 0\n\nsoil_quality_on_herb &lt;- -.5\n\nherb_avg_soil_avg_density &lt;- 33\n\nmu_herbivory &lt;- herb_avg_soil_avg_density + \n  soil_quality_eff_per_unit* soil_quality + \n  (insect_eff_per_g + soil_quality_on_herb*soil_quality) * insect_biomass \n\nsigma_herb &lt;- 5\nobs_herbivory &lt;- rnorm(n = 200, mu_herbivory, sigma_herb)\n\ntibble(soil_quality, insect_biomass, obs_herbivory) |&gt; \n  ggplot(aes(x = soil_quality, y = obs_herbivory, col = insect_biomass)) + \n  geom_point()\n\n\n\n\n\n\n\n\nHere is a Stan program to model this interaction\n\n# class-output: stan\n\nmultiple_regression &lt;- cmdstan_model(\n  here::here(\n    \"posts/2023-11-01-standalone-gq/multiple_regression.stan\"\n    ))\n\nmultiple_regression\n\ndata{\n  int&lt;lower=0&gt; n;\n  vector[n] soil;\n  vector[n] insects;\n  vector[n] herbivory;\n}\nparameters{\n  real avg_herb;\n  vector[3] beta;\n  real&lt;lower=0&gt; sigma;\n}\nmodel{\n  sigma ~ exponential(.25);\n  beta ~ std_normal();\n  avg_herb ~ normal(30, 5);\n  herbivory ~ normal(avg_herb + beta[1]* soil + beta[2]*insects + beta[3]*(soil .* insects), sigma);\n}\n\n\n\nmultiple_post &lt;- multiple_regression$sample(data = list(n = length(soil_quality), soil = soil_quality, insects = insect_biomass, herbivory = obs_herbivory), parallel_chains = 2, refresh = 0)\n\nRunning MCMC with 4 chains, at most 2 in parallel...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.8 seconds.\n\n\nWe can see that the posteriors are close to the true values (not the point of this post, but always good to check)\n\nmultiple_post$summary()\n\n# A tibble: 6 × 10\n  variable     mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;num&gt;    &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -416.    -415.    1.63   1.48   -419.    -414.     1.00    1904.\n2 avg_herb   32.9     32.9   0.336  0.333    32.4     33.5    1.00    3855.\n3 beta[1]     0.192    0.190 0.191  0.192    -0.123    0.507  1.00    4847.\n4 beta[2]     1.93     1.93  0.0607 0.0589    1.83     2.03   1.00    4096.\n5 beta[3]    -0.474   -0.474 0.0341 0.0345   -0.530   -0.418  1.00    4922.\n6 sigma       4.80     4.79  0.238  0.235     4.43     5.20   1.00    4098.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\nNow suppose we want to plot this interaction. We could do so in R, no problem. We could also do that in the model above. But you might not want to! reasons include:\n\nkeeping the output of any one model small(ish) so that you can actually work with it\n\n\n# class-output: stan\n\nmulti_reg_triptych &lt;- cmdstan_model(\n  here::here(\n    \"posts/2023-11-01-standalone-gq/multi_reg_triptych.stan\"\n    ))\n\nmulti_reg_triptych\n\ndata {\n  int&lt;lower=0&gt; npred;\n  vector[npred] new_soil;\n  vector[npred] new_insect;\n}\n// copied from the previous model!\nparameters{\n  real avg_herb;\n  vector[3] beta;\n  real&lt;lower=0&gt; sigma;\n}\ngenerated quantities {\n  vector[npred] pred_herbivory;\n  for (i in 1:npred){\n    pred_herbivory[i] = normal_rng(avg_herb + beta[1]* new_soil[i] + beta[2]*new_insect[i] + beta[3]*(new_soil[i] * new_insect[i]), sigma);\n  }\n}\n\n\nget the prediction data ready\n\nnewdata &lt;- expand_grid(new_insect = c(-5, 0, 5),\n            new_soil = seq(from = -10, to = 10, length.out = 11))\n\n\nmulti_trip &lt;- multi_reg_triptych$generate_quantities(\n  fitted_params = multiple_post,\n  data = list(\n    new_insect = newdata$new_insect,\n    new_soil = newdata$new_soil,\n    npred = nrow(newdata)\n  )\n)\n\nRunning standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 1.0 seconds.\n\n\n\nmulti_trip |&gt; \n  gather_rvars(pred_herbivory[i]) |&gt; \n  bind_cols(newdata) |&gt; \n  ggplot(aes(x = new_soil, dist = .value)) + \n  stat_lineribbon() + \n  facet_wrap(~new_insect) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  labs(x = \"new_soil\", y = \"predicted herbivory\")\n\n\n\n\nVisualizing an interaction. The effect of soil quality on herbivory varies with herbivore biomass (in a very pretend, make-believe example)."
  },
  {
    "objectID": "posts/2022-11-29-selection-on-plasticity/index.html",
    "href": "posts/2022-11-29-selection-on-plasticity/index.html",
    "title": "Selection on plasticity",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\n# source(\"posts/2022-11-29-selection-on-plasticity/functions.R\")\n## functions for measuring selection on plasticity\nsimulate_many_moms &lt;- function(pop_average_dponte = 138,\n                               mom_quality_max = 4,\n                               quality_on_dponte = 2,\n                               quality_on_csize = .2,\n                               n_females = 42,\n                               lifespan = 5,\n                               temp_range  = c(2, 12)) {\n\n  general_temp &lt;- runif(lifespan, temp_range[1], max = temp_range[2])\n\n  general_temp_c &lt;- general_temp - mean(general_temp)\n\n  mom_qualities &lt;- runif(n_females, min = 0, max = 4)\n\n  many_moms_temperature &lt;- expand_grid(year = 1:lifespan,\n                                       idF1 = 1:n_females) |&gt;\n    mutate(mom_quality = mom_qualities[idF1],\n           general_temp = general_temp[year],\n           general_temp_c = general_temp_c[year],\n           ## adding the biology\n           ## Effect of temperature -- does it depend on quality? let's say that it DOES (for now)\n           effet_temp_dponte_qual = -.7*mom_quality,\n           effet_temp_csize_qual = .1*log(mom_quality),\n           # csize\n           mom_avg_csize = log(pop_average_csize) +  quality_on_csize*log(mom_quality),\n           temp_avg_csize = exp(mom_avg_csize + effet_temp_csize_qual*general_temp_c),\n           # dponte\n           mom_avg_dponte = pop_average_dponte + quality_on_dponte*mom_quality,\n           temp_avg_dponte = mom_avg_dponte + effet_temp_dponte_qual*general_temp_c,\n           ## observations\n           obs_csize = rpois(n = length(year), lambda = temp_avg_csize),\n           obs_dponte = rnorm(n = length(year), mean = temp_avg_dponte, sd = 3) |&gt; round()\n    )\n  return(many_moms_temperature)\n}"
  },
  {
    "objectID": "posts/2022-11-29-selection-on-plasticity/index.html#model-for-measuring-selection-directly",
    "href": "posts/2022-11-29-selection-on-plasticity/index.html#model-for-measuring-selection-directly",
    "title": "Selection on plasticity",
    "section": "Model for measuring selection directly:",
    "text": "Model for measuring selection directly:\n\ntrue_corr_plasticity_avg &lt;- .7\nsd_avg &lt;- .9\nsd_plasticity &lt;- .3\nn_females &lt;- 47\n\ncorrmat &lt;- matrix(c(1, true_corr_plasticity_avg, true_corr_plasticity_avg, 1),\n       byrow = TRUE, ncol = 2)\n\nvar_covar &lt;- diag(c(sd_avg, sd_plasticity)) %*% corrmat %*% diag(c(sd_avg, sd_plasticity))\n\nfemale_avg_and_plasticity &lt;- MASS::mvrnorm(\n  n = n_females,\n  mu = c(0,0),\n  Sigma = var_covar)\n\nWe have the average and the slope for each female.\nFor a bit of extra realism, lets simulate several years and let the females belong to different cohorts. for simplicity, lets say each female lives for\n\ntwenty_years_environment &lt;- runif(20, min = -3, max = 3)\n\nfemale_start_years &lt;- sample(1:16, size = n_females, replace = TRUE)\nlibrary(tidyverse)\n\ndf &lt;- tibble(\n  female_id = rep(1:n_females, each = 4),\n  year_id = rep(female_start_years, each = 4) + 0:3,\n  env = twenty_years_environment[year_id]\n)\n\ndf$env |&gt; mean()"
  },
  {
    "objectID": "posts/2023-11-08-tree-cooling/index.html",
    "href": "posts/2023-11-08-tree-cooling/index.html",
    "title": "causal DAGS and full-luxury shade trees",
    "section": "",
    "text": "library(brms)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)\nlibrary(ggdag)"
  },
  {
    "objectID": "posts/2023-11-08-tree-cooling/index.html#the-dag",
    "href": "posts/2023-11-08-tree-cooling/index.html#the-dag",
    "title": "causal DAGS and full-luxury shade trees",
    "section": "The DAG",
    "text": "The DAG\n\n dagified &lt;- dagify(\n    cooling ~ tree_size,\n    cooling ~ tree_diversity,\n    cooling ~ tree_density,\n    tree_size ~ soil + age,\n    tree_diversity ~ soil + age,\n    tree_density ~ past_land_use,\n    soil ~ past_land_use,\n    labels = c(\n      \"cooling\" = \"Cooling\\n Benefit\",\n      \"tree_size\" = \"Tree\\n Size\",\n      \"tree_diversity\" = \"Tree\\n Diversity\",\n      \"tree_density\" = \"Tree\\n Density\",\n      \"soil\" = \"Soil\",\n      \"age\" = \"Age\", \n      \"past_land_use\" = \"Past Land\\n Use\"\n    ),\n    exposure = 'past_land_use',\n    outcome = 'cooling',\n    coords = list(x = c(cooling = 0, tree_density = -1, tree_size = 0, tree_diversity = 1, age = 1, soil = 0, past_land_use = 0),\n                  y = c(cooling = 3, tree_density = 2, tree_size = 2, tree_diversity = 2, age = 1, soil = 1, past_land_use = 0))) %&gt;%\n    tidy_dagitty() %&gt;%\n    mutate(status = case_when(name == \"cooling\" ~ 'outcome',\n                              name == \"past_land_use\" ~ 'exposure',\n                              .default = 'NA'))\n\nggplot(dagified, aes(x = x, y = y, xend = xend, yend = yend)) +\n    theme_dag() + \n    geom_dag_point(aes(color = status)) +\n    geom_dag_label_repel(aes(label = label, fill = status),\n                         color = \"white\", fontface = \"bold\") +\n    geom_dag_edges() + \n    scale_fill_manual(values = c('darkseagreen', 'grey', 'lightskyblue')) + \n    scale_colour_manual(values = c('darkseagreen', 'grey', 'lightskyblue')) + \n    theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/2023-11-08-tree-cooling/index.html#data-simulations",
    "href": "posts/2023-11-08-tree-cooling/index.html#data-simulations",
    "title": "causal DAGS and full-luxury shade trees",
    "section": "data simulations",
    "text": "data simulations"
  },
  {
    "objectID": "posts/2023-11-08-tree-cooling/index.html#full-luxury",
    "href": "posts/2023-11-08-tree-cooling/index.html#full-luxury",
    "title": "causal DAGS and full-luxury shade trees",
    "section": "Full luxury",
    "text": "Full luxury"
  },
  {
    "objectID": "posts/2023-11-08-tree-cooling/index.html#posterior-inference",
    "href": "posts/2023-11-08-tree-cooling/index.html#posterior-inference",
    "title": "causal DAGS and full-luxury shade trees",
    "section": "Posterior inference",
    "text": "Posterior inference"
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html",
    "title": "Growth curves",
    "section": "",
    "text": "library(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nsuppressPackageStartupMessages(library(cmdstanr))\nMany animals and plants grow quickly when small and more slowly as they mature. There are many popular ways of describing this relationship; one very common and convenient relationship is the Von-Bertanalaffy (VB) growth curve:\n\\[\nL_t = L_0e^{-rt} + L_\\infty(1 - e^{-rt})\n\\tag{1}\\]\nThis can also be written as\n\\[\nL_t = L_\\infty - (L_\\infty - L_0)e^{-rt}\n\\]\nThis curve has a long tradition in ecology. It can be derived from simple assumptions about how different aspects of metabolism scale with the body size of an organism. I’m not going to derive it here because I don’t want this to be a huge post!\nI like this second way of writing the equation because it highlights that the VB equation is a linear transformation of an exponential function. We start out at \\(L_0\\) and exponentially decay towards \\(L_\\infty\\)."
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html#a-single-tree",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html#a-single-tree",
    "title": "Growth curves",
    "section": "a single tree",
    "text": "a single tree\nI’m going to do a simple simulation of one tree growing. here is code that does that\n\nsim_vb_one_tree &lt;- function(\n    time = seq(from = 10, to = 200, by = 5),\n    Lo = .01,\n    Lmax = 150,\n    r = .03,\n    sd = 5){\n  tibble::tibble(time,\n                 Lt = Lmax - (Lmax - Lo) * exp(-r*time),\n                 Lt_obs  = rnorm(length(Lt),\n                                 mean = Lt,\n                                 sd = 5))\n}\n\nvb_one_tree &lt;- sim_vb_one_tree()\n\nvb_one_tree |&gt; \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() + \n  geom_line(aes(y = Lt)) + \n  theme_bw()"
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html#recover-parameters",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html#recover-parameters",
    "title": "Growth curves",
    "section": "Recover parameters",
    "text": "Recover parameters\nHere is a stan model that matches this data generating process:\n\n\ndata{\n  int&lt;lower=0&gt; n;\n  vector[n] time;\n  vector[n] Lt;\n}\nparameters{\n  real&lt;lower=0&gt; r;\n  real&lt;lower=0&gt; Lmax;\n  real&lt;lower=0&gt; sigma_obs;\n}\nmodel{\n  Lt ~ normal(Lmax * (1 - exp(-r*time)), sigma_obs);\n  r ~ lognormal(-3, 1);\n  Lmax ~ normal(200, 20);\n  sigma_obs ~ exponential(1);\n}\n\n\n\none_tree_sim &lt;- sim_vb_one_tree(\n    Lmax = 150,\n    r = .03,\n    sd = 5)\n\none_tree_list &lt;- list(n = nrow(one_tree_sim),\n                      time  = one_tree_sim$time, \n                      Lt = one_tree_sim$Lt_obs)\n\none_tree_post &lt;- vb_one_tree$sample(data = one_tree_list,\n                                    refresh = 0L,\n                                    parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.4 seconds.\n\none_tree_post$summary() |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nlp__\n-84.0475862\n-83.7282000\n1.2654516\n1.0264040\n-86.4815100\n-82.6899900\n1.002211\n1726.271\n2253.937\n\n\nr\n0.0293271\n0.0292921\n0.0010091\n0.0009937\n0.0277237\n0.0310195\n1.002767\n2022.287\n2140.461\n\n\nLmax\n149.9431923\n149.9170000\n1.2118152\n1.1979408\n147.9899500\n151.9602000\n1.002967\n2163.288\n2291.239\n\n\nsigma_obs\n4.7191941\n4.6703850\n0.5231559\n0.5085615\n3.9512310\n5.6504515\n1.000848\n2331.954\n2412.225\n\n\n\n\n\nThese posterior intervals cover the numbers used to make up the data pretty well! Let’s look at the model predictions on a figure:\n\nexpected_df &lt;- one_tree_post |&gt; \n  spread_rvars(Lmax, r) |&gt; \n  expand_grid(time = seq(0, 200, length.out = 14)) |&gt; \n  mutate(Lt = Lmax * (1 - exp(-r * time)))\n\nexpected_plot &lt;- expected_df |&gt; \n  ggplot(aes(x = time, ydist = Lt)) + \n  stat_dist_lineribbon()\n\nexpected_plot\n\n\n\n\nGrowth curve for one tree. the line shows the expected value, with posterior uncertainty around exactly what that average should be.\n\n\n\n\nThis relationship shows the average line, the expected size of the tree. We can add the original data like this:\n\none_tree_sim |&gt; \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() +\n  stat_dist_lineribbon(aes(x = time, dist = Lt),\n                  data = expected_df, inherit.aes = FALSE) + \n  theme_bw()\n\n\n\n\n\n\n\n\nAt the time of this writing the error messages here are particularly unhelpful. If you try to use stat_lineribbon rather than stat_dist_lineribbon you get the following misleading message:\n\none_tree_sim |&gt; \n  ggplot(aes(x = time, y = Lt_obs)) + \n  geom_point() +\n  stat_lineribbon(aes(x = time, y = Lt),\n                  data = expected_df, inherit.aes = FALSE)\n\nError: Discrete value supplied to continuous scale"
  },
  {
    "objectID": "posts/2022-10-14-growth_curve_measurement_error/index.html#adding-measurement-error",
    "href": "posts/2022-10-14-growth_curve_measurement_error/index.html#adding-measurement-error",
    "title": "Growth curves",
    "section": "Adding measurement error",
    "text": "Adding measurement error\nThe above model reproduces predictions of the original line, but ignores measurement error. Here’s a few ways to add that into this same approach:\n\nSimulating observations in R\nOne way to do this is after the fact, using the handy tidyverse dplyr::rowwise() syntax, combined with posterior::rfun(). The latter function transforms rnorm into a function that both takes and produces an rvar, the specialized format for working with posterior draws. The latter function makes sure we redo this for every row of our dataframe.\n\nexpected_df &lt;- one_tree_post |&gt; \n  spread_rvars(Lmax, r, sigma_obs) |&gt; \n  expand_grid(time = seq(0, 200, length.out = 14)) |&gt; \n  mutate(Lt = Lmax * (1 - exp(-r * time))) |&gt; \n  rowwise() |&gt; \n  mutate(Lt_obs = posterior::rfun(rnorm)(n = 1, mean = Lt, sd = sigma_obs))\n\nexpected_df |&gt; \n  ggplot(aes(x = time, dist = Lt_obs)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\") + \n  geom_point(aes(y = Lt_obs), data = one_tree_sim,\n             pch = 21, fill = \"darkorange\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nThis has the advantage of happening all in R, keeping our posterior distribution slim.\nHowever sometimes it can be both convenient and more readable to keep the whole process inside Stan, and here’s how:\n\n\nPosterior predictive simulations in Stan\n\nvb_one_tree_gq &lt;- cmdstan_model(\n  stan_file = here::here(\n    \"posts/2022-10-14-growth_curve_measurement_error/vb_one_tree_gq.stan\"))\n\nvb_one_tree_gq\n\ndata{\n  int&lt;lower=0&gt; n;\n  vector[n] time;\n  vector[n] Lt;\n  int&lt;lower=0&gt; n_new;\n  vector[n_new] time_new;\n}\nparameters{\n  real&lt;lower=0&gt; r;\n  real&lt;lower=0&gt; Lmax;\n  real&lt;lower=0&gt; sigma_obs;\n}\nmodel{\n  Lt ~ normal(Lmax * (1 - exp(-r*time)), sigma_obs);\n  r ~ lognormal(-3, 1);\n  Lmax ~ normal(200, 20);\n  sigma_obs ~ exponential(1);\n}\ngenerated quantities{\n  vector[n_new] Lt_predicted;\n\n  for (i in 1:n_new){\n    Lt_predicted[i] = normal_rng(Lmax * (1 - exp(-r*time_new[i])), sigma_obs);\n  }\n}\n\n\n\none_tree_predictions &lt;- vb_one_tree_gq$sample(\n  data = purrr::splice(one_tree_list,\n                       time_new = seq(0, 200, length.out = 14),\n                       n_new = 14),\n  refresh = 0L,\n  parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.4 seconds.\n\none_tree_predictions |&gt; \n  spread_rvars(Lt_predicted[i]) |&gt; \n  mutate(time_new = seq(0, 200, length.out = 14)) |&gt; \n  ggplot(aes(x = time_new, dist = Lt_predicted)) + \n  stat_lineribbon() +\n  scale_fill_brewer(palette = \"Greens\") + \n  geom_point(aes(x = time, y = Lt_obs), \n             inherit.aes = FALSE,\n             data = one_tree_sim,\n             pch = 21, fill = \"darkorange\") + \n  theme_bw()"
  },
  {
    "objectID": "posts/2022-10-23-probability-integral-transform/index.html",
    "href": "posts/2022-10-23-probability-integral-transform/index.html",
    "title": "Probability integral transforms",
    "section": "",
    "text": "Tip\n\n\n\nThis is not a review or a proof of the Probability Integral Transform. A grad student asked me “what is this and why does it work” and I wanted to explore it with them.\nAmong the many wonderful plots that you can make with bayesplot, you will find one called ppc_loo_pit_overlay. You can read a lot more about it in this fabulous paper on Visualisation in Bayesian Workflow (Gabry et al. 2019)\nSo what is it, and how does it work?"
  },
  {
    "objectID": "posts/2022-10-23-probability-integral-transform/index.html#exploring-with-simulation",
    "href": "posts/2022-10-23-probability-integral-transform/index.html#exploring-with-simulation",
    "title": "Probability integral transforms",
    "section": "Exploring with simulation",
    "text": "Exploring with simulation\nTake random numbers from a distribution\n\nsome_numbers &lt;- rnorm(1560, mean = 14, sd = 2.5)\nhist(some_numbers)\n\n\n\n\n\n\n\n\nThen run them through that distribution’s CDF\n\nsome_pit &lt;- pnorm(some_numbers, mean = 14, sd = 2.5)\nhist(some_pit)\n\n\n\n\n\n\n\n\nSure enough we get a uniform shape!"
  },
  {
    "objectID": "posts/2022-10-23-probability-integral-transform/index.html#what-happens-when-you-are-wrong",
    "href": "posts/2022-10-23-probability-integral-transform/index.html#what-happens-when-you-are-wrong",
    "title": "Probability integral transforms",
    "section": "What happens when you are wrong",
    "text": "What happens when you are wrong\nlet’s make some curves that don’t really match\n\nlibrary(tidyverse)\nn &lt;- 4000\ntibble(meanval = seq(from = 1, to = 14, length.out = 6),\n       sd = 2.5) |&gt; \n  expand_grid(x = seq(from = 0, to = 18, length.out = 30)) |&gt;\n  mutate(normal_dist = dnorm(x, mean = meanval, sd = sd),\n         gamma_dist = dgamma(x, \n                        shape = meanval^2/sd^2,\n                        rate = meanval/sd^2)) |&gt; \n  pivot_longer(ends_with(\"dist\"), \n               names_to = \"distribution\",\n               values_to = \"value\") |&gt; \n  ggplot(aes(x = x, y = value, colour = distribution)) + \n  geom_line()  +\n  facet_wrap(~meanval)\n\n\n\n\n\n\n\n\nWe can see that the fit gets worse as the mean drops\nlet’s simulate data from the gamma and use the PIT assuming instead it is normal:\n\nn &lt;- 4000\ntibble(meanval = seq(from = 1, to = 14, length.out = 6),\n       sd = 2.5) |&gt; \n  rowwise() |&gt;\n  mutate(normal_dist = list(rnorm(n, mean = meanval, sd = sd)),\n         gamma_dist = list(rgamma(n, \n                        shape = meanval^2/sd^2,\n                        rate = meanval/sd^2))) |&gt; \n  pivot_longer(ends_with(\"dist\"), \n               names_to = \"distribution\",\n               values_to = \"samples\") |&gt; \n  rowwise() |&gt; \n  mutate(pit_samples = list(pnorm(samples, mean = meanval, sd = sd))) |&gt; \n  select(-samples) |&gt; \n  # filter(distribution == \"gamma_dist\") |&gt; \n  unnest(pit_samples) |&gt; \n  ggplot(aes(x = pit_samples)) + \n  geom_histogram() + \n  facet_grid(distribution~meanval)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nlet’s try it with just the gamma, but changing both moments and always using the normal:\n\nn &lt;- 4000\nexpand_grid(meanval = seq(from = 1, to = 14, length.out = 6),\n       sdval = seq(from = .2, to = 7, length.out = 4)) |&gt; \n  rowwise() |&gt;\n  mutate(gamma_dist = list(rgamma(n, \n                        shape = meanval^2/sdval^2,\n                        rate = meanval/sdval^2))) |&gt; \n  rowwise() |&gt; \n  mutate(pit_samples = list(\n    pnorm(gamma_dist,\n          mean = meanval,\n          sd = sdval))) |&gt; \n  select(-gamma_dist) |&gt; \n  # filter(distribution == \"gamma_dist\") |&gt; \n  unnest(pit_samples) |&gt; \n  ggplot(aes(x = pit_samples)) + \n  geom_histogram() + \n  facet_grid(sdval~meanval)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nand with the lognormal\n\nn &lt;- 4000\nexpand_grid(meanval = seq(from = 1, \n                          to = 14, \n                          length.out = 6),\n       sdval = seq(from = .2, \n                   to = 7, \n                   length.out = 4)) |&gt; \n  rowwise() |&gt;\n  mutate(\n    cf = log(sdval/meanval)^2 + 1,\n    lnorm_dist = list(rlnorm(n, \n                        meanlog = log(meanval) - .5*cf, \n                        sdlog = sqrt(cf))\n                      )\n    )|&gt; \n  rowwise() |&gt; \n  mutate(pit_samples = list(\n    pnorm(lnorm_dist,\n          mean = meanval,\n          sd = sdval)\n    # plnorm(lnorm_dist, \n    #        meanlog = log(meanval) - .5*cf, \n    #        sdlog = sqrt(cf))\n    )) |&gt; \n  select(-lnorm_dist) |&gt; \n  # filter(distribution == \"gamma_dist\") |&gt; \n  unnest(pit_samples) |&gt; \n  ggplot(aes(x = pit_samples)) + \n  geom_histogram() + \n  facet_grid(sdval~meanval)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/2022-10-23-probability-integral-transform/index.html#what-did-we-learn-here",
    "href": "posts/2022-10-23-probability-integral-transform/index.html#what-did-we-learn-here",
    "title": "Probability integral transforms",
    "section": "What did we learn here",
    "text": "What did we learn here\nThe PIT is one of an arsenal of diagnostic tools. The idea here is to run data “backwards” through the distribution we’ve chosen. If the distribution we chose is something like realize, the result should look kind of flat. If we are very far wrong, it won’t be flat. So this can serve as a quick goodness-of-fit test for your response distribution"
  },
  {
    "objectID": "draft/pres.html#tips-i-think-you-should-know",
    "href": "draft/pres.html#tips-i-think-you-should-know",
    "title": "remarks",
    "section": "Tips I think you should know",
    "text": "Tips I think you should know"
  },
  {
    "objectID": "draft/pres.html#what-did-you-do",
    "href": "draft/pres.html#what-did-you-do",
    "title": "remarks",
    "section": "What did you do?",
    "text": "What did you do?\nrite the damned equations for your models\nLook it up in one of several books!\n\\[\ny = \\beta_0 + \\beta_{\\text{water}}\\text{water}\n\\]\nsome examples"
  },
  {
    "objectID": "draft/pres.html#what-is-your-0",
    "href": "draft/pres.html#what-is-your-0",
    "title": "remarks",
    "section": "what’ is your 0",
    "text": "what’ is your 0"
  },
  {
    "objectID": "draft/pres.html#what-are-your-units",
    "href": "draft/pres.html#what-are-your-units",
    "title": "remarks",
    "section": "what are your units",
    "text": "what are your units"
  },
  {
    "objectID": "draft/pres.html#why-does-this-work",
    "href": "draft/pres.html#why-does-this-work",
    "title": "remarks",
    "section": "Why does this work",
    "text": "Why does this work"
  }
]