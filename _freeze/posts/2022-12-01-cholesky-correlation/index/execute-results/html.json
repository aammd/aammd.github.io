{
  "hash": "ed523b1ecf1bb18a5ba4b59e68c67e83",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Cholesky decomposition\"\nauthor: \"Andrew MacDonald\"\ndescription: |\n  It's giving correlations.\ndate: 11 Nov 2022\ncategories: [UdeS, stan]\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\n```\n:::\n\n\n\nYou can give uncorrelated random numbers a specific correlation by using the Cholesky decomposition. \nThis comes in handy when you're modelling correlated random variables using MCMC.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neg <- rethinking::rlkjcorr(1, 2, 1)\neg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]\n[1,]  1.0000000 -0.5587252\n[2,] -0.5587252  1.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\ncc <- chol(eg)\n\npurrr::rerun(30,{\n  zz <- matrix(data = rnorm(500), ncol = 2)\n  # plot(zz)\n  rr <- t(cc) %*% t(zz)\n  # plot(t(rr))\n  cor(t(rr))[1,2]\n}) |> \n  purrr::flatten_dbl() |> density() |> plot(main = \"Simulated correlations\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `rerun()` was deprecated in purrr 1.0.0.\nâ„¹ Please use `map()` instead.\n  # Previously\nrerun(30, {\nzz <- matrix(data = rnorm(500), ncol = 2)\nrr <- t(cc) %*% t(zz)\ncor(t(rr))[1, 2]\n})\n\n  # Now\nmap(1:30, ~ {\nzz <- matrix(data = rnorm(500), ncol = 2)\nrr <- t(cc) %*% t(zz)\ncor(t(rr))[1, 2]\n})\n```\n\n\n:::\n\n```{.r .cell-code}\nabline(v=eg[1,2])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\nHere we can see that the correlation we want is -0.5587252, and indeed we're able to give exactly that correlation to uncorrelated random numbers. \n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\neg <- rethinking::rlkjcorr(1, 2, 5)\nprint(eg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]\n[1,] 1.00000000 0.08358276\n[2,] 0.08358276 1.00000000\n```\n\n\n:::\n\n```{.r .cell-code}\ncc <- chol(eg)\n\nt(cc) %*% cc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]\n[1,] 1.00000000 0.08358276\n[2,] 0.08358276 1.00000000\n```\n\n\n:::\n\n```{.r .cell-code}\nmm <- matrix(rnorm(4000, mean = 0, sd = 1), ncol = 2)\n\nplot(mm)\n```\n\n::: {.cell-output-display}\n![uncorrelated numbers](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nyy <- t(t(cc) %*% t(mm))\n\nplot(yy)\n```\n\n::: {.cell-output-display}\n![after being given a correlation](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n\n## Doing it by hand\n\nBecause the case for only two random variables is pretty simple, we can actually write out the Cholesky decomposition by hand. \nHere I want to give some independent random numbers a correlation of -0.8:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- -.8\nL <- matrix(c(1,0, p, sqrt(1 - p^2)), ncol = 2, byrow = TRUE)\nzz <- matrix(data = rnorm(1000), ncol = 2)\nyy <- t(L %*% t(zz))\nplot(yy)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(yy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]\n[1,]  1.0000000 -0.8005567\n[2,] -0.8005567  1.0000000\n```\n\n\n:::\n:::\n\n\n\nThis can be written without matrix multiplication like this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz1 <- rnorm(1000)\nz2 <- rnorm(1000)\n\ny1 <- z1\ny2 <- p*z1 + sqrt(1 - p^2)*z2\n\nplot(y1, y2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(y2, y1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.8105853\n```\n\n\n:::\n:::\n\n\n\nIn a Stan model, I might want to use a scaled beta distribution to model the correlation between, say, slopes and intercepts in a model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- .84\nphi <- 80\ng <- rbeta(1, mu*phi, (1 - mu)*phi)\np_trans <- g*2 - 1\n\nsim_y_corr <- function(n, p) {\n  z1 <- rnorm(n)\n  z2 <- rnorm(n)\n  \n  y1 <- z1\n  y2 <- p*z1 + sqrt(1 - p^2)*z2\n  \n  return(data.frame(y1, y2))\n}\n\nsim_y_corr(n = 1000, p = p_trans) |> plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nor even work on the logit scale:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq <- rnorm(1, mean = -1, sd = .1)\np_trans_n <- plogis(q)*2 - 1\nsim_y_corr(n = 1000, p = p_trans_n) |> plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\nThis is even a little easier to think about: because of all the transformations, 0 on the logit scale still means a 0 correlation; negative and positive numbers mean negative and positive correlations as well. \n\n\n\n\n\nAnother way to make correlation matrices is here: https://www.rdatagen.net/post/2023-02-14-flexible-correlation-generation-an-update-to-gencorgen-in-simstudy/\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}