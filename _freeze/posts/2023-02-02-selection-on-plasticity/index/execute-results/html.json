{
  "hash": "88971fa2534f4c1a624bd80990a60e1b",
  "result": {
    "markdown": "---\ntitle: \"Validating a model of selection on plasticity\"\nauthor: \"Andrew MacDonald\"\ndescription: |\n  Plus Ã§a change, plus c'est la change qui change\ndate: 02 Feb 2023\ncategories: [UdeS, stan]\nexecute:\n  eval: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\n```\n:::\n\n\nwhen an animal shows a difference\n\nAll animals respond to the environment, adapting their physical or biological characteristics into\n\nall an\n\nthe response that an animal has, is in part determined by heir genetics, by heritable variation in their ability\n\nother factors can of course invluence who an animal responds to changing environment. It might be that animals only respond to changing environment when they are in good condition. or that changes in the environment always provoke differenc responses, but hat selection only happnens a few years out of the total\n\nanyway, we are going to simulate data from a complex model of selection on two correlated traits.\n\nlet's start small\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# environment \nnindivid <- 38\nneach <- 4\n\nfake_obs <- expand_grid(indiv_id = rep(1:nindivid, each = neach)) |> \n  mutate(X = runif(length(indiv_id), min = 11, max = 26))\n\n# slopes\nslope_mean <- 2.6\nslope_sd <- .9\nb_1 <- rnorm(nindivid, mean = slope_mean, sd = slope_sd)\nb_0 <- 3\n\nfake_obs |> \n  mutate(b_0 = b_0,\n         b_1 = b_1[indiv_id],\n         y = b_0 + b_1 * X) |> \n  ggplot(aes(x = X, y = y, group = indiv_id)) + geom_line()\n```\n:::\n\n\nit's unlikely that everyone has the same yintercept\n\na different approach, based on group-mean centering, will help us think about this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb_0 <- rnorm(nindivid, mean = 45, sd = 6)\nfake_obs_2par <- fake_obs |> \n  group_by(indiv_id) |> \n  mutate(x_c = X - mean(X), \n         y = b_0[indiv_id] + b_1[indiv_id] * x_c)\n\nfake_obs_2par |> \n  ggplot(aes(x = X, y = y, group = indiv_id)) + geom_line()\n```\n:::\n\n\ncontrast this with the display for the centered variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_obs_2par |> \n  ggplot(aes(x = x_c, y = y, group = indiv_id)) + geom_line()\n```\n:::\n\n\nthis is just a simple demo of a pretty general pattern!\n\ncentering just moves the line around -- I think we would model this this way, if we felt that the\n\nlet's start simulating data and comparing the fit with models\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd_obs <- 3\nfake_obs_error <- fake_obs_2par |> \n  mutate(yobs = rnorm(n = length(y), mean = y, sd = sd_obs))\n\nfake_obs_error |> \n  ggplot(aes(x = X, y = yobs)) + \n  geom_point()\n```\n:::\n\n\nWe can already try to model this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\n\nm <- lmer(yobs ~ X + (X|indiv_id), data = fake_obs_error)\n\nm |> summary()\n```\n:::\n\n\nworks but not really helpful for the intercepts -- we need to fit a centered model to get those values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmer(yobs ~ x_c + (x_c|indiv_id), data = fake_obs_error) |> \n  summary()\n```\n:::\n\n\nNow we can recover parameters that went in to the model.\n\n## two correlated predictors\n\nThis will come together with the rest of the model soon, but I want to get a start on this now. The two traits we are mesauring are kind of weird and different from the tutal way of thinking about environemtnal traits. let me try to explain it. so there was another procedure earlier which attempted to measure the \"window\" of temperature which provokes the greatest change.\n\nas an aside, do we expect the greatest variation to be found with the greatest selection? consider the number of adjectives in a scientivi paper vs the number of refrerences. I don't know, but I would epect that the former is way more variable -- and matters far less.\n\nanyway that was done. So the two response variables here are measured in the same units but not exactly the same VALUES for each\n\njust makes me curious to start by simulating this (and perhaps also to look in the data for examples)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(orig = rerun(350, runif(30, min = -1, max = 1))) |> \n  rowwise() |> \n  mutate(x1 = mean(head(orig, 21)),\n         x2 = mean(tail(orig, 21))) |> \n  ggplot(aes(x = x1, y = x2)) + geom_point()\n```\n:::\n\n\nSure enough they are correlated. I'm sure with some effort I could calculate the equation for this correlation, but let's not.\n\nDid audrey do this differently for each bird? for each environmental variable or what? does it count as conditioning?\n\nTK ask her this question.\n\n## back to main question: measuring selection\n\nlets begin by replicating the bivariate model of fitness that you see, for example, in\n\nstart with fake obs eror\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# environment \nnindivid <- 38\nneach <- 4\n\nfake_obs <- expand_grid(indiv_id = rep(1:nindivid, each = neach)) |> \n  mutate(X = runif(length(indiv_id), min = 11, max = 26))\n\n# slopes\nslope_mean <- 2.6\nslope_sd <- .9\nintercept_mean <- 45\nintercept_sd <- 6\nb_1 <- rnorm(nindivid, mean = slope_mean, sd = slope_sd)\nb_0 <-  rnorm(nindivid, mean = slope_mean, sd = slope_sd)\n\nfake_obs |> \n  mutate(b_0 = b_0,\n         b_1 = b_1[indiv_id],\n         y = b_0 + b_1 * X)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfake_obs_error\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}