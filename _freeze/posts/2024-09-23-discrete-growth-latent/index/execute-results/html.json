{
  "hash": "9f6ead1603c466473b7742044cf2d3f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Ricker Model with Allee effects\"\nauthor: \"Andrew MacDonald\"\ndescription: |\n  How to model discrete growth with low-density effects.\ndate: 25 Sept 2024\neditor: source\ncategories: [UdeS, stan]\ndraft: false\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(targets)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- 0.05\nc <- 4\nN0 <- 5\n\ntime <- 300\n\nN <- numeric(time)\n\nN[1] <- N0\n\nfor (t in 2:time){\n  N[t] <- N[t-1]*exp(r * (N[t-1] - c)/N[t-1])\n}\n\nplot(N)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nlet's add density dependence \n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- 0.05\nc <- 4\nN0 <- 5\nK <- 500\n\ntime <- 300\n\nN <- numeric(time)\n\nN[1] <- N0\n\nfor (t in 2:time){\n  ri <- rnorm(1, mean = 0, sd = 1)\n  # print(ri)\n  N[t] <- N[t-1]*exp(r * (1 - N[t-1]/K))\n}\n\nplot(N, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nNow let's add some uncertainty every time step\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_ricker_allee <- function(\n    r = 0.05,\n    C = 9,#44\n    N0 = 7,\n    K = 500,\n    time = 300,\n    sd_process = 1.5){\n  \n  N <- numeric(time)\n  \n  N[1] <- N0\n  \n  for (t in 2:time){\n    ri <- rnorm(1, mean = 0, sd = sd_process)\n    # print(ri)\n    N[t] <- N[t-1]*exp(exp(log(r) + ri) * (1 - N[t-1]/K) * (1 - C/N[t-1]))\n  }\n  \n  return(N)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nallee_sim <- map_df(1:50, ~ tibble(\n  N = sim_ricker_allee(\n    time = 300,C = 6, N0 = 7),\n  time = 1:300,\n  7),\n  .id = \"sim\") |> \n  bind_cols(type = \"allee\")\n\n\nno_allee_sim <- map_df(1:50, ~ tibble(\n  N = sim_ricker_allee(\n    time = 300,\n    C = 0,\n    N0 = 7),\n  time = 1:300,\n  7),\n  .id = \"sim\") |> \n  bind_cols(type = \"no allee\")\n\n\n\ntwo_sims <- bind_rows(allee_sim, no_allee_sim)\n\ntwo_sims |> \n  ggplot(aes(x = time, y = N, group = sim)) + \n  geom_line() + \n  coord_cartesian(ylim = c(0, 1000)) + \n  facet_wrap(~type)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 287 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntwo_sims |> \n  ggplot(aes(x = time, y = N, group = sim)) + \n  geom_line() + \n  facet_wrap(~type) +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 100))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 287 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\nand if we look at these another way we can see the difference:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndeltaN_data <- two_sims |> \n  filter(type == \"no allee\") |> \n  group_by(sim) |> \n  mutate(deltaN = log(N/lag(N))) |> \n  drop_na(deltaN)\n\ndeltaN_data |> \n  ggplot(aes(x = N, y = deltaN)) + \n  geom_point() + \n  coord_cartesian(ylim = c(-10, 5), xlim = c(0, 600))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndeltaN_data |> \n  ggplot(aes(x = N, y = deltaN)) + \n  geom_point() + \n  coord_cartesian(ylim = c(-.0,1), xlim = c(0, 600))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\nThis really doesn't seem right! \nThe y intercept is supposed to be $r$, and the X intercept is supposed to be $K$! \nIs the cause the variation in growth rate? that is, the parameter `sd_process` above?\n\n\nLet's make a new simulation where this is set to a very low value:\n:swea\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_df(1:50, ~ tibble(\n  N = sim_ricker_allee(\n    time = 300,\n    C = 0,\n    N0 = 7,\n    sd_process = 0.01),\n  time = 1:300,\n  7),\n  .id = \"sim\") |> \n  group_by(sim) |> \n  mutate(deltaN = log(N/lag(N))) |> \n  drop_na(deltaN) |> \n  ggplot(aes(x = N, y = deltaN)) + \n  geom_point() + \n  coord_cartesian(ylim = c(0, .1), xlim = c(0, 600)) + \n  geom_hline(yintercept = .05) + \n  geom_vline(xintercept = 500)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nMuch better! so the process error makes this pretty traditional plot go kind of haywire.\n\nInterestingly, looking back at the previous figures, you can see that the error is not _around_ the \"correct\" line at all but mostly below it. \nThat suggests that trying to model error based on lagged growth is probably not going to give a useful answer for the parameters\n\n## Observation error\n\nSo far all of this has been in a perfect, imaginary world where we have perfect information on the population density. \nIn reality, we'll always have a **sample** of the population density. \nA simple model for this variation is that it follows a Poisson distribution:\n\n\n$$ \n\\begin{align}\nY_t &\\sim \\text{Poisson}(N_t)\\\\\nN_{t+1} &= N_t e^{r\\left(1 - \\frac{N_t}{K}\\right)} \\\\\n\\end{align}\n$$\n\nLets do a simulation of several observations of *one single* time series:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1618)\navg_dens <- sim_ricker_allee(C = 5, N0 = 7, time = 120)\n\nobs_dens <- avg_dens |> \n  imap(\n    ~tibble(\n      time_id = as.numeric(.y),\n      obs = rpois(5, lambda = .x),\n      obs_id = seq_along(obs)\n    )\n    \n  ) |> \n  bind_rows()\n\n\nobs_dens |> \n  ggplot(aes(x = time_id, y = obs,group = obs_id)) + \n  geom_line() + \n  geom_line(aes(x = time_id, y = avg), inherit.aes = FALSE, \n            col = \"red\",\n            data = tibble(\n              time_id = seq_along(avg_dens),\n              avg = avg_dens))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThis shows a few things of interest: the wiggling red line, which shows variation in growth rate at each timestep. This is process error. \nWe can also see variation around this; these is variation coming from a Poisson distribution centered on the true mean population size.\n\n\n## Coding and validating a model\n\n\nWe're going to torture the math a little bit, to make it more convenient to write it Stan:\n\nTake the expression for the average and log both sides:\n\n$$\n\\begin{align}\nN_{t+1} &= N_te^{r\\left(1 - \\frac{N_t}{K}\\right)} \\\\\n\\ln(N_{t+1}) &= \\ln{N_t} + r(1 - \\frac{N_t}{K})  \\\\\nL_{t+1} &= L_t + e^{\\ln{r} + \\ln(1 - e^{L_t - \\ln{K}})} \\\\\nL_{t+1} &= L_t + e^{s + \\ln(1 - e^{L_t - J})} \\\\\n\\end{align}\n$$\nHere to keep notation simple I'm just writing \n\n* $s = \\ln{r}$\n* $J = \\ln{K}$\n* $Lt = \\ln{N_t}$\n\nI know what you're thinking: get help, Andrew! \n\nThere's a couple reasons for this violence: \n\n* Working on the log scale is easier for parameter estimation. it keeps values on similar scales, even though, for example $r$ and $K$ have very different magnitudes. \n* We can take advantage of Stan's built-in [composed functions](https://mc-stan.org/docs/functions-reference/real-valued_basic_functions.html#composed-functions)\n* it will be easier to add hierarchical effects if (when :P ) we want to do that!\n\nHere's a more complete rendering of the model, which will set us up for writing Stan code in the next section:\n\n$$\n\\begin{align}\nY_i &\\sim \\text{Poisson\\_log}(L_{t[i]}) \\\\\nL_{t+1} &= L_t + e^{\\left(s + \\ln\\left(1 - e^{L_t - J}\\right)\\right)} \\\\\nL_0 &\\sim \\text{Normal}(2,1) \\\\\ns &\\sim \\text{Normal}(-3, 0.5) \\\\\nJ &\\sim \\text{Normal}(6, 1)\n\\end{align}\n$$\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}