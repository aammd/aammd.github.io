{
  "hash": "1e842872c0e53e1b0c66459fc23b93c4",
  "result": {
    "markdown": "---\ntitle: \"Transformed regression in Stan\"\nauthor: \"Andrew MacDonald\"\ndescription: |\n  Curvy lines done dirt cheap.\ndate: 22 Aug 2023\ndraft: false\neditor: source\ncategories: [UdeS, stan, QCBS]\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\nlibrary(cmdstanr)\n```\n:::\n\n\n##  Sometimes things slow down\n\n> There are no straight lines in ecology\n\nA lot of relationships in ecology are curved lines, often because there's a constraint on the thing we are modelling as a response variable. For example, animals might get used to a specific stimulus over repeat exposures, with the result that eventually the response time drops to 0. However, response time can never be less than 0, and so the relationship is constrained from below. Relationships can be bounded above as well. Consider how the mass of an individual organism scales with increasing resources: more food means a bigger body, but eventually that relationship will flatten out.\n\nIn ecology, its very common to model these nonlinear relationships by applying a transformation to the response variable and modelling the resulting relationship with a gaussian linear regression. Sometimes this works out fine. Increasingly, ecologists are trying to go beyond this with models that reflect the data-generating process more explicitly. For example, an ecologist might model growth with one of the many growth equations. However, even if we develop a nonlinear model with a non-gaussian likelihood and all the features, our colleagues will still want a comparison to the now-traditional transformed lines.\n\nI wanted to write some simple Stan models to fit these transformed-response models. \nI was curious about the specific shape of the relationship we are fitting when we use these models. I have a suspicion that these curves are often fit to data, not because the scientist wants to make this curving shape, but because it improves residual plots.  Whatever the reason for their use, I find that making pictures of something really helps me to understand it. My colleagues use these models, so this post is a beginning exploration of them.\n\n## Two pictures of two curves\n\nI love the base R function `curve()`; it might be my favourite base function! Let's look at the curve that results from these two kinds of transformations.  \n\nWhen I say \"results from\", I mean what happens if you reverse the transformation on the response variable. In the case of a log-transformation model:\n\n$$\n\\begin{align}\n\\text{log}(y) &= a + bx \\\\\ny &= e^{a + bx}\n\\end{align}\n$$\n\nAnd in the case of a square root model:\n\n$$\n\\begin{align}\n\\sqrt{y} &= a + bx \\\\\ny &= (a + bx)^2\n\\end{align}\n$$\n\nIn both cases, the effect is like having a link function on the response variable. \nTo get the curve, you just reverse the transformation on both.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(exp(7 - 2*x), xlim = c(0, 4))\n```\n\n::: {.cell-output-display}\n![Log-transformed responses are like fitting an exponential curve](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve((7-2*x)^2, xlim = c(0, 8))\n```\n\n::: {.cell-output-display}\n![Square root transformed models are like fitting a quadratic curve](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-note}\nI find this a confusing way to think of the square root curve; I prefer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsteep <- 2\nminmax <- 4\nelev <- 1\ncurve(steep * (x - minmax)^2 + elev, xlim  = c(0, 8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThis is far easier to set priors on -- useful to remember for those (rare?) times when this is a relationship you actually want to work with.\n:::\n\n\n## Writing the models in Stan\n\nLet's write out the model we're fitting, using `g()` to refer to either the log or the square root:\n\n$$\n\\begin{align}\ng(y) &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= a + bx \\\\\na &\\sim \\text{Normal}(...) \\\\\nb &\\sim \\text{Normal}(...) \\\\\n\\sigma &\\sim \\text{Exponential}(...) \\\\\n\\end{align}\n$$\n\nYou can see from here that we're putting the likelihood ( the top line, $\\text{Normal}(\\mu, \\sigma)$ ) through the transformation function also. This means that the \"errors\" or variation around the curve also get transformed. In the case of the log transformation, we are putting that normal distribution through the exponential function -- in other words, we're using a [Lognormal distribution](https://en.wikipedia.org/wiki/Lognormal_distribution).\nWith the square root transformation, it seems like perhaps we're modelling something like a [generalized chi square](https://stats.stackexchange.com/questions/93383/square-of-normal-distribution-with-specific-variance).\n\n### Square root transformation\n\nI want to begin with a simple prior predictive simulation from the square-root model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt_prior <- cmdstan_model(\n  here::here(\"posts/2023-08-22-transformed-regression/sqrt_prior.stan\")\n  )\nsqrt_prior\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata{\n  int n;\n  vector[n] x;\n}\nparameters{\n  real slope;\n  real intercept;\n  real<lower=0> sigma;\n}\nmodel {\n  slope ~ std_normal();\n  intercept ~ std_normal();\n  sigma ~ exponential(1);\n}\ngenerated quantities {\n  vector[n] yrep;\n  for (i in 1:n){\n    yrep[i] = square(normal_rng(slope * x[i] + intercept, sigma));\n  }\n}\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxseq <- seq(from = 0, \n    to = 20, \n    length.out = 15)\n\nsqrt_prior_reps <- sqrt_prior$sample(\n  data = list(x = xseq,\n              n = 15),\n  refresh=0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.8 seconds.\n```\n:::\n\n```{.r .cell-code}\nsqrt_prior_plot <- sqrt_prior_reps$draws() |> \n  tidybayes::gather_draws(yrep[i], ndraws = 12) |> \n  mutate(x = xseq[i])\n  \nsqrt_prior_plot |> \n  ggplot(aes(x= x, y = .value)) + geom_point() + \n  facet_wrap(~.draw)\n```\n\n::: {.cell-output-display}\n![some prior predictive simulations from a square-root transformed model.](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nTo me, this illustrates the difficulty of setting useful priors on a square-root transformed model. \n\n## Log transformation\n\nlet's try with log-transformed curves. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_trans <- cmdstan_model(\n  here::here(\"posts/2023-08-22-transformed-regression/log_trans.stan\"))\nlog_trans\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata{\n  int n;\n  vector[n] x;\n  vector[n] y;\n}\ntransformed data{\n  vector[n] ylog = log(y);\n}\nparameters{\n  real slope;\n  real intercept;\n  real sigma;\n}\nmodel{\n  ylog ~ normal(intercept + slope*x, sigma);\n}\ngenerated quantities{\n  vector[n] ybar = exp(intercept + slope*x);\n  vector[n] yrep;\n  for(i in 1:n){\n    yrep[i] = exp(normal_rng(intercept + slope*x[i], sigma));\n  }\n}\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a bit of fake data\nxx <- 0:14\nyb <- 5.7  - 0.8*xx\nyy <- exp(rnorm(length(xx), mean = yb, sd = 1))\nplot(xx,yy)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlog_trans_post <- log_trans$sample(\n  data = list(n = length(xx), x = xx, y = yy),\n  refresh  = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_trans_post$draws() |> tidybayes::gather_rvars(ybar[i]) |> \n  mutate(xx = xx[i]) |> \n  ggplot(aes(x = xx, ydist = .value)) + stat_lineribbon()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_trans_post$draws() |> tidybayes::gather_rvars(yrep[i]) |> \n  mutate(xx = xx[i]) |> \n  ggplot(aes(x = xx, ydist = .value)) + stat_lineribbon()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Square root transformed model\n\nGoing back to a square root model, fitting it to the same data that I just used for the log-transformed model.\nI'm doing it this way because the toy dataset I made and used above is much easier to reason about. I also want to see how a square-root model fits data that comes from a different data-generating process.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt_trans <- cmdstan_model(here::here(\"posts/2023-08-22-transformed-regression/sqrt_trans.stan\"))\nsqrt_trans\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.stan}\ndata{\n  int n;\n  vector[n] x;\n  vector[n] y;\n}\ntransformed data{\n  vector[n] ytrans = sqrt(y);\n}\nparameters{\n  real slope;\n  real intercept;\n  real sigma;\n}\nmodel{\n  ytrans ~ normal(intercept + slope*x, sigma);\n}\ngenerated quantities{\n  vector[n] ybar = square(intercept + slope*x);\n  vector[n] yrep;\n  for(i in 1:n){\n    yrep[i] = square(normal_rng(intercept + slope*x[i], sigma));\n  }\n}\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt_trans_post <- sqrt_trans$sample(\n  data = list(n = length(xx), x = xx, y = yy), \n  refresh=0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.9 seconds.\n```\n:::\n\n```{.r .cell-code}\nsqrt_trans_post$draws() |> tidybayes::gather_rvars(ybar[i]) |> \n  mutate(xx = xx[i]) |> \n  ggplot(aes(x = xx, ydist = .value)) + stat_lineribbon()\n```\n\n::: {.cell-output-display}\n![square-root transformed model fit to data from a log-transformed model. Just to prove you can](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt_trans_post$summary() |> \n  slice(2:4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 10\n  variable    mean median    sd   mad    q5    q95  rhat ess_bulk ess_tail\n  <chr>      <num>  <num> <num> <num> <num>  <num> <num>    <num>    <num>\n1 slope     -0.924 -0.923 0.235 0.228 -1.31 -0.547  1.00    1617.    1727.\n2 intercept 10.1   10.1   1.93  1.85   6.98 13.3    1.00    1629.    1426.\n3 sigma      3.82   3.71  0.827 0.763  2.69  5.39   1.00    1409.    1677.\n```\n:::\n:::\n\n\nUnintuitive as these parameters are, you at least can get your downwards and decelerating curve from this model! \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}