{
  "hash": "f5c115acd95a4033c71556c429c437b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"\"\nauthor: \"Andrew MacDonald\"\ndescription: |\n  Inspired by Ives 2003.\ndate: 17 Nov 2023\neditor: source\ncategories: [UdeS, stan, reproduction, MAR]\nbibliography: references.bib\nknitr:\n  opts_chunk: \n    warning: false\n    message: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(tidybayes)\n```\n:::\n\n\n## Background\n\nHow do species populations change over time?\n\nHow much of that change is caused by interactions with their community?\n\n20 years ago a very influential paper was written by Ives, Dennis, Cottingham and Carpenter [@ives2003].\nIves et al. present a method called a first-order Multivariate AutoRegressive model, also known as MAR(1). The core idea is that species are growing in a density-dependent way, but at a rate that is influenced by the per-capita effects of every other interacting population.\nA transition matrix captures the effects of each species on every other, and lets us predict how the vector species abundances changes over time. \n\n\nIves et al. advance this argument by beginning with univariate (single-species) approach, and that is what this post is about. Perhaps a future post will cover all the models.\n\n$$\n\\begin{align}\nn_t &= n_{t - 1}e^{a + (b - 1)ln(n_{t-1})} \\\\\nln(n_t) = x_t &= a + bx_{t - 1} \n\\end{align}\n$$\nWe can predict the curve at any time $t$ with this expression, from finite series:\n\n$$\nx_t = x_\\infty + b^t(x_0 - x_\\infty)\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(200 + (.9)^x * (4 - 200), xlim = c(0, 100))\n```\n\n::: {.cell-output-display}\n![Curve of a Gompertz growth model.](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nHowever there might be variation every time step because growth rate differences are caused by lots of unmeasured things.\n\n$$\n\\begin{align}\nX_t &= a + bX_{t-1} + \\epsilon_t \\\\\n\\epsilon &\\sim \\text{Normal}(0, \\sigma)\n\\end{align}\n$$\n\nThis leads to a mean and variance at time $t = \\infty$\n\n$$\n\\begin{align}\n\\mu_\\infty &= \\frac{a}{1 - b} \\\\\nv_\\infty &= \\frac{\\sigma^2}{1 - b^2}\n\\end{align}\n$$\n\n\nand the mean and variance at time $t$\n\n$$\n\\begin{align}\n\\mu_t &= \\mu_\\infty + b^t(x_0 - \\mu_\\infty) \\\\\nv_t &= \\sigma^2\\frac{1 - (b^2)^t}{1 - b^2} = v_\\infty(1 - b^{2t})\n\\end{align}\n$$\nEverything here is on the log scale. \nThe result is something that we can work with in a model for the likelihood -- the mean and variance of a normal distribution.\n\n## Simulations\n\nHere are simulations from a one-species AR-1 model that imitate Ives et al. figure 1.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nsimulate_pop_growth <- function(\n    a = 0, \n    b, \n    sigma = 1, \n    tmax = 50, \n    x0 = -8) {\n  \n  xvec <- numeric(tmax)\n  \n  xvec[1] <- x0\n  \n  ## process error\n  eta <- rnorm(tmax, mean = 0, sd = sigma)\n  \n  for(time in 2:tmax){\n    xvec[time] <- a + b*xvec[time-1] + eta[time]\n  }\n  \n  return(xvec)\n}\n\nmap_dfr(1:10, ~ tibble(pop = simulate_pop_growth(b = 0.6, tmax = 50),\n                       time = 1:length(pop)), .id = \"sim\") |> \n  ggplot(aes(x = time, y = pop, group = sim)) + \n  geom_line() + \n  geom_hline(yintercept = 0) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Growth with a = 0, b = .6, sigma = 1](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmap_dfr(1:10, ~ tibble(pop = simulate_pop_growth(b = 0.95, tmax = 50),\n                       time = 1:length(pop)), .id = \"sim\") |> \n  ggplot(aes(x = time, y = pop, group = sim)) + \n  geom_line() + \n  geom_hline(yintercept = 0)+ \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Growth with a = 0, b = .95, sigma = 1](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n\nIt's fun to take a look at this curve after exponentiating it, so as to see the real population sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_dfr(1:10, \n        ~ tibble(pop = simulate_pop_growth(\n          a = 3, b = 0.4, tmax = 10, sigma = 0.03),\n          time = 1:length(pop)), .id = \"sim\") |> \n  ggplot(aes(x = time, y = exp(pop), group = sim)) + \n  geom_line() + \n  geom_hline(yintercept = 0, lty = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n### Functions for the mean and variance\n\nWe can also get a plot of the changing mean and variance over time. Just from playing with these, we can see what the simulations earlier showed: that variance depends on both the process error $\\sigma$ and on the parameter that controls the amount of density dependence, $b$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalc_mean <- function(a, b, time, n_start){\n  mu_max <- a / (1 - b)\n  \n  mu_max + b^time * (n_start - mu_max)\n}\n\ncalc_var <- function(b, time, sigma){\n  bsq <- b^2\n  \n  var_max = sigma^2/(1 - bsq)\n  \n  var_max * (1 - bsq^time)\n}\n\ncurve(calc_mean(0, .8, n_start = -8, time = x), xlim = c(0, 50))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncurve(calc_var(.9, time = x, sigma = 1),\n      xlim = c(0,50), ylim = c(0, 10))\n\ncurve(calc_var(.8, time = x, sigma = 1), add = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\nDo these numbers reflect the distribution we see in the simulations?\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nset.seed(5002)\nsome_sims <- map_dfr(1:300, ~ tibble(pop = simulate_pop_growth(b = 0.9, tmax = 30),\n                       time = 0:(length(pop)-1)), .id = \"sim\")\n\nsim_meanvar <- some_sims |> \n  group_by(time) |> \n  summarize(sim_mean = mean(pop),\n            sim_var = var(pop))\n\nsome_sims |> \n  filter(time == 22) |> \n  pluck(\"pop\") |> \n  hist(probability = TRUE, breaks = 30, xlab = \"population size (log)\", main = \"Simulated and predicted population size distribution\")\n\ncurve(dnorm(x, \n            mean = calc_mean(a = 0, b = .9, time = 22, n_start = -8),\n            sd = sqrt(calc_var(b = .9, time = 22, sigma = 1))), \n      add = TRUE)\n```\n\n::: {.cell-output-display}\n![Simulations match the theoretical predictions very closely.  Math works!](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\na_fig <- 0\nb_fig <- 0.8\nnstart_fig <- -8\nsigma_fig <- 1\n\ntibble(\n  time = 0:25,\n  mean = calc_mean(\n    a = a_fig, b = b_fig, time = time,\n    n_start = nstart_fig),\n  sd = sqrt(calc_var(\n    b = b_fig, time = time, sigma = sigma_fig))) |> \n  ggplot(aes(x = time,\n             ymin = mean - sd*2,\n             ymax = mean + sd*2, \n             y = mean)) + \n  geom_ribbon(fill = \"lightblue\")+\n  geom_line(col = \"darkblue\", lwd = 2) + \n  geom_line(\n    aes(x = time,\n        y = pop, \n        group = sim),\n    inherit.aes = FALSE,\n    data = map_dfr(1:10, \n                   ~ tibble(\n                     pop = simulate_pop_growth(\n                       a = a_fig, \n                       b = b_fig,\n                       tmax = 25, \n                       sigma = sigma_fig),\n                     time = 0:(length(pop)-1)\n                     ),\n                   .id = \"sim\"\n                   )\n  ) + \n  NULL + \n  labs(x = \"Time\", y = \"log population size\")\n```\n\n::: {.cell-output-display}\n![Simulations match the theoretical predictions very closely.  Math works!](index_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\n## Aphids\n\nBelow I explore this model and try to fit some examples in Stan.\nI'm building this example to match work being done at UdeS by students of [Matt Barbour](https://www.mattbarbour.com/research). In these experiments, a single aphid is placed on a radish plant. Aphids are clonal, and give birth to live (!) young. \n\nEach colony, incredibly, starts with a single aphid. This simplifies the expressions for the average and the variance because the starting population size is $ln(1) = 0$\n\n$$\n\\begin{align}\n\\mu_t &= a\\frac{1 - b^t}{1 - b} &= \\mu_\\infty(1 - b^t) \\\\\nv_t &= \\sigma^2\\frac{1 - (b^2)^t}{1 - b^2} &= v_\\infty(1 - b^{2t})\n\\end{align}\n$$\n\nThe entire experiment fits on a single tray in a growth chamber, and Katerie replicated the experiment 6 times.\n\n## Math: a model for one clone\n\nWhen we are making replicates observations of a single clone, we only need to know four quantites to predict both the average and the variation around that average: $a$, $b$, $\\sigma$ and the time that passed since the start, $t$\n\nHere is the full Bayesian model with priors. I used prior simulations to come up with these numbers (that's the next section below!)\n\n$$\n\\begin{align}\nx_{i} &\\sim \\text{Normal}(\\mu_t, \\sqrt{v_t})\\\\\n\\mu_t &= \\mu_\\infty(1 - b^t) \\\\\nv_t &= v_\\infty(1 - b^{2t}) \\\\\n\\mu_\\infty &= \\frac{a}{1 - b} \\\\\nv_\\infty &= \\frac{\\sigma^2}{1 - b^2} \\\\\na &\\sim \\text{Normal}(2, .5) \\\\\nb &\\sim \\text{Beta}(5, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\n$$\n\n\n## Make fake data\n\n\nSimulating from the data-generating model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_one_gompertz <- function(K = 1000, \n         a = 1, \n         b = 0.86,\n         s = .3,\n         ntime = 15){\n  \n  N = c(0, \n        rnorm(ntime - 1, \n              mean = a * (1 - b^(1:(ntime-1))) / (1 - b),\n              sd = s * (1 - (b^2)^(1:(ntime-1))) / (1 - b^2)\n        )\n  )\n        \n        \n  return(tibble(N = N, time = 0:(ntime-1)))\n}\n\nmap_df(1:6, ~ list(sim_one_gompertz()), .id = \"sim_id\") |> \n  ggplot(aes(x = time, y = N, group = sim_id)) + \n  geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the model in stan\nar_1 <- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1.stan\"))\n\nar_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{.stan}\ndata{\n  int n;\n  vector[n] time;\n  vector[n] x;\n}\n// transformed data {\n//   vector[n] x = log(pop);\n// }\nparameters {\n  real<lower=0> a;\n  real<lower=0,upper=1> b;\n  real<lower=0> sigma;\n}\ntransformed parameters {\n  real mu_max = a / (1 - b);\n  real sigma_max = sigma /sqrt(1 - b^2);\n}\nmodel {\n  a ~ normal(2, .5);\n  b ~ beta(5,2);\n  sigma ~ exponential(5);\n  x ~ normal(\n    mu_max .* (1 - pow(b, time)),\n    sigma_max .* sqrt(1 - pow(b^2, time))\n    );\n}\ngenerated quantities {\n  vector[15] x_pred;\n  x_pred[1] = 0;\n  for (j in 1:14) {\n    x_pred[j+1] = normal_rng(\n      mu_max * (1 - pow(b, j)),\n      sigma_max * sqrt(1 - pow(b^2, j))\n      );\n  }\n}\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngomp_one_pop_df <- map_df(1:6, ~ list(\n  sim_one_gompertz()), .id = \"sim_id\")\n  \n  \ngomp_nozero <- gomp_one_pop_df  |> \n    filter(time != 0)\n\ngomp_ar_1_sample <- ar_1$sample(data = list(n = nrow(gomp_nozero),\n                        x = gomp_nozero$N,\n                        time = gomp_nozero$time),\n                        parallel_chains = 4, refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 0.9 seconds.\nChain 1 finished in 1.1 seconds.\nChain 3 finished in 0.9 seconds.\nChain 4 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.9 seconds.\nTotal execution time: 1.5 seconds.\n```\n\n\n:::\n:::\n\n::: {.cell .preview-image}\n\n```{.r .cell-code}\ngomp_ar_1_sample |> \n  spread_rvars(x_pred[time]) |> \n  ggplot(aes(x = time-1, ydist = x_pred)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  theme_bw() + \n  geom_line(aes(x = time, y = N, group = sim_id),\n            inherit.aes = FALSE, data = gomp_one_pop_df) + \n  labs(x = \"Time\", y = \"log population size\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngomp_ar_1_sample$summary(variables = c(\"a\", \"b\", \"sigma\",\n                                       \"mu_max\", \"sigma_max\")) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|variable  |      mean|    median|        sd|       mad|        q5|        q95|     rhat| ess_bulk| ess_tail|\n|:---------|---------:|---------:|---------:|---------:|---------:|----------:|--------:|--------:|--------:|\n|a         | 0.8755229| 0.8715515| 0.0653034| 0.0640476| 0.7744877|  0.9901877| 1.002237| 1274.784| 1329.060|\n|b         | 0.8919885| 0.8926680| 0.0188091| 0.0186207| 0.8595486|  0.9219138| 1.002168| 1276.568| 1300.837|\n|sigma     | 0.5386121| 0.5334915| 0.0482727| 0.0461511| 0.4668362|  0.6243722| 1.004019| 1438.492| 1641.133|\n|mu_max    | 8.2620125| 8.1184950| 0.9562658| 0.8650823| 6.9728300| 10.0094000| 1.002092| 1446.378| 1782.139|\n|sigma_max | 1.2005219| 1.1903850| 0.1052177| 0.1015655| 1.0467550|  1.3880110| 1.002135| 1844.450| 1556.341|\n\n\n:::\n:::\n\n\n\n\n## Say it again but different: parameterizing based on $\\mu$ and $\\sigma$\n\nIt might be easier to set priors directly on equilibrium population size ($\\mu_\\infty$) and variance at equilibrium ($v_\\infty$) so I experimented with parameterizing the model directly that way. It works just as well!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the model in stan\nar1_mu_sigma <- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_mu_sigma.stan\"))\n\nar1_mu_sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{.stan}\ndata{\n  int n;\n  vector[n] time;\n  vector[n] x;\n}\n// transformed data {\n//   vector[n] x = log(pop);\n// }\nparameters {\n  real mu_max;\n  real<lower=0,upper=1> b;\n  real<lower=0> sigma_max;\n}\ntransformed parameters {\n  // real mu_max = a / (1 - b);\n  // real sigma_max = sigma /sqrt(1 - b^2);\n}\nmodel {\n  mu_max ~ normal(7, .5);\n  b ~ beta(5,2);\n  sigma_max ~ exponential(1);\n  x ~ normal(\n    mu_max .* (1 - pow(b, time)),\n    sigma_max .* sqrt(1 - pow(b^2, time))\n    );\n}\ngenerated quantities {\n  vector[15] x_pred;\n  x_pred[1] = 0;\n  for (j in 1:14) {\n    x_pred[j+1] = normal_rng(\n      mu_max * (1 - pow(b, j)),\n      sigma_max * sqrt(1 - pow(b^2, j))\n      );\n  }\n}\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngomp_ar1_mu_sigma_sample <- ar1_mu_sigma$sample(\n  data = list(n = nrow(gomp_nozero),\n              x = gomp_nozero$N,\n              time = gomp_nozero$time),\n  parallel_chains = 4, refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngomp_ar1_mu_sigma_sample |> \n  spread_rvars(x_pred[time]) |> \n  ggplot(aes(x = time-1, ydist = x_pred)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  theme_bw() + \n  geom_line(aes(x = time, y = N, group = sim_id),\n            inherit.aes = FALSE, data = gomp_one_pop_df)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngomp_ar1_mu_sigma_sample$summary(variables = c(\"mu_max\", \"b\", \"sigma_max\")) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|variable  |      mean|   median|        sd|       mad|        q5|       q95|     rhat| ess_bulk| ess_tail|\n|:---------|---------:|--------:|---------:|---------:|---------:|---------:|--------:|--------:|--------:|\n|mu_max    | 7.4195465| 7.408785| 0.3897996| 0.3887970| 6.8015800| 8.0852720| 1.003136| 1324.390| 1827.973|\n|b         | 0.8758691| 0.876516| 0.0121230| 0.0118971| 0.8543245| 0.8948875| 1.002905| 1356.657| 1740.710|\n|sigma_max | 1.1726875| 1.167125| 0.0945322| 0.0938486| 1.0264060| 1.3328110| 1.000760| 2029.433| 1859.360|\n\n\n:::\n:::\n\n\n\n## More than one model at once\n\nHere is a no-pooling approach to modelling different clones.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_multispp <- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_multispp.stan\"))\nar1_multispp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{.stan}\ndata{\n  int n;\n  int nclone;\n  vector[n] time;\n  vector[n] x;\n  array[n] int<lower=1, upper=nclone> clone_id;\n}\n// transformed data {\n//   vector[n] x = log(pop);\n// }\nparameters {\n  vector[nclone] log_a;\n  vector[nclone] logit_b;\n  vector[nclone] log_sigma;\n}\ntransformed parameters {\n  vector[nclone] b = inv_logit(logit_b);\n  vector[nclone] mu_max = exp(log_a) ./ (1 - b);\n  vector[nclone] sigma_max = exp(log_sigma) ./ sqrt(1 - b^2);\n}\nmodel {\n  log_a ~ normal(.7, .2);\n  logit_b ~ normal(1, .2);\n  log_sigma ~ normal(-1.5, .5);\n  x ~ normal(\n    mu_max[clone_id] .* (1 - pow(b[clone_id], time)),\n    sigma_max[clone_id] .* sqrt(1 - pow(b[clone_id]^2, time))\n    );\n}\ngenerated quantities {\n  matrix[15, nclone] x_pred;\n  x_pred[1,] = rep_row_vector(0, nclone);\n  for (s in 1:nclone){\n    for (j in 1:14) {\n      x_pred[j+1,s] = normal_rng(\n        mu_max[s] .* (1 - pow(b[s], j)),\n        sigma_max[s] .* sqrt(1 - pow(b[s]^2, j))\n        );\n    }\n  }\n}\n```\n\n\n:::\n:::\n\n\nA few things are different in this model compared to the previous one:\n\n* I'm passing in ID numbers for each clone, and an integer number for maximum number of clones\n* the model parameters are now vectors, not scalars\n* I've also put the parameters on different scale. This will come in handy later when the model becomes hierarchical. Whenever I personally do this, I also change the names (see Note below).\n* The predictions are now a matrix. This is because we need two different pieces of information for each observation: what time it is, and which clone we're talking about.\n\n:::{.callout-note}\n### Call me by your name\n\nThis code shows off a habit I've developed over the last few months of working on Bayesian models. When I put a parameter on a link scale, I change the parameter name to add the name of that link scale. \n\nFor example, you might have a strictly positive parameter and model it like this:\n\n```stan\nparameters {\n  real<lower=0> alpha;\n}\nmodel {\n  alpha ~ normal(1, .5);\n  ....\n}\n```\n\nBut then you might decide to model that on a log scale, either to reparameterize or to prepare for making it hierarchical later. Using link functions like the log and logit is standard in hiearchical models because it allows us to calculate a different value for every group, above and below the overall average, while respecting any constraints\n\n```stan\nparameters {\n  real alpha_log;\n}\nmodel {\n  alpha_log ~ normal(0, .2);\n}\n```\n\nI don't usually try to make sure the prior is EXACTLY equivalent, but its usually pretty straightforward to get kind of close. \n:::\n\nTo validate this model, I'm going to generate 6 aphid clones by choosing parameters for a and b from different \n\n\n::: {.cell}\n\n```{.r .cell-code}\nas <- runif(n = 6, min = 1.5, max = 2.5)\nbs <- runif(n = 6, min = .4, max = .9)\ngomp_many_df <- expand_grid(clone_id = 1:6, rep_id = 1:10) |> \n  mutate(a = as[clone_id],\n         b = bs[clone_id]) |> \n  rowwise() |> \n  mutate(x = list(sim_one_gompertz(a = a, b = b)))\n\ngomp_nozero_many_df <- gomp_many_df  |> \n  unnest(x) |> \n    filter(time != 0)\n\ngomp_nozero_many_df |> \n  ggplot(aes(x = time, y = N, group = rep_id)) + \n  geom_line() + \n  facet_wrap(~clone_id)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_multispp_samp <- ar1_multispp$sample(\n  data = list(n = nrow(gomp_nozero_many_df),\n              nclone = max(gomp_nozero_many_df$clone_id),\n              time = gomp_nozero_many_df$time,\n              x = gomp_nozero_many_df$N,\n              clone_id = gomp_nozero_many_df$clone_id), \n  parallel_chains = 4,\n  refresh = 0\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 34.5 seconds.\nChain 3 finished in 34.9 seconds.\nChain 1 finished in 35.1 seconds.\nChain 4 finished in 35.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 35.0 seconds.\nTotal execution time: 35.4 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\nar1_multispp_samp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n lp__       79.64  79.98 3.07 3.01 74.05 84.00 1.00     1728     2506\n log_a[1]    0.86   0.86 0.02 0.02  0.83  0.90 1.00     4228     2969\n log_a[2]    0.58   0.58 0.03 0.03  0.52  0.63 1.00     4768     2881\n log_a[3]    0.79   0.79 0.03 0.03  0.75  0.83 1.00     4648     2895\n log_a[4]    0.56   0.56 0.03 0.03  0.50  0.61 1.00     4467     2727\n log_a[5]    0.52   0.51 0.03 0.03  0.47  0.57 1.00     4103     3076\n log_a[6]    0.85   0.85 0.03 0.02  0.81  0.90 1.00     4921     2812\n logit_b[1]  1.17   1.17 0.04 0.04  1.10  1.24 1.00     4148     2972\n logit_b[2]  0.59   0.59 0.06 0.06  0.48  0.69 1.00     4790     2715\n logit_b[3]  0.59   0.59 0.05 0.05  0.51  0.67 1.00     4720     2849\n\n # showing 10 of 127 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_multispp_samp |> \n  gather_rvars(x_pred[time,clone_id]) |> \n  ggplot(aes(x = time-1, ydist = .value)) + stat_lineribbon() + \n  facet_wrap(~clone_id) + \n  scale_fill_brewer(palette = \"Greens\") + \n  ## add data\n  geom_line(aes(x = time, y = N, group = rep_id), \n            data = gomp_nozero_many_df,\n            inherit.aes = FALSE) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-note}\n### Time equals 🫖 - 1\n\nOne of the trickiest parts of working with these models has been adding and subtracting $1$. Here's some notes for the next time I get confused:\n\n* *take out the first value* We can't use data including `time=0` in a stan model because that implies 0 variance, which just causes lots of errors with the normal likelihood. Taking this point out is no loss, because it doesn't give us any real information -- there's no variance at all.\n* *using position in the matrix as data for predictions* I generate predictions for new clonal lines, in order to generate the posterior predictive distribution and plot the figures. I store these predictions in a matrix. However, the first observation (`time = 0`) is actually stored in the first position of the matrix (i.e., row 1). I just evaluate time as all the integers between 0 and 14, which gives two weeks of experiment in this case.  You can see in the `generated quantities` block that I actually use the index value in the for-loop (I called it `j`) as the value of time. This requires a weird trick: the value of `j` gets used, but then it is placed in the `j+1` position in the vector. This is because there is no \"row 0\" to go with the observation at _time_ 0, so everything is pushed up by 1.\n* *subtract 1 to get the correct time* This means that to get the right time, we need to subtract 1 from the value used in `generated quantities`. When I use `tidybayes` to extract the predictive distribution, I use `gather_rvars(x_pred[time,clone_id])` to specify that I want to call the first dimension of the matrix \"time\", and the second \"clone_id\". But then when I plot these data, I need to subtract 1 from the row position of a prediction to get the actual time value. This is because time = 0 is in row 1, time = 1 is in row 2, etc.  Doing this gives a figure where model predictions match \n:::\n\n\n### Hierarchical model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_multilevel <- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_multilevel.stan\"))\nar1_multilevel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{.stan}\n// modified from previous models with help from ChatGPT!\ndata {\n  int n;\n  int nclone;\n  vector[n] time;\n  vector[n] x;\n  array[n] int<lower=1, upper=nclone> clone_id;\n}\ntransformed data{\n  vector[n] twotime = 2 * time;\n}\nparameters {\n  vector[nclone] mu_log_a;\n  vector[nclone] mu_logit_b;\n  vector[nclone] mu_log_sigma;\n\n  cholesky_factor_corr[3] L_corr;  // Cholesky factorization of the correlation matrix\n  vector<lower=0>[3] sigma_params;  // Standard deviations for log_a, logit_b, log_sigma\n\n  matrix[nclone, 3] z_params_raw;  // Unconstrained parameters\n}\n\ntransformed parameters {\n  matrix[nclone, 3] z_params = z_params_raw * diag_pre_multiply(sigma_params, L_corr);\n\n  vector[nclone] log_a = mu_log_a + z_params[,1];\n  vector[nclone] logit_b = mu_logit_b + z_params[,2];\n  vector[nclone] log_sigma = mu_log_sigma + z_params[,3];\n\n  vector[nclone] b = inv_logit(logit_b);\n  vector[nclone] mu_max = exp(log_a - log1m_inv_logit(b));\n  vector[nclone] sigma_max = exp(log_sigma) ./ sqrt(1 - square(b));\n}\n\nmodel {\n  mu_log_a ~ normal(0.7, 0.2);\n  mu_logit_b ~ normal(1.7, 0.2);\n  mu_log_sigma ~ normal(-.7, 0.25);\n\n  L_corr ~ lkj_corr_cholesky(4);  // Prior on the Cholesky factor for the correlation matrix\n\n  sigma_params[1] ~ exponential(4);\n  sigma_params[2] ~ exponential(4);\n  sigma_params[3] ~ exponential(3.5);\n\n  to_vector(z_params_raw) ~ std_normal();\n\n  x ~ normal(\n    mu_max[clone_id] .* (1 - pow(b[clone_id], time)),\n    sigma_max[clone_id] .* sqrt(1 - pow(b[clone_id], twotime))\n  );\n}\n\ngenerated quantities {\n  matrix[15, nclone] x_pred;\n  x_pred[1,] = rep_row_vector(0, nclone);\n  for (s in 1:nclone){\n    for (j in 1:14) {\n      x_pred[j+1,s] = normal_rng(\n        mu_max[s] .* (1 - pow(b[s], j)),\n        sigma_max[s] .* sqrt(1 - pow(b[s]^2, j))\n      );\n    }\n  }\n}\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_multilevel_samp <- ar1_multilevel$sample(\n  data = list(n = nrow(gomp_nozero_many_df),\n              nclone = max(gomp_nozero_many_df$clone_id),\n              time = gomp_nozero_many_df$time,\n              x = gomp_nozero_many_df$N,\n              clone_id = gomp_nozero_many_df$clone_id), \n  parallel_chains = 4,\n  refresh = 500\n)\n\nar1_multilevel_samp\n```\n:::\n\n\nWe can fit to previous simulations no problem, but we can also simulate data directly from the model. Because this has gotten kind of big, I'm going to make a Stan program just for simulating data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_multilevel_prior <- cmdstan_model(here::here(\"posts/2023-11-15-ives03-ts/ar1_multilevel_prior.stan\"))\nar1_multilevel_prior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{.stan}\n// modified from previous models with help from ChatGPT!\ndata {\n  int nclone;\n  int nrep;\n}\n\nparameters {\n  vector[nclone] mu_log_a;\n  vector[nclone] mu_logit_b;\n  vector[nclone] mu_log_sigma;\n\n  cholesky_factor_corr[3] L_corr;  // Cholesky factorization of the correlation matrix\n  vector<lower=0>[3] sigma_params;  // Standard deviations for log_a, logit_b, log_sigma\n\n  matrix[nclone, 3] z_params_raw;  // Unconstrained parameters\n}\n\ntransformed parameters {\n  matrix[nclone, 3] z_params = z_params_raw * diag_pre_multiply(sigma_params, L_corr);\n\n  vector[nclone] log_a = mu_log_a + z_params[,1];\n  vector[nclone] logit_b = mu_logit_b + z_params[,2];\n  vector[nclone] log_sigma = mu_log_sigma + z_params[,3];\n\n  vector[nclone] b = inv_logit(logit_b);\n  vector[nclone] mu_max = exp(log_a - log1m_inv_logit(b));\n  vector[nclone] sigma_max = exp(log_sigma) ./ sqrt(1 - square(b));\n}\n\nmodel {\n  mu_log_a ~ normal(0.7, 0.2);\n  mu_logit_b ~ normal(1.7, 0.2);\n  mu_log_sigma ~ normal(-.7, 0.25);\n\n  L_corr ~ lkj_corr_cholesky(4);  // Prior on the Cholesky factor for the correlation matrix\n\n  sigma_params[1] ~ exponential(4);\n  sigma_params[2] ~ exponential(4);\n  sigma_params[3] ~ exponential(3.5);\n\n  to_vector(z_params_raw) ~ std_normal();\n}\n\ngenerated quantities {\n  array[nclone, nrep, 15] real x_pred;\n  for (s in 1:nclone){\n    for (r in 1:nrep){\n      x_pred[s, r, 1] = 0;\n    }\n  }\n  for (s in 1:nclone){\n    for (r in 1:nrep){\n      for (j in 1:14) {\n        x_pred[s, r, j+1] = normal_rng(\n          mu_max[s] .* (1 - pow(b[s], j)),\n          sigma_max[s] .* sqrt(1 - pow(b[s]^2, j))\n          );\n      }\n    }\n  }\n}\n```\n\n\n:::\n:::\n\n\nThis model code lacks the likelihood, but uses the same configuration as the previous non-centered model. \nIt also has a bit more going on the Generated Quantities block. \nHere, we are simulating multiple replicates using each clone, which simulates the actual experiment.\n\nWe can sample from this prior distribution and visualize it to see what it says. This is really the only way to get a good idea of what priors really mean in a nonlinear model like this one!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmultilevel_prior <- ar1_multilevel_prior$sample(\n  data = list(nclone = 12, nrep = 10), \n  chains = 1,\n  refresh = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 1 chain...\n\nChain 1 finished in 1.5 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\nprior_pred_multilevel <- multilevel_prior |> \n  gather_draws(x_pred[clone_id, rep, time], ndraws = 1) |> \n  mutate(time = time -1)\n\nprior_pred_multilevel |> \n  ggplot(aes(x = time, y = .value, group = rep)) + \n  geom_line() + \n  facet_wrap(~clone_id)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nFinally we can put this model through a very similar process of fitting and plotting as previous. \nHere I'm doing data prep, sampling and plotting all in one plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_df <- prior_pred_multilevel |> \n  filter(time != 0)\n\nar1_multilevel_samp <- ar1_multilevel$sample(\n  data = list(n = nrow(prior_df),\n              nclone = max(prior_df$clone_id),\n              time = prior_df$time,\n              x = prior_df$.value,\n              clone_id = prior_df$clone_id), \n  parallel_chains = 4,\n  refresh = 0\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 394.3 seconds.\nChain 3 finished in 398.0 seconds.\nChain 4 finished in 399.1 seconds.\nChain 1 finished in 402.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 398.5 seconds.\nTotal execution time: 402.6 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\n# ar1_multilevel_samp\n\nar1_multilevel_samp |> \n  gather_rvars(x_pred[time,clone_id]) |> \n  ggplot(aes(x = time-1, ydist = .value)) + stat_lineribbon() + \n  facet_wrap(~clone_id) + \n  scale_fill_brewer(palette = \"Greens\") + \n  ## add data\n  geom_line(aes(x = time, y = .value, group = rep), \n            data = prior_pred_multilevel,\n            inherit.aes = FALSE, col = \"darkorange\", alpha = .3)  + \n  labs(y = \"Population size (log)\", \n       x = \"Time\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nNote that this did not work perfectly, even though we are fitting to the prior predictive distribution. There are divergent iterations!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nar1_multilevel_samp$diagnostic_summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$num_divergent\n[1] 2 2 6 1\n\n$num_max_treedepth\n[1] 0 0 0 0\n\n$ebfmi\n[1] 0.8415368 1.0193399 0.8426104 0.9076898\n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}